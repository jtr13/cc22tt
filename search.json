[{"path":"index.html","id":"welcome","chapter":"1 Welcome!","heading":"1 Welcome!","text":"Artwork credit: Xingye Feng","code":""},{"path":"community-contribution.html","id":"community-contribution","chapter":"2 Community Contribution","heading":"2 Community Contribution","text":"fairly open-ended assignment provides opportunity receive credit contributing collective learning class, perhaps beyond. reflect minimum 3 hours work. complete assignment must submit short description contribution. appropriate, attach relevant files.many ways can contribute:organize lead workshop particular topic (date may assignment due date need schedule )help students find final project partnersgive well-rehearsed 5 minute lightning talk class datavis topic (theory tool) (email set date – may assignment due date need schedule )create video tutorial (length)create cheatsheet resourcewrite tutorial tool ’s well documentedbuild viz product (ex. htmlwidget RStudio add-) class use[idea](Note: translations allowed)may draw expand existing resources. , critical cite sources.","code":""},{"path":"community-contribution.html","id":"important-logistics","chapter":"2 Community Contribution","heading":"2.1 IMPORTANT LOGISTICS","text":"","code":""},{"path":"community-contribution.html","id":"groups","chapter":"2 Community Contribution","heading":"2.1.1 Groups","text":"may work partner choosing. work alone, need join group 1, simply submit work CourseWorks solo assignment.work partner, add group CC page People tab. Ed Discussion can used find partners similar interests.","code":""},{"path":"community-contribution.html","id":"what-to-submit","chapter":"2 Community Contribution","heading":"2.1.2 What to submit","text":"cases something tangible upload, tutorial, cheatsheet, etc. Alternatively may submit link material online (YouTube video, etc.) ’s nothing tangible include longer description (see 2.).cases something tangible upload, tutorial, cheatsheet, etc. Alternatively may submit link material online (YouTube video, etc.) ’s nothing tangible include longer description (see 2.).explanation motivation project, need addresses, evaluation project including learned / might differently next time. (1/2 page)explanation motivation project, need addresses, evaluation project including learned / might differently next time. (1/2 page)","code":""},{"path":"community-contribution.html","id":"submitting-your-assignment","chapter":"2 Community Contribution","heading":"2.1.3 Submitting your assignment","text":"must submit assignment twice: CourseWorks (can graded) class, details follow.CourseWorks submission (assignment): submit work .Rmd rendered .pdf .html file, just problem sets. work lend format, write assignment text box .CourseWorks submission (assignment): submit work .Rmd rendered .pdf .html file, just problem sets. work lend format, write assignment text box .Class (GitHub) submission: detail provided separate assignment.Class (GitHub) submission: detail provided separate assignment.","code":""},{"path":"community-contribution.html","id":"grading","chapter":"2 Community Contribution","heading":"2.1.4 Grading","text":"graded quality work, originality, effort invested. sources used must cited.","code":""},{"path":"github-submission-instructions.html","id":"github-submission-instructions","chapter":"3 GitHub submission instructions","heading":"3 GitHub submission instructions","text":"chapter gives information need upload community contribution. Please read entire document carefully making submission. particular note fact bookdown requires different .Rmd format ’re used , must make changes beginning file described submitting.","code":""},{"path":"github-submission-instructions.html","id":"background","chapter":"3 GitHub submission instructions","heading":"3.1 Background","text":"web site makes use bookdown package render collection .Rmd files nicely formatted online book chapters subchapters. job submit slightly modified version community contribution .Rmd file GitHub repository source files web site stored. backend, admins divide chapters book sections order .community contribution different format, create short .Rmd file explains , includes links relevant files, slides, etc. can post GitHub repo (another online site.)","code":""},{"path":"github-submission-instructions.html","id":"preparing-your-.rmd-file","chapter":"3 GitHub submission instructions","heading":"3.2 Preparing your .Rmd file","text":"submit ONE Rmd file.completing modifications, .Rmd look like sample .Rmd.Create concise, descriptive name project. instance, name base_r_ggplot_graph something similar work contrasting/working base R graphics ggplot2 graphics. Check .Rmd filenames file make sure name isn’t already taken. project name words joined underscores, white space. Use .Rmd .rmd. addition, letters must lowercase. Create copy .Rmd file new name.Create concise, descriptive name project. instance, name base_r_ggplot_graph something similar work contrasting/working base R graphics ggplot2 graphics. Check .Rmd filenames file make sure name isn’t already taken. project name words joined underscores, white space. Use .Rmd .rmd. addition, letters must lowercase. Create copy .Rmd file new name.Completely delete YAML header (section top .Rmd includes name, title, date, output, etc.) including --- line.Completely delete YAML header (section top .Rmd includes name, title, date, output, etc.) including --- line.Choose short, descriptive, human readable title project title show table contents – look examples panel left. Capitalize first letter (“sentence case”). first line document, enter single hashtag, followed single whitespace, title. important follow format bookdown renders title header. use single # headers anywhere else document.Choose short, descriptive, human readable title project title show table contents – look examples panel left. Capitalize first letter (“sentence case”). first line document, enter single hashtag, followed single whitespace, title. important follow format bookdown renders title header. use single # headers anywhere else document.second line blank, followed name(s):\n# Base R vs. ggplot2\n\nAaron Burr Alexander Hamilton\n\ncontent starts . second line blank, followed name(s):project requires data, please use built-dataset read directly URL, :\ndf <- readr::read_csv(\"https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv\")  absolutely must include data file, please use small one, many reasons desirable keep repository size small possible.project requires data, please use built-dataset read directly URL, :df <- readr::read_csv(\"https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv\")  absolutely must include data file, please use small one, many reasons desirable keep repository size small possible.included setup chunk .Rmd file, please remember remove label setup chunk, .e., use:\n{r, include=FALSE}\ninstead :\n{r setup, include=FALSE}included setup chunk .Rmd file, please remember remove label setup chunk, .e., use:instead :project requires libraries installed included document, please adhere following conventions. evaluate install.packages() statements document. Consumers .Rmd file won’t want packages get installed knit document. Include library() statements top .Rmd file, title, name, setup, content. chapter requires installation package source (GitHub installation), please add comment identifying . Please mention well PR. example library() section install statements won’t evaluated:\n\n# remotes::install_github(\"twitter/AnomalyDetection\")\nlibrary(\"AnomalyDetection\") # must installed sourceIf project requires libraries installed included document, please adhere following conventions. evaluate install.packages() statements document. Consumers .Rmd file won’t want packages get installed knit document. Include library() statements top .Rmd file, title, name, setup, content. chapter requires installation package source (GitHub installation), please add comment identifying . Please mention well PR. example library() section install statements won’t evaluated:developed .Rmd file moving library() statements rest file content, highly recommended knit review document . may change namespace available section code development, causing function work exhibit unexpected behavior.file contain getwd() / setwd() calls (never use scripts anyway!) write statements.Want get fancy? See optional tweaks section .","code":"# Base R vs. ggplot2\n\nAaron Burr and Alexander Hamilton\n\nYour content starts here. {r, include=FALSE}{r setup, include=FALSE}\n# remotes::install_github(\"twitter/AnomalyDetection\")\nlibrary(\"AnomalyDetection\") # must be installed from source"},{"path":"github-submission-instructions.html","id":"submission-steps","chapter":"3 GitHub submission instructions","heading":"3.3 Submission steps","text":"submit work, following “Workflow #4” – submitting pull request someone else’s repository write access. Instructions available lecture slides topic well tutorial. repeated abbreviated form, specific instructions naming conventions, content information, important details.Fork cc22tt repo (repo) GitHub account.Fork cc22tt repo (repo) GitHub account.Clone/download forked repo local computer.Clone/download forked repo local computer.Create new branch name project name, case sample_project. skip step. merge PR doesn’t come branch. already forgot , check tutorial fix .Create new branch name project name, case sample_project. skip step. merge PR doesn’t come branch. already forgot , check tutorial fix .Copy modified .Rmd file name root directory branch. example, sample_project.Rmd.Copy modified .Rmd file name root directory branch. example, sample_project.Rmd.include .html file. (order bookdown package work, .Rmd files rendered behind scenes.)include .html file. (order bookdown package work, .Rmd files rendered behind scenes.)[OPTIONAL] resources (images) included project, create folder resources/. example, resources/sample_project/. Put resources files . sure change links .Rmd include resources/.../, example:\n![Test Photo](resources/sample_project/pumpkins.jpg)[OPTIONAL] resources (images) included project, create folder resources/. example, resources/sample_project/. Put resources files . sure change links .Rmd include resources/.../, example:![Test Photo](resources/sample_project/pumpkins.jpg)ready submit project, push branch remote repo. Follow tutorial create pull request.ready submit project, push branch remote repo. Follow tutorial create pull request.point back forth begin team managing pull requests. asked make changes, simply make changes local branch, save, commit, push GitHub. new commits added pull request; need , , create new pull request. (, based circumstances, make sense close pull request start new one, tell .)point back forth begin team managing pull requests. asked make changes, simply make changes local branch, save, commit, push GitHub. new commits added pull request; need , , create new pull request. (, based circumstances, make sense close pull request start new one, tell .)pull request merged, ’s fine delete local clone (folder) well forked repository GitHub account.pull request merged, ’s fine delete local clone (folder) well forked repository GitHub account.","code":""},{"path":"github-submission-instructions.html","id":"optional-tweaks","chapter":"3 GitHub submission instructions","heading":"3.4 Optional tweaks","text":"prefer links chapter open new tabs, add {target=\"_blank\"} link, :\n[edav.info](edav.info){target=\"_blank\"}prefer links chapter open new tabs, add {target=\"_blank\"} link, :[edav.info](edav.info){target=\"_blank\"}Note headers (##, ###, etc.) converted numbered headings : ## –> 3.1 ### –> 3.1.1  headings appear chapter subheadings sub-subheadings navigation panel left. Think logical structure users navigate chapter. recommend using ## ### headings since “sub-sub-subheadings” 4.1.3.4 generally unnecessary look messy.Note headers (##, ###, etc.) converted numbered headings : ## –> 3.1 ### –> 3.1.1  headings appear chapter subheadings sub-subheadings navigation panel left. Think logical structure users navigate chapter. recommend using ## ### headings since “sub-sub-subheadings” 4.1.3.4 generally unnecessary look messy.Unfortunately, ’s simple way preview chapter ’s actually merged project. (bookdown preview_chapter() option works entire book rendered least become complex require packages project grows.) really want preview , fork clone minimal bookdown repo, add .Rmd file, click “Build book” button Build tab (next Git), open .html files _book folder web browser see rendered book.  ’re interested bookdown options, see official reference book.  useful tweaks share? Submit issue PR.Unfortunately, ’s simple way preview chapter ’s actually merged project. (bookdown preview_chapter() option works entire book rendered least become complex require packages project grows.) really want preview , fork clone minimal bookdown repo, add .Rmd file, click “Build book” button Build tab (next Git), open .html files _book folder web browser see rendered book.  ’re interested bookdown options, see official reference book.  useful tweaks share? Submit issue PR.","code":""},{"path":"github-submission-instructions.html","id":"faq","chapter":"3 GitHub submission instructions","heading":"3.5 FAQ","text":"","code":""},{"path":"github-submission-instructions.html","id":"what-should-i-expect-after-creating-a-pull-request","chapter":"3 GitHub submission instructions","heading":"3.5.1 What should I expect after creating a pull request?","text":"Within week create pull request, apply label assign classmate “PR merger” review files submit see meet requirements.Within week create pull request, apply label assign classmate “PR merger” review files submit see meet requirements.take time can process pull requests, long see pull request repo, don’t worry.take time can process pull requests, long see pull request repo, don’t worry.PR merger contacts regarding pull request, usually means files fail meet requirements. explain wrong, please fix soon possible.PR merger contacts regarding pull request, usually means files fail meet requirements. explain wrong, please fix soon possible.","code":""},{"path":"github-submission-instructions.html","id":"what-if-i-catch-mistakes-before-my-pull-request-is-merged","chapter":"3 GitHub submission instructions","heading":"3.5.2 What if I catch mistakes before my pull request is merged?","text":"Just make changes branch, commit push GitHub. automatically added pull request.","code":""},{"path":"github-submission-instructions.html","id":"what-if-i-catch-mistakes-after-my-pull-request-is-merged","chapter":"3 GitHub submission instructions","heading":"3.5.3 What if I catch mistakes after my pull request is merged?","text":"may submit additional pull requests fix material site. edits small, fixing typos, easiest make edits directly GitHub, following instructions. merge first pull requests edits, please patient.","code":""},{"path":"github-submission-instructions.html","id":"other-questions","chapter":"3 GitHub submission instructions","heading":"3.5.4 Other questions","text":"additional questions, please ask Discussions section respond.Thank contributions!","code":""},{"path":"sample-project.html","id":"sample-project","chapter":"4 Sample project","heading":"4 Sample project","text":"Joe Biden Donald TrumpThis chapter gives sample layout Rmd file.Test Photo","code":""},{"path":"r-window-functions-cheatsheet.html","id":"r-window-functions-cheatsheet","chapter":"6 R window functions cheatsheet","heading":"6 R window functions cheatsheet","text":"Gokul Sunilkumar Pooja SrinivasanWhat window function?window function performs calculation across set table rows somehow related current row. Although functionalities might sound similar aggregate functions, window functions cause rows become grouped single output row like non-window aggregate calls . Instead, rows retain separate identities. Behind scenes, window function able access just current row query result.Although usage window functions common database systems related applications, single source lookup help ease developers’ work programming. Hence, come cheatsheet .Check cheatsheet : https://github.com/gokul-sunilkumar/RWindowFunctions/blob/main/RWindowFunctionsCheatSheet.pdf","code":""},{"path":"rmd-chunk-option-cheat-sheet.html","id":"rmd-chunk-option-cheat-sheet","chapter":"7 Rmd chunk option cheat sheet","heading":"7 Rmd chunk option cheat sheet","text":"Yunchen JiangMotivation ContributionAs graduate student undergraduate degree actuarial mathematics, course almost first exposure Rstudio. received Pset, first thing confused code chunk r markdown. project, contribute making condensed cheat sheet commonly used chunk options refine understanding rmd files simultaneously help r beginners similar situation .Cheat sheetChunk options written chunk headers form tag=value like :\n{r -chunk, echo=FALSE, fig.height=4, dev=‘jpeg’}\n…special chunk option chunk label (e.g., -chunk example). chunk label need tag. prefer form tag=value, also use chunk option label explicitly:\n{r, label=‘-chunk’, echo=FALSE, fig.height=4, dev=‘jpeg’}\n…can also write chunk options body code chunk #| like :\n{r}\n#| -chunk, echo = FALSE,\n#| fig.height=4, dev=‘jpeg’\n…syntax, chunk options must written continuous lines beginning chunk body. can break options onto many lines wish lines must start special comment prefix #|can also use YAML syntax write options inside chunk form tag: value. Normally provide one option per line like :\n{r}\n#| echo: false\n#| fig.width: 4\n#| dev: ‘jpeg’\n…chunk label chunk assumed unique within document. especially important cache plot filenames, filenames based chunk labels.list commonly used chunk options knitr documented format “option: (default value; type value)“.Code evaluation:eval: (TRUE; logical numeric) Whether evaluate code chunk. can also numeric vector choose R expression(s) evaluate.\ne.g., eval = c(1, 3, 4) evaluate first, third, fourth expressions, eval = -(4:5) evaluate expressions except fourth fifth.Text output:echo: (TRUE; logical numeric) Whether display source code output document. Besides TRUE/FALSE, shows/hides source code, can also use numeric vector choose R expression(s) echo chunk.\ne.g., echo = 2:3 means echo 2nd 3rd expressions, echo = -4 means exclude 4th expression.results: (‘markup’) Mark text output appropriate environments depending output format. example, R Markdown, text output character string “[1] 1 2 3”, actual output knitr produces :case, results=‘markup’ means put text output fenced code blocks.warning: (TRUE; logical) Whether preserve warnings (produced warning()) output. FALSE, warnings printed console instead output document:\n{r}\nwithCallingHandlers(\nexpr = .numeric(c(“1”, “”)),\nwarning = function(w) warn <<- paste(“** warning:”, w$message, “**“)\n)\nWarning: NAs introduced coercion[1] 1 NA{r, warning = false}\nwithCallingHandlers(\nexpr = .numeric(c(“1”, “”)),\nwarning = function(w) warn <<- paste(“** warning:”, w$message, “**“)\n)\n[1] 1 NAIt can also take numeric values indices select subset warnings include output. Note values reference indices warnings (e.g., 3 means “third warning thrown chunk”) indices expressions allowed emit warnings.error: (True; logical) Whether preserve errors (stop()). default, errors code chunks Rmd document halt R. want show errors without stopping R, may use chunk option error = TRUE:\n{r,error = TRUE}\n1 + “”\nsee error message output document compile Rmd document: Error 1 + “”: non-numeric argument binary operator. R Markdown, error = FALSE default, means R stop error running code chunks.include: (TRUE; logical) Whether include chunk output output document. FALSE, nothing written output document, code still evaluated plot files generated plots chunk, can manually insert figures later.Code decoration:comment: (‘##’; character) prefix added line text output. default, text output commented ##, readers want copy run source code output document, can select copy everything chunk, since text output masked comments (ignored running copied text). Set comment = ’’ remove default ##.prompt: (FALSE; logical) Whether add prompt characters R code. TRUE, knitr add > start line code displayed final document. Note adding prompts can make difficult readers copy R code output, prompt = FALSE may better choice.highlight: (TRUE) Whether syntax highlight source code.Cache:cache: (FALSE; logical) Whether cache code chunk. caching turned via chunk option cache = TRUE, knitr write R objects generated code chunk cache database, can reloaded next time. evaluating code chunks second time, cached chunks skipped (unless modified), objects created chunks loaded previously saved databases (.rdb .rdx files), files saved chunk evaluated first time, cached files found (e.g., may removed hand).cache.path: (‘cache/’; character) prefix used generate paths cache files. R Markdown, default value based input filename, e.g., cache paths chunk label FOO file INPUT.Rmd form INPUT_cache/FOO_..cache.lazy: (TRUE; logical) Whether lazyLoad() directly load() objects. large objects, lazyloading may work, cache.lazy = FALSE may desirabledependson: (NULL; character numeric) character vector chunk labels specify chunks chunk depends . option applies cached chunks —sometimes objects cached chunk may depend cached chunks, chunks changed, chunk must updated accordingly. dependson numeric vector, means indices chunk labels, e.g., dependson = 1 means chunk depends first chunk document, dependson = c(-1, -2) means depends previous two chunks (negative indices stand numbers chunks chunk, note always relative current chunk).Plots:fig.width, fig.height: (7; numeric) Width height plot (inches), used graphics device.fig.ext: (NULL; character) File extension figure output. NULL, derived graphical device; see knitr:::auto_exts details.fig.asp: (NULL; numeric) aspect ratio plot, .e., ratio height/width. fig.asp specified, height plot (chunk option fig.height) calculated fig.width * fig.asp.fig.dim: (NULL; numeric) numeric vector length 2 provide fig.width fig.height, e.g., fig.dim = c(5, 7) shorthand fig.width = 5, fig.height = 7. fig.asp fig.dim provided, fig.asp ignored (warning).fig.align: (‘default’; character) Alignment figures output document. Possible values default, left, right, center. default make alignment adjustments.fig.path: (‘figure/’; character) prefix used generate figure file paths. fig.path chunk labels concatenated generate full paths. may contain directory like figure/prefix-; directory created exist.fig.show: (‘asis’; character) show/arrange plots. Possible values follows:asis: ‘hide’, knitr generate plots created chunk, include final document. ‘hold’, knitr delay displaying plots created chunk end chunk. ‘animate’, knitr combine plots created chunk animation.","code":"[1] 1 2 3"},{"path":"helpful-ggplot2-extensions-cheatsheet.html","id":"helpful-ggplot2-extensions-cheatsheet","chapter":"8 Helpful ggplot2 Extensions Cheatsheet","heading":"8 Helpful ggplot2 Extensions Cheatsheet","text":"Frank Li Liang ZhuangCurrently, total 117 ggplot2 extensions extensions add additional features ggplot2 make powerful. cheatsheet includes useful information four ggplot2 extensions: ggbump, ggradar, ggpol, treemapify.ggbump: creates varies bump charts ggplot. Bump charts good plot path nodes statistical significance.ggradar: creates radar charts. Radar charts useful data values multiple common variables widely used performance analysis.ggpol: adds additional features ggplot2 including GeomArcbar, GeomParliament, GeomCircle, GeomTshignlight, FacetShare, GeomBartext, GeomBoxjitter. details : https://erocoar.github.io/ggpol/.treemapify: creates tree maps ggplot2. useful \ndata hierarchy, country GDP company’s stock market share.extensions, use link: https://exts.ggplot2.tidyverse.org/gallery/cheatsheet available : https://github.com/leolisticierism/EDAV_Community_Contribution/blob/main/EDAV%20Community%20Contribution%20Cheat%20Sheet.pdf","code":""},{"path":"commonly-used-graph-cheatcheet.html","id":"commonly-used-graph-cheatcheet","chapter":"9 Commonly-used graph cheatcheet","heading":"9 Commonly-used graph cheatcheet","text":"Wangtao Zheng, UNI:wz2618During R coding exercises EDAV homework, realized necessity creating R code library/cheatsheet include codes used draw commonly-applied graphs data visualization. way, one need memorize code used drawing different graphs, saves time.R codes used draw commonly-used graphs thus included cheatsheet. Graphs useful data visualization projects, scatterplot, boxplot, bar plot, histogram, etc., included cheatsheet. also included coding methods various modifications graphs convenience reasons, cheatsheet can help members community different circumstances.can find cheatsheet :https://github.com/zwt950715/EDAV-Fall-2022-Community-Contribution/blob/main/%20Useful%20Cheatsheet%20For%20Data%20Visualization.pdfThis cheatsheet covers materials first two problem sets EDAV course fall 2022. Thus, types graphs coding method can added cheatsheet make useful R programming EDAV purposes. Hope cheatsheet can help learning, looking forward valuable advices!","code":""},{"path":"git-and-version-control.html","id":"git-and-version-control","chapter":"10 Git and version control","heading":"10 Git and version control","text":"Dianjing Fan Yijia HeVersion control one essential techniques programmers. Version control Git helpful programmers software teams manage changes codes time. allows developers collaboratively work faster, smarter, efficiently reducing development time communication costs. can keep track every modification codes contributor without conflicting concurrent work. addition, knowledge practice version control Git usually required programmers workplace setting. time, well taught school courses explicitly mentioned job description cases. Therefore, create cheat sheet Git beginners learn understand Git within short time. project, gained deeper understanding version control Git terms use Git, terminology, , practiced must-know commands used Git. might add real-world examples using Git commands, record short tutorial video beginners, include knowledge Github next time.can find cheatsheet :\nhttps://github.com/BrownSugarBobaMilkTea/edav_git_cheatsheet/blob/main/git_cheatsheet.pdf","code":""},{"path":"useful-plots-for-bioinfomatics-in-r.html","id":"useful-plots-for-bioinfomatics-in-r","chapter":"11 Useful plots for Bioinfomatics in R","heading":"11 Useful plots for Bioinfomatics in R","text":"Xiangming HuangThis cheat sheet includes simple ways create survival curve, heatmap, volcano plot. plots useful visualization statistical analysis Bioinformatics. Many R packages specialized Biological analysis available free. packages avoided cheatsheet possible help readers understand data processed plots constructed elementary way.link: https://github.com/xmh3698/plotsForBio/blob/main/plotsForBio.pdf","code":""},{"path":"different-ways-to-create-world-map-in-r.html","id":"different-ways-to-create-world-map-in-r","chapter":"12 Different ways to create world map in R","heading":"12 Different ways to create world map in R","text":"Yuhang Qiu Yinqi Wang","code":""},{"path":"different-ways-to-create-world-map-in-r.html","id":"explanation-of-motivation","chapter":"12 Different ways to create world map in R","heading":"12.1 Explanation of Motivation","text":"Plotting world map good visualization strategy someone wants analyze data country country, even continent continent. Using world map also helps us terms visualizing locations distances (countries, cities, etc.). Making world map also good geographic researches. examples use world map : Gini around world, countries developed undeveloped. lot topics world map can visualize. community contribution project, ’ve learned make world map (different style ways) using multiple packages “rnaturalearth” “rworldmap” got refreshing thorough understanding packages methods. might differently next time trying record video tutorial packages methods ’ve introduced discussed cheatsheet.link cheat sheet :https://github.com/a844574798/worldMapCheatSheet/blob/main/WorldMapCheatsheet.pdf","code":""},{"path":"color-in-r-python-and-jshtmlcss.html","id":"color-in-r-python-and-jshtmlcss","chapter":"13 Color in R, Python, and JS/HTML/CSS","heading":"13 Color in R, Python, and JS/HTML/CSS","text":"Shujie HuColor choice matters. Either simple visualization user interface design, good color selection easiest way achieve incredible results. right color selection supports better information readability enhances viewer navigation capabilities. can also fulfill subconscious aesthetic user needs even stimulate intuitive interactions. User interface design, color central part online marketing strategy. Researchers various fields investigating impacts color brought viewer, numerous amount color palettes created programming languages, matter programming language use, always can find plenty packages use.However, don’t know name specific color palette, ’s hard even look internet. Therefore, motivation project collect beautiful color palettes visualization languages used far. cheatsheet addresses need find good color scheme presenting data students may encounter visualization projects, either R, Python, JS/HTML/CSS. also serves brief tutorial using package. presenting colors name, easy locate colors want help compare colors package, thereby saving time browsing internet.evaluation project, think cheatsheet serves purpose well. ’ve learned many color packages. color packages picked ones think beautiful cover almost every possible colors presented cheatsheet. also learned hexadecimal triplets represent colors web programming languages, found several useful resources may need future projects. cheasheet presents part research done, since order create cheatsheet need know lot able summarize findings briefly.cheatsheet available :\nhttps://github.com/tracyhsj/Color/blob/main/Colors%20palettes%20in%20R%2C%20Python%2C%20and%20HTML:CSS%20.pdfTest Photo","code":""},{"path":"color-palettes-for-data-visualization.html","id":"color-palettes-for-data-visualization","chapter":"14 Color Palettes for Data Visualization","heading":"14 Color Palettes for Data Visualization","text":"Zixiao Zhang Yingyi Zhu","code":""},{"path":"color-palettes-for-data-visualization.html","id":"explanation-of-the-motivation","chapter":"14 Color Palettes for Data Visualization","heading":"14.1 Explanation of the motivation","text":"Data visualization mainly uses graphics clearly effectively present results data analysis. addition choosing right plot express trends relationships data, choosing suitable color scheme also significant. color represents unique message, color combination can show data type difference categories. Good use color can reduce level understanding data trying convey help viewer understand meaning data intuitively. Poor use color can opposite effect. example, many colors can disrupt viewer’s attention prevent understanding true meaning plot.set cheat sheet provide users color-matching tips introduce packages can use R help color matching. community contribution project, offer several color schemes. process, learned colors appear scheme. learn use colors right situations. might creates video tutorial picking right color next time.Check cheatsheet :https://github.com/XIXXII/colorPalettesCheatSheet/blob/master/colorPalattesForDV.pdf","code":""},{"path":"data-visualization-using-seaborn.html","id":"data-visualization-using-seaborn","chapter":"15 Data visualization using Seaborn","heading":"15 Data visualization using Seaborn","text":"Sida Huang Longxiang ZhangVisualization data can help data scientists perform exploratory analysis data efficiently. taking EDAV course, learned exploratory data visualization analysis using R language. However, considering many students familiar Python language, decided present content EDAV course Python.main content community contribution tutorial exploratory data visualization analysis using Python third-party library seaborn. designed Community Contribution based 9-th 13-rd chapters edav information website.detailed tutorials can found : https://github.com/amethystorm/EDAV_CC20.","code":""},{"path":"machine-learning-cheatsheet-for-r-and-python.html","id":"machine-learning-cheatsheet-for-r-and-python","chapter":"16 Machine Learning Cheatsheet for R and Python","heading":"16 Machine Learning Cheatsheet for R and Python","text":"Jingyi FengThe popular languages training machine learning models R Python (extensive packages resources). community contribution, build cheat sheet data pre-processing, machine learning, simple data visualization R Python. structure cheat sheet follows:Data processing (import file, scaling, train test split)Supervised learning techniques (regression models, classification models)Unsupervised learning techniques (PCA, K-Means)Evaluating model performance (RMSE, R2, accuracy, confusion matrix, ROC-AUC, F1-score)Preliminary data visualization (ggplot2, seaborn)cheat sheet available : https://github.com/jenniferfjy/Community_Contribution/blob/main/MLCheatSheet.pdf","code":""},{"path":"data-visualization-with-r-ggplot2-vs.-matlab.html","id":"data-visualization-with-r-ggplot2-vs.-matlab","chapter":"17 Data Visualization with R ggplot2 vs. Matlab","heading":"17 Data Visualization with R ggplot2 vs. Matlab","text":"Yuxin LinMatlab popular programming language area mathematical computation calculus, linear algebra, matrix manipulation.[1] Also, similar R, Matlab powerful tool data analytics, machine learning, data visualization. However, common application domain, tutorials plotting Matlab introduce deal simple simulated array inputs rather data set files.Motivated assignment analyzing auditory fMRI data Matlab applied machine learning course, cheatsheet created beginners Matlab programming. cheat sheet provides codes plotting graphs highly frequently used process data exploration results visualization, language R package ggplot2 Matlab. Although codes plotting Matlab appears trivial, noted Matlab differs R programming language functions, like use histograms bar plots. side--side contrast, cheat sheet expected help people experience R familiar Matlab plotting smooth start.Link: https://github.com/linlinlin-yx/R-ggplot2-vs-Matlab-Plot/blob/main/r_vs_matlab.pdf","code":""},{"path":"data-visualization-with-r-ggplot2-vs.-matlab.html","id":"reference","chapter":"17 Data Visualization with R ggplot2 vs. Matlab","heading":"17.0.1 Reference","text":"[1] https://www.geeksforgeeks.org/differences--matlab--r-programming-language/[2] https://www.mathworks.com/help/matlab/creating_plots/types--matlab-plots.html[3] https://www.mathworks.com/help/matlab/matlab_prog/access-data---table.html[4] ggplot2 cheatsheet (https://ggplot2.tidyverse.org/#cheatsheet)[5] edav.info (https://edav.info/spatial-data.html)","code":""},{"path":"ggplot2-plots-in-python-cheat-sheet-tutorial.html","id":"ggplot2-plots-in-python-cheat-sheet-tutorial","chapter":"18 Ggplot2 plots in python cheat sheet tutorial","heading":"18 Ggplot2 plots in python cheat sheet tutorial","text":"Braden Huffman","code":""},{"path":"ggplot2-plots-in-python-cheat-sheet-tutorial.html","id":"motivation","chapter":"18 Ggplot2 plots in python cheat sheet tutorial","heading":"18.1 Motivation","text":"R, ggplot2 powerful visualization tool data scientists knowledge . Ggplot2 might even useful visualization tool, event ggplot2 used, data scientist needs able visualize data. created cheat sheet provides documentation examples create popular R graphs according http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html, equivalents Python. also included couple graphs discussed class.","code":""},{"path":"ggplot2-plots-in-python-cheat-sheet-tutorial.html","id":"table","chapter":"18 Ggplot2 plots in python cheat sheet tutorial","heading":"18.2 Table","text":"\nTable 18.1: Cheat sheet\n","code":"\ndata %>% \n  kbl(caption = \"Cheat sheet\", width=10) %>% \n  kable_material_dark(full_width=F, font_size=8)"},{"path":"ggplot2-plots-in-python-cheat-sheet-tutorial.html","id":"example","chapter":"18 Ggplot2 plots in python cheat sheet tutorial","heading":"18.3 Example","text":"Consider following time series data:x: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10y: 5, 4, 7, 6, 6, 19, 20, 15, 10, 8Imagine moment boss instructed create Area Plot data. know easily. fact, can five lines code.smile moment, knowing actually home dinner family today. keep smiling utters words, “Python.” area plots fairly popular type graph, realize don’t even know python way create area chart, decide consult cheat sheet.cheat sheet takes following website https://www.python-graph-gallery.com/area-plot/. Fortune shine’s today. Creating Area Plot Python isn’t hard thought .https://colab.research.google.com/drive/1DxD5UZolQxcphI44kvK8w4YR-RkZNYgc#scrollTo=UdOui0Fhk24e.show boss graph linked iPython notebook, isn’t impressed artistic ability, impressed speed.Clearly Area Plots aren’t worlds hardest graphs create, many graphs table difficult find create Python. hope tutorial cheat sheet make easier start creating favorite ggplot2 graphs Python.","code":"\nx <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\ny <- c(5, 4, 7, 6, 6, 19, 20, 15, 10, 8)\ndf <- data.frame(x,y)\n\nggplot(data = df) +\n  geom_area(mapping=aes(x,y))"},{"path":"model-evaluation-selection-visualization-with-easystats.html","id":"model-evaluation-selection-visualization-with-easystats","chapter":"19 Model evaluation, selection & visualization with easystats","heading":"19 Model evaluation, selection & visualization with easystats","text":"Zhuolin Luo (zl2852), Kaiyuan Liu (kl3447)created cheat sheet package named easystats R. easystats collective framework packages, including “performance”, “parameters”, etc. intends easier statistical modeling, reporting, visualization.cheat sheet focus functions, examples, visualizations related reporting, checking, visualizing model parameters, model predictions, model assumptions, model performance, model selection easystats package. Hopefully can smoothen model building process. cheat sheet divided following parts:Parameters: section includes functions reporting features parameters model.Predictions: section includes functions making model-based estimations predictions, visualization examples provided.Assumption Diagnostics: section includes functions testing whether model satisfies specific assumption returning corresponding value, visualization examples provided.Performance: section includes functions report metrics evaluating model’s performance.Model Selection: section includes functions compares performance among different models helps select model, visualization examples provided.cheatsheet available : [Cheat Sheet Model Evaluation, Selection & Visualization easystats] (https://github.com/ChristalL99/Fall_2022_5702_EDAV_Community_Contribution/blob/17c60a9ddf50770303bc7d9b12ff308cd13a6370/Cheatsheet%20for%20Model%20Evaluation,%20Selection%20&%20Visualization%20with%20easystats.pdf)","code":""},{"path":"time-series-itsmr-cheat-sheet.html","id":"time-series-itsmr-cheat-sheet","chapter":"20 Time Series: ITSMR Cheat Sheet","heading":"20 Time Series: ITSMR Cheat Sheet","text":"Qinqi Zhang Yanqing Wang","code":""},{"path":"time-series-itsmr-cheat-sheet.html","id":"introduction","chapter":"20 Time Series: ITSMR Cheat Sheet","heading":"20.1 Introduction","text":"Cheatsheet ITSMR package R. contains functions modeling forecasting time series data. time series set observations one recorded specific time. Forecasting time series different methods, recursive prediction algorithms, Durbin-Levinson algorithm, innovations algorithm. instance, ITSMR package contains functions computing ACVF, ACF & PACF, checking causality & invertibility, visualization, etc. also includes five methods estimating ARMA parameters: Yule-Walker, Burg, Hannan-Rissanen, maximum likelihood, innovations method functions, innovations algorithm used compute variance White Noise process.","code":""},{"path":"time-series-itsmr-cheat-sheet.html","id":"motivation-1","chapter":"20 Time Series: ITSMR Cheat Sheet","heading":"20.2 Motivation","text":"Introduction Time Series Forecasting Peter J. Brockwell Richard . Davis introduces application ISTM2000 also models, forecasts visualizes time series data tsm format. Unfortunately, although application powerful convenient, Windows system can operate application. R language constraints computer systems popularized analyzing data. Therefore, ITSMR package R language discovered might able become substitution users without Windows systems. time series data txt file accepted package. Similar ISTM2000, also pre-loaded datasets number datasets less ITSM2000 application. ITSMR package main functions time series forecasting. Moreover, cheat sheet ITSMR package created discovery, bringing introduction functions related concepts.Link: https://github.com/CommunityContributionGroup/CC/blob/main/ITSMR_Cheat_Sheet.pdf","code":""},{"path":"preprocessing-and-visualization-of-time-series-data.html","id":"preprocessing-and-visualization-of-time-series-data","chapter":"22 Preprocessing and Visualization of Time Series Data","heading":"22 Preprocessing and Visualization of Time Series Data","text":"Siyuan DingIn tutorial, learn visualize time series data. reaching time series data, always organized want, need preprocessing first visualize . use time series dataset Covid-19 vaccination 2020-12-14 2022-10-30 example tutorial, dataset available https://raw.githubusercontent.com/govex/COVID-19/master/data_tables/vaccine_data/us_data/time_series/time_series_covid19_vaccine_doses_admin_US.csv goal visualize relationship vaccinated doses state 2022, 2022-01-01.use three packages tutorial: dplyr, lubridate tidyr manipulate dataset preprocessing, ggplot2 visualization.","code":"\n# The packages can be installed by command: install.packages()\nlibrary(dplyr) \nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(tidyr)"},{"path":"preprocessing-and-visualization-of-time-series-data.html","id":"preprocess-on-a-time-series-data","chapter":"22 Preprocessing and Visualization of Time Series Data","heading":"22.1 Preprocess on a Time Series Data","text":"part, first work transforming original data downloaded online data frame work . look data see whether problem data points discipline Time Series Data. Finally deal missing values. preprocessings done, data good visualize analyze.","code":""},{"path":"preprocessing-and-visualization-of-time-series-data.html","id":"transform-dataset","chapter":"22 Preprocessing and Visualization of Time Series Data","heading":"22.1.1 Transform DataSet","text":"first load get overall look dataset, find data 61 rows 698 columns. visualize relationship vaccinated doses state, need three things: state name, state population state vaccination population day. achieve , can use select dplyr package r. focus data 2022 example .Now, table 305 columns, make dataframe works better, woule like four columns Province_State, Population, Date, Vaccination_Doses Date comes colnames table Vaccination_Doses number population state date comes original entries date columns. achieve , can temporarily ignore Population column since identical state, can just full join later work vaccination population first. duplicate state name number dates times, build matrix store value vaccination dose state day extracting original dataset using subset dplyr transform dataframe putting together.Now transformed dataframe vaccinated doses, can full join Population state. province available population data, deal , can use drop_na function tidyr. functino let us drop rows according column na values.time, got transformed dataframe information needed, remember, time series dataset, need careful Date column! must check whether entries date type need sort date visualization date type like string.found data type date ! Now, need transform date type using lubridate package. lubridate, can manipulate dates easily, many functions packages. string daymonthyear, example “12032000”, can use dmy() return date format “2000-03-12”; Similarly, string monthdayyear format, can use mdy(), can use ymd() data yearmonthday format. Just remember m represents month, d represents day, y represents year able find correct function need. example case, entries Date now string yearmonthday format, use ymd() . use function, first need remove ‘X’ character beginning date, can achieve substring function.can double check data type, find date format now. dataframe transformed good use.","code":"\n# load data\nvaccination_all <- read.csv(\"https://raw.githubusercontent.com/govex/COVID-19/master/data_tables/vaccine_data/us_data/time_series/time_series_covid19_vaccine_doses_admin_US.csv\")\ncolnames(vaccination_all)[1:15]##  [1] \"UID\"            \"iso2\"           \"iso3\"           \"code3\"         \n##  [5] \"FIPS\"           \"Admin2\"         \"Province_State\" \"Country_Region\"\n##  [9] \"Lat\"            \"Long_\"          \"Combined_Key\"   \"Population\"    \n## [13] \"X2020.12.14\"    \"X2020.12.15\"    \"X2020.12.16\"\ncolnames(vaccination_all)[(ncol(vaccination_all)-5):ncol(vaccination_all)]## [1] \"X2022.11.07\" \"X2022.11.08\" \"X2022.11.09\" \"X2022.11.10\" \"X2022.11.11\"\n## [6] \"X2022.11.12\"\nvaccination_df <- vaccination_all %>% select(Province_State, Population, `X2022.01.01`:tail(names(vaccination_all),1))\ncolnames(vaccination_df)[1:15]##  [1] \"Province_State\" \"Population\"     \"X2022.01.01\"    \"X2022.01.02\"   \n##  [5] \"X2022.01.03\"    \"X2022.01.04\"    \"X2022.01.05\"    \"X2022.01.06\"   \n##  [9] \"X2022.01.07\"    \"X2022.01.08\"    \"X2022.01.09\"    \"X2022.01.10\"   \n## [13] \"X2022.01.11\"    \"X2022.01.12\"    \"X2022.01.13\"\ncolnames(vaccination_df)[(ncol(vaccination_df)-5):ncol(vaccination_df)]## [1] \"X2022.11.07\" \"X2022.11.08\" \"X2022.11.09\" \"X2022.11.10\" \"X2022.11.11\"\n## [6] \"X2022.11.12\"\nvaccination_df <- vaccination_df[,-2]\n# Find number of States\nstate <- vaccination_df$Province_State\nState_num <- length(state)\n# Find number of Days\ndate <- colnames(vaccination_df)[c(-1)]\ntime_window <- dim(vaccination_df)[2]-1\n# Then in the transformed dataframe, each state should occur for time_window times\nState <- rep(state, each = time_window)\n# Then in the transformed dataframe, each date should occur for State_num times\nDate <- rep(date, State_num)\n# Build a matrix to contain the vaccination doses\nvac_matrix <- matrix()\n# We select the daily vaccination doses for each state and store them in a Matrix\nfor (i in 1:State_num){\n  vac_matrix <- rbind(vac_matrix,matrix(unlist(vaccination_df%>%subset(Province_State==state[i]))[-1]))\n}\n# Drop the first column, which is NA\nvac_matrix <- vac_matrix[2:length(vac_matrix)]\n# Get the transformed dataset\nVac_DF <- cbind.data.frame(State, Date, vac_matrix)\ncolnames(Vac_DF) <- c(\"State\", \"Date\", \"Vaccinated Doses\")\nVac_DF$`Vaccinated Doses` = as.numeric(Vac_DF$`Vaccinated Doses`)\nhead(Vac_DF)##     State        Date Vaccinated Doses\n## 1 Alabama X2022.01.01          5624234\n## 2 Alabama X2022.01.02          5624234\n## 3 Alabama X2022.01.03          5624234\n## 4 Alabama X2022.01.04          5678299\n## 5 Alabama X2022.01.05          5681793\n## 6 Alabama X2022.01.06          5695747\nstate_pop <- vaccination_all %>% select(Province_State, Population)\nVac_DF <- Vac_DF %>% full_join(state_pop, by = c (\"State\" = \"Province_State\"))\nVac_DF <- Vac_DF %>% drop_na(Population)\nhead(Vac_DF)##     State        Date Vaccinated Doses Population\n## 1 Alabama X2022.01.01          5624234    4903185\n## 2 Alabama X2022.01.02          5624234    4903185\n## 3 Alabama X2022.01.03          5624234    4903185\n## 4 Alabama X2022.01.04          5678299    4903185\n## 5 Alabama X2022.01.05          5681793    4903185\n## 6 Alabama X2022.01.06          5695747    4903185\nstate <- unique(Vac_DF$State)\nState_num <- length(state)\ndate <- unique(Vac_DF$Date)\ntime_window <- length(date)\n# Check data type of the Date column\nclass(Vac_DF$Date)## [1] \"character\"\n# We first remove the 'X' before the date\nVac_DF$Date <- substring(Vac_DF$Date,2)\nVac_DF$Date <- ymd(Vac_DF$Date )\nhead(Vac_DF)##     State       Date Vaccinated Doses Population\n## 1 Alabama 2022-01-01          5624234    4903185\n## 2 Alabama 2022-01-02          5624234    4903185\n## 3 Alabama 2022-01-03          5624234    4903185\n## 4 Alabama 2022-01-04          5678299    4903185\n## 5 Alabama 2022-01-05          5681793    4903185\n## 6 Alabama 2022-01-06          5695747    4903185\nclass(Vac_DF$Date)## [1] \"Date\""},{"path":"preprocessing-and-visualization-of-time-series-data.html","id":"deal-with-missing-values","chapter":"22 Preprocessing and Visualization of Time Series Data","heading":"22.1.2 Deal with Missing Values","text":"moving , first check missing values.found missing values, need worry missing values case.missing values example case, two choices, either impute missing values remove . make future analysis accurate, better try impute . Since data time series data, vaccinated doses day 1 greater doses day 2, conversely, vaccinated doses day 2 less doses day 1, impute missing value closest non missing value, achieve fill function tidyr package. function impute missing value previous next value, argument .direction can define direction impute.Since missing values, can move deal problematic data.","code":"\nsum(is.na(Vac_DF$`Vaccinated Doses`))## [1] 0"},{"path":"preprocessing-and-visualization-of-time-series-data.html","id":"deal-with-problematic-data-points","chapter":"22 Preprocessing and Visualization of Time Series Data","heading":"22.1.3 Deal with Problematic Data Points","text":"moving visualization analysis, need careful whether data cumulative ! data cumulative, value monotonically, example case, since data vaccination doses state day, vaccinated doses day 2 less value day 1. need check whether data day 2 smaller value day 1, assign value day 2 value day 1.Now finished preprocessings. brief conclusion, find dataset online, first load transform usable dataframe. usually requires functions dplyr working time series data, also need functions lubridate make date date type variable instead character can order later visualization. imputing missing values, tidyr popular package use since impute missing value value nearest date. need look data points, deal problematic data points according dataset , example case, working vaccinated doses along time, must monotonically increasing variable. finishing steps, move visualizing data.","code":"\nfor (i in 1:length(state)){\n  for (j in 2:time_window){\n    if ((Vac_DF %>% subset(State == state[i]) %>% select(`Vaccinated Doses`))[j,] < \n        (Vac_DF %>% subset(State == state[i]) %>% select(`Vaccinated Doses`))[j-1,]){\n      Vac_DF[\"Vaccinated Doses\"][Vac_DF[\"State\"] == state[i]][j] <- Vac_DF[\"Vaccinated Doses\"][Vac_DF[\"State\"] == state[i]][j-1]\n    }\n  }\n}"},{"path":"preprocessing-and-visualization-of-time-series-data.html","id":"visualize-a-time-series-data","chapter":"22 Preprocessing and Visualization of Time Series Data","heading":"22.2 Visualize a Time Series Data","text":"visualize time series data, mainly focus trend. want see data changes time, like geom_line ggplot2 show changes time state.graph give us much informaiton tell state, make graph varries color according states command color = State.graph looks much better now can find line corresponded state color. can easily find state highest vaccinated doses California. since California large vaccinated doses, range plot great make trend clear. may want check states’ population. get deeper insight, can visualize population state using geom_bar.bar plot give much insights without ordering, order easily find populatioin order state, can order plot using factor().ordering barplot, find California highest population among states result vaccinated doses greatest amount California give much information California great population . order get deeper insights, can visualize vaccinated doses rate, divide vaccinated doses population. Since like see trends time difference state, use geom_line() color state.Now can get better look find District Columbia highest vaccinated doses rate proves important us look vaccinated doses rate instead absolute vaccinated doses since population affect results.Now got great visualization time series data.","code":"\nggplot(data = Vac_DF, mapping = aes(x = Date, y = `Vaccinated Doses`, group = State)) +\n  geom_line() +\n  ggtitle(\"Vaccinated Doses V.S. Date\")\nggplot(data = Vac_DF, mapping = aes(x = Date, y = `Vaccinated Doses`, group = State, color = State)) +\n  geom_line() +\n  ggtitle(\"Vaccinated Doses V.S. Date\")\nggplot(data = Vac_DF, mapping = aes(x= Population, y = State)) + \n  geom_bar(stat=\"identity\")\norder <- unique(Vac_DF %>% select(Population, State) %>% arrange(Population) %>% mutate(State = factor(State)))\nPop_Order_DF <- Vac_DF %>% mutate(State = factor(State, levels = order$State, ordered = TRUE))\nggplot(data = Pop_Order_DF, mapping = aes(x= Population, y = State)) + \n  geom_bar(stat=\"identity\")\nVac_Rate_DF <- Vac_DF %>% mutate(`Vaccinated Doses Rate` = `Vaccinated Doses`/Population)\n\nggplot(data = Vac_Rate_DF, mapping = aes(x = Date, y = `Vaccinated Doses Rate`, group = State, color = State)) +\n  geom_line() +\n  ggtitle(\"Vaccinated Doses Rate V.S. Date\")"},{"path":"preprocessing-and-visualization-of-time-series-data.html","id":"conclusion","chapter":"22 Preprocessing and Visualization of Time Series Data","heading":"22.3 Conclusion","text":"conclusion, dealing time series data, need spend time preprocessing, including transforming date usable dataframe using dplyr tidyrpackage, dealing missing values, problematic data. transforming data, important watch date type! need date class instead character, can order visualizing, date format can converted functions lubridate package. finish theses preprocessing works, can start visualization part using ggplot2, try remove potential affecting factors order get clear look data trend time.","code":""},{"path":"how-to-use-sqldf.html","id":"how-to-use-sqldf","chapter":"23 How to use sqldf","heading":"23 How to use sqldf","text":"Conor Ryan","code":"\nlibrary(sqldf)\nlibrary(tidyverse)"},{"path":"how-to-use-sqldf.html","id":"motivation-2","chapter":"23 How to use sqldf","heading":"23.1 Motivation","text":"sqldf library lets work dataframes database tables, can query whatever SQL-style manipulation want, without worrying logistics managing databases. can useful various dataframe manipulations, often need preparing data visualization.reasons thought library use tutorial:Github page kind mess, official CRAN documentation particularly user-friendly.pretty cool tool think : overhead extra work, can just call SQL dataframe.option use SQL incredibly useful dealing working many languages. need quick R visualization just working Python, might easier just manipulate data via SQL rather figure exact R syntax thing.Certain data manipulation just suited SQL syntax, like complicated left joins window functions.scale data gets large memory, library offers impressive advantages. Even can load large dataset memory, slow; way faster initial manipulation (like filtering data 100-fold) library, reasonable deal resulting dataframe.approach significant improvement something like dbplyr knitr SQL engine. approach still requires manual management connections tables. Additionally, knitr hardly suited non-report-style work R. sqldf usable wider variety scenarios.","code":""},{"path":"how-to-use-sqldf.html","id":"usage","chapter":"23 How to use sqldf","heading":"23.2 Usage","text":"","code":""},{"path":"how-to-use-sqldf.html","id":"basics","chapter":"23 How to use sqldf","heading":"23.2.1 Basics","text":"’ve installed sqldf, really easy loading library writing SQL:create database, load data table, cleanup table. package handled behind scenes.can realistic, basic manipulation. R might :SQL can :","code":"\nsqldf('select * from iris') |> head()##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\niris |>\n  filter(Petal.Length > 2.0) |>\n  mutate(Sepal_Product = Sepal.Length * Sepal.Width) |>\n  group_by(Species) |>\n  summarize(mean_sepal_product=mean(Sepal_Product)) |>\n  head()## # A tibble: 2 × 2\n##   Species    mean_sepal_product\n##   <fct>                   <dbl>\n## 1 versicolor               16.5\n## 2 virginica                19.7\nsqldf('\n  select Species, avg(`Sepal.Length` * `Sepal.Width`) as mean_sepal_product\n  from iris\n  where `Petal.Length` > 2.0\n  group by 1\n') |>\n  head()##      Species mean_sepal_product\n## 1 versicolor            16.5262\n## 2  virginica            19.6846"},{"path":"how-to-use-sqldf.html","id":"more-advanced-sql-tasks","chapter":"23 How to use sqldf","heading":"23.2.2 More advanced SQL tasks","text":"library becomes powerful use things SQL uniquely good . example, although simple join matching column condition relatively easy R (Python), following sort condition annoying accomplish:Similarly, window functions become far accessible package:unique tasks might even preferable just use SQL rather R dataframe manipulation. fine; every tool can everything prefectly – SQL excels specific things.","code":"\nsqldf('\n  select a.Species, b.Species, avg(a.`Sepal.Width`) as `a.Width.Avg`\n  from iris a\n  join iris b\n    on a.species != b.species\n    and a.`Sepal.Length` > b.`Sepal.Length`\n    and a.`Sepal.Width` < b.`Sepal.Width`\n  group by 1,2\n') |>\n  head()##      Species    Species a.Width.Avg\n## 1 versicolor     setosa    2.764037\n## 2 versicolor  virginica    2.738889\n## 3  virginica     setosa    2.901931\n## 4  virginica versicolor    2.737284\nsqldf('\n  select Species, avg(`Sepal.Length`) over (partition by Species order by `Sepal.Length` desc rows between unbounded preceding and current row) as running_mean\n  from iris\n') |>\n  head()##   Species running_mean\n## 1  setosa     5.800000\n## 2  setosa     5.750000\n## 3  setosa     5.733333\n## 4  setosa     5.675000\n## 5  setosa     5.640000\n## 6  setosa     5.600000"},{"path":"how-to-use-sqldf.html","id":"alternate-data-sources","chapter":"23 How to use sqldf","heading":"23.2.3 Alternate data sources","text":"also don’t already dataframe -memory use library. Suppose iris .csv machine:wanted immediately get memory rows filtered :great didn’t ever “useless” version dataframe ever code; immediately get version filtering done., data lives .csv remote host?Hopefully can see options powerful. Although iris small, sometimes data large, may want deal loading many millions rows R going filter anyway. example later Performance section.","code":"\n# disabled because we were asked to not write any data\nwrite.table(iris, 'iris.csv', sep = \",\", quote = FALSE, row.names = FALSE)\n# disabled because we were asked to not write any data\nread.csv.sql('iris.csv',  sql = 'select * from file where \"Petal.Length\" > 2.0') |>\n  head()\nread.csv.sql(\n  'https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv',\n  sql = 'select * from file where \"Petal.Length\" > 2.0'\n) |>\n  head()##   sepal.length sepal.width petal.length petal.width      variety\n## 1          7.0         3.2          4.7         1.4 \"Versicolor\"\n## 2          6.4         3.2          4.5         1.5 \"Versicolor\"\n## 3          6.9         3.1          4.9         1.5 \"Versicolor\"\n## 4          5.5         2.3          4.0         1.3 \"Versicolor\"\n## 5          6.5         2.8          4.6         1.5 \"Versicolor\"\n## 6          5.7         2.8          4.5         1.3 \"Versicolor\""},{"path":"how-to-use-sqldf.html","id":"advanced-database-usage","chapter":"23 How to use sqldf","heading":"23.2.4 Advanced database usage","text":"hood, sqldf actually loads dataframe temporary database table. want, can also manage database intelligently. contrived use case, worth knowing. Suppose ’re dealing lot data plan two subsequent queries. better read dataframe table reuse table. can accomplished via:can also just pass database administrative command function well. example, manage entire database (create new schemas, tables, adjust permissions) really wanted . Although appropriate tool might worth considering.","code":"\nsqldf() # keep iris as a table in the db## <SQLiteConnection>\n##   Path: :memory:\n##   Extensions: TRUE\nsqldf('select * from iris') |> # iris now loaded as a table. can reuse it.\n  head()##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\nsqldf() # connection closed and iris table deleted## NULL"},{"path":"how-to-use-sqldf.html","id":"performance","chapter":"23 How to use sqldf","heading":"23.3 Performance","text":"Certain tasks actually end optimal done sqldf. example, following arbitrarily large join, fans nearly billion rows, took roughly 30 seconds laptop finish. (Although task nonsense, real data, one may come across use case actually needs something similar.)following R equivalent, third ‘join’, took longer four took SQL. preserve computer, attempt fourth merge (commented ).library also becomes helpful dealing large datasets disk. example, arbitrary .csv ~600MB (include file project, feel free try large file). require load entire file R first, finished 30 seconds.worthwhile improvement; R equivalent completed 45 seconds.difference becomes meaningful dataset’s size increases relative machine’s memory. file several GB, preprocessing temporary database table becomes increasingly efficient relative pure R. observed marginal version optimization . functionality especially useful know never need refer back full file (meaning use e.g. transformed version ).","code":"\nsqldf('\n  select count(*)\n  from iris a\n  join iris b using (species)\n  join iris c using (species)\n  join iris d using (species)\n  join iris e using (species)\n')##    count(*)\n## 1 937500000\nmerge(iris, iris, by=\"Species\") |>\n  merge(iris, by=\"Species\") |>\n  merge(iris, by=\"Species\") |>\n  #merge(iris, by=\"Species\") |>\n  nrow()## [1] 18750000\n# disabled because I cannot provide this (intentionally) large file\nread.csv.sql(\n  '~/Downloads/star2002-1.csv',\n  sql='select `X1`, avg(`X807`) from file where `X4518` > 5500 group by 1'\n) |>\n  head()\n# disabled because I cannot provide this (intentionally) large file\nread.csv(file = '~/Downloads/star2002-1.csv') |>\n  filter(X4518 > 5500) |>\n  group_by(X1) |>\n  summarize(some_avg=mean(X807)) |>\n  head()"},{"path":"how-to-use-sqldf.html","id":"combining-with-ggplot","chapter":"23 How to use sqldf","heading":"23.4 Combining with ggplot","text":"can also combine sqldf manipulations ggplot easily make visualizations. use sqldf scenarios excels , outlined , becomes powerful. infinite combinations , one simple illustrative example:","code":"\nsqldf('\n      select Species, avg(`Sepal.Width`) as avg_width\n      from iris\n      group by 1\n') |>\n  ggplot(aes(x=reorder(Species, avg_width), y=avg_width)) +\n  geom_bar(stat='identity') +\n  coord_flip() +\n  xlab('Species')"},{"path":"how-to-use-sqldf.html","id":"conclusion-1","chapter":"23 How to use sqldf","heading":"23.5 Conclusion","text":"sqldf important option available manipulating data. clear, replacement knowing use R general. One restrict using sqldf uniquely advantageous SQL-style work, don’t want deal writing perfect R. guide useful anyone new library exactly scenarios one might opt use .Personally, ’m glad chose deep dive library create guide. educational many ways, like: learning databases, understanding R can blend SQL, elucidating things R vs. SQL excel . certainly referring back document, think makes easy review exactly quickly use library, without getting --weeds nuts bolts (much existing documentation , opinion). One thing ’d like dedicate effort next time exactly replicating complex SQL commands R; knowing likely useful point, even practical right now. Finally, wish known library sooner, know sure optimize parts workflow going foward.","code":""},{"path":"how-to-use-sqldf.html","id":"references","chapter":"23 How to use sqldf","heading":"23.6 References","text":"https://github.com/ggrothendieck/sqldfhttps://cran.r-project.org/web/packages/sqldf/index.htmlhttps://www.geeksforgeeks.org/window-functions--sql/","code":""},{"path":"googlevis-in-r.html","id":"googlevis-in-r","chapter":"24 googleVis in R","heading":"24 googleVis in R","text":"Sushant Prabhu Kiyan Mohebbizadeh","code":""},{"path":"googlevis-in-r.html","id":"introducing-googlevis","chapter":"24 googleVis in R","heading":"24.1 Introducing googleVis","text":"GoogleVis package R allows users R use Google Charts API.interface R Google Charts allows users access Google Charts’ interactive charts. googleVis allows users use data R data frames create Google Charts without uploading data onto Google.Demonstrating using googleVis Library - Installation Usage","code":"install.packages('googleVis')\nlibrary(googleVis)"},{"path":"googlevis-in-r.html","id":"why-use-googlevis","chapter":"24 googleVis in R","heading":"24.2 Why use googleVis ?","text":"googleVis package allows users create interactive visualizations R’s popular visualization package (ggplot) allow.Although packages work conjunction ggplot make interactive visualizations, googleVis offers holistic package allows unique interactive visualizations.using Google Charts, one able create wide variety visualizations ranging typical bar line graphs mapping timeline charts one package.visualizations created googleVis add level interest consumer due interactive layer viewers able gather specific bits information hovering clicking values visualizations. allows increased aesthetics, also information transferred viewers.","code":""},{"path":"googlevis-in-r.html","id":"googlevis-rendering-interaction","chapter":"24 googleVis in R","heading":"24.3 googleVis Rendering & Interaction","text":"output googleVis can either embedded HTML file read dynamically. visualizations often rendered online web format. Therefore, browser internet connection required view interactive version output compared ggplotFor use R, googleVis allows user render Shiny file allows preview interaction within R. used preview chart final render.googleVis package R allows users R use Google Charts API.interface R Google Charts allows users access Google Charts’ interactive charts.googleVis allows users use data R data frames create Google Charts without uploading data onto Google.","code":""},{"path":"googlevis-in-r.html","id":"basic-graphs-line-bar-combo","chapter":"24 googleVis in R","heading":"24.3.1 Basic Graphs (Line, Bar, Combo)","text":"charts best used comparisons groups. seen examples, comparisons costs owning different pets.line graph shows different variables flow within among groups. audience able determine within group trends seeing lines intersect within group. Showing trends variables lines groups allows us make comparisons among various groups clarity.organizing variables certain way, one able get sense population trends.bar column chart essentially just rotated axis. allow great group comparisons well comparisons among groups. However, charts best used -group comparisons.Combo charts great multiple variable comparisons allow user get best worlds. carefully selecting variables represented bars ones lines, user able best show relationship within groups trends population.","code":"\ndf = data.frame(pet=c('cat', 'dog', 'hamster', 'snake'),\n                food_cost_monthly=c(50, 100, 10, 40),\n                medical_cost_monthly=c(30, 60, 5, 50))\nLine <- gvisLineChart(df)\n\nBar <- gvisBarChart(df)\n\nColumn <- gvisColumnChart(df)\n\nSteppedArea <- gvisSteppedAreaChart(df, xvar=\"pet\", \n                                    yvar=c(\"food_cost_monthly\", \"medical_cost_monthly\"),\n                                    options=list(isStacked=TRUE))\n\nCombo <- gvisComboChart(df, xvar=\"pet\",\n                        yvar=c(\"food_cost_monthly\", \"medical_cost_monthly\"),\n                        options=list(seriesType=\"bars\",\n                                     series='{1: {type:\"line\"}}'))\nplot(Line)\nplot(Bar)\nplot(Column)\nplot(SteppedArea)\nplot(Combo)"},{"path":"googlevis-in-r.html","id":"googlevis-histogram-chart","chapter":"24 googleVis in R","heading":"24.3.2 googleVis Histogram Chart","text":"histogram allows users represent distribution one particular group variable showing frequency particular group variable within range. charts googleVis advantage regular histograms almost histogram allows recommends specific information regarding counts different points visualization, however, googleVis audience can look distribution access specific metrics interaction well.","code":"\ndf <- iris\nHistogram <- gvisHistogram(data.frame(Sepal_Width = df$Sepal.Width))\n\nplot(Histogram)"},{"path":"googlevis-in-r.html","id":"googlevis-alluvialsankey-chart","chapter":"24 googleVis in R","heading":"24.3.3 googleVis Alluvial/Sankey Chart","text":"Alluvial charts best show movement sample population among different variables. example movement students within school class class represented. visualization can helpful data ordinal timeline specific values. googleVis, audience exposed general trends clean looking chart well specifics graph interaction.","code":"\ndf <- data.frame(From=c(rep(\"Math\",3), rep(\"Science\", 3)),\n                    To=c(rep(c('Lunch', 'Art', 'Music'),2)),\n                    Weight=c(17,15,13,5,12,8))\n\nAlluvial <- gvisSankey(df, from=\"From\", to=\"To\", weight=\"Weight\")\n\nplot(Alluvial)"},{"path":"googlevis-in-r.html","id":"googlevis-geographic-chart","chapter":"24 googleVis in R","heading":"24.3.4 googleVis Geographic Chart","text":"Map visualizations googleVis incredibly easy create manipulate. useful comparing different geographic areas . googleVis automatically color scales values interaction allows map simple clean, get specific values hovering particular geographic area.","code":"\ndf = data.frame(country=c('US', 'CN', 'BR', 'IS', 'RU', 'TH', 'TR', 'ID', 'MX', 'IR' ),\n                incarceration_rate = c(2068800, 1690000, 811707, 478600, 471490, 309282, 291198, 266259, 220866, 189000))\n\nG <- gvisGeoChart(df, locationvar = \"country\", colorvar = \"incarceration_rate\",\n                  options=list(\n                         gvis.editor=\"Edit the Geo Chart !\"))\n\nplot(G)"},{"path":"googlevis-in-r.html","id":"googlevis-gauge-chart","chapter":"24 googleVis in R","heading":"24.3.5 googleVis Gauge Chart","text":"gauge charts interactive, however offer unique way model data always within certain range. example, temperatures, speeds, pressure, etc. chart allows quick comparison groups aesthetic value presentation.","code":"\ntemperature <- data.frame(city=c('Las Vegas', 'Los Angeles', 'Pheonix', 'Dallas', 'Houston', 'Miami'),\n                          temp=c(115, 103, 120, 110, 112, 101))\nGauge <-  gvisGauge(temperature, \n                    options=list(min=0, max=150, greenFrom=0,\n                                 greenTo=50, yellowFrom=50, yellowTo=100,\n                                 redFrom=100, redTo=150, width=400, height=300))\n\nplot(Gauge)"},{"path":"googlevis-in-r.html","id":"googlevis-tabular-chart","chapter":"24 googleVis in R","heading":"24.3.6 googleVis Tabular Chart","text":"data formatted table can paged sorted. flexible option select single rows either keyboard mouse. also powers sorting rows across dimensions columns dataset. navigation paged tabular information smooth simple.","code":"\n## Tabular Data Un-Paged\nPopulation_Tabular_Unpaged <- gvisTable(Population[1:30,],\n                                        formats=list(Population=\"#,###\",'% of World Population'='#.#%'))\n\nplot(Population_Tabular_Unpaged)\n## Tabular Data Paged\nPopulation_Tabular_paged <- gvisTable(Population[1:30,], \n                                      formats=list(Population=\"#,###\",'% of World Population'='#.#%'),\n                                      options=list(page='enable',\n                                                   height='automatic',\n                                                   width='automatic'))\n\nplot(Population_Tabular_paged)"},{"path":"googlevis-in-r.html","id":"googlevis-tree-map-chart","chapter":"24 googleVis in R","heading":"24.3.7 googleVis Tree Map Chart","text":"googleVis tree map visual representation data tree, node 0 children, 1 parent barring root node. One can specify many levels display simultaneously, optionally display deeper levels. One can move tree person left-clicks node, moves back tree person right-clicks graph.total size graph determined size elements contained graph.googleVis tree map chart captures relative sizes data categories, helps quick insight datapoints bigger contributors category. Color helps scrutinize datapoints underperforming / overperforming) compared siblings category.","code":"\nCountry_Tree <- gvisTreeMap(Regions, \"Region\", \"Parent\", \"Val\", \"Fac\", \n                     options=list(width=800, height=500, fontSize=15,\n                                  minColor='#cfe2f3',midColor='#6fa8dc',maxColor='#0b5394',\n                                  headerHeight=10,fontColor='black',showScale=TRUE))\n\nplot(Country_Tree)"},{"path":"googlevis-in-r.html","id":"googlevis-annotation-chart","chapter":"24 googleVis in R","heading":"24.3.8 googleVis Annotation Chart","text":"Annotation charts useful, interactive time series like line charts enable annotations.annotated charts leveraged highlight specific data value-add contextual notes within visualization.answer “?” kind questions, well defined annotations highlight significance data chart, keen detail textual description / annotation.One can also slice interactive timeline chart look snapshot data aesthetically pleasing also provides great detail insights within visualization. annotation charts SVG (scalable vector graphics) /VML (vector graphics rendering ).","code":"\nStock_Annotation <- gvisAnnotationChart(Stock, datevar=\"Date\",numvar=\"Value\", idvar=\"Device\", titlevar=\"Title\",\n                                        annotationvar=\"Annotation\",\n                                        options=list(displayAnnotations=TRUE,\n                                        chart=\"{chartArea:{backgroundColor:'#ebf0f7'}}\",\n                                        legendPosition='newRow',width=800, height=450,\n                                        scaleColumns='[0,1]',scaleType='allmaximized'))\n\nplot(Stock_Annotation)"},{"path":"googlevis-in-r.html","id":"googlevis-calendar-chart","chapter":"24 googleVis in R","heading":"24.3.9 googleVis Calendar Chart","text":"googleVis calendar chart definitive visualization can used show activity course longer duration time, example months years decades. One can illustrate variation 1 quantity depending days given week, trends timeline period.calendar charts demonstrate data records, events, daily, weekly, monthly, yearly calendar. highly interactive one can view value hovering particular time entire timeperiod.","code":"\nCalendar_Temp <- gvisCalendar(Cairo, datevar=\"Date\", numvar=\"Temp\",\n                    options=list(title=\"Cairo's variation in Daily\n                                 temperature\",height=400,width=1000,\n                                 calendar=\"{yearLabel: { fontName:'sans-serif',\n                                 fontSize: 20, color: 'black', bold: true},\n                                 cellSize: 10,cellColor:{stroke: 'black', strokeOpacity: 0.2},\n                                 focusedCellColor: {stroke:'red'}}\"), chartid=\"Calendar\")\n\nplot(Calendar_Temp)"},{"path":"googlevis-in-r.html","id":"googlevis-timeline-chart","chapter":"24 googleVis in R","heading":"24.3.10 googleVis Timeline Chart","text":"googleVis Timeline chart great fascinating way visualizing different dates / events. example, showing duration Presidents & Vice Presidents / Sessions Congress timeline period. exact times durations given one interactively hovers bars.timeline charts versatile visuals illustrating sequence events chronologically. provides amazing aid conceptualize event sequences / processes gain valuable insights, sometimes maybe summarize historical events, time frame minutes, hours, years datewise.","code":"\nPosition_Timeline_Data <- data.frame(Position=c(rep(\"President\", 4), rep(\"Vice\", 4)),\n                    Name=c(\"William Clinton\",\"George Bush\", \"Barack Obama\", \"   Donald Trump\",\n                          \" Albert Gore\",\"Dick Cheney\", \"Biden, Jr.\", \"Michael Pence\"),\n                    start=as.Date(x=rep(c(\"1993-01-20\",\"2001-01-20\", \"2009-01-20\",\"2017-01-20\"),2)),\n                    end=as.Date(x=rep(c(\"2001-01-20\",\"2009-01-20\", \"2017-01-20\", \"2021-01-20\"),2)))\n\nTimeline <- gvisTimeline(data=Position_Timeline_Data, \n                         rowlabel=\"Name\",\n                         barlabel=\"Position\",\n                         start=\"start\", \n                         end=\"end\",\n                         options=list(timeline=\"{groupByRowLabel:false}\",\n                                      backgroundColor='#e3f4ff', \n                                      height=400,colors=\"['#0e407d', '#78b2ff', '#3737ab']\"))\n\nplot(Timeline)"},{"path":"googlevis-in-r.html","id":"googlevis-gantt-chart","chapter":"24 googleVis in R","heading":"24.3.11 googleVis Gantt Chart","text":"googleVis Gantt charts help teams plan work around deadlines allocate resources efficiently.Project planners also leverage Gantt charts maintain bird’s eye high level view projects track . depict relationship start end dates tasks, milestones, dependent tasks entire timeline project. Gantt chart illustrates breakdown project component tasks effectively.","code":"\ndaysToMilliseconds <- function(days){days * 24 * 60 * 60 * 1000}\ndat <- data.frame(taskID = c(\"PS\", \"EDA\", \"R\", \"ML\", \"DP\"),\n                 taskName = c(\"Identify Problem Statement\", \"EDA Analysis\", \"Research\",\n                              \"Machine Learning Modelling\", \"Data Preprocessing\"),\n                 resource = c(NA, \"write\", \"write\", \"complete\", \"write\"),\n                 start = c(as.Date(\"2022-10-01\"), NA, as.Date(\"2022-10-02\"), as.Date(\"2022-10-08\"), NA),\n                 end = as.Date(c(\"2022-10-04\", \"2022-10-08\", \"2022-10-08\",\n                                 \"2022-10-13\", \"2022-10-05\")),\n                 duration = c(NA, daysToMilliseconds(c(3, 1, 1, 1))),\n                 percentComplete = c(100, 25, 20, 0, 100),\n                 dependencies = c(NA, \"PS, DP\", NA,\n                 \"EDA\", \"PS\"))\n\nGantt_Tasks <- gvisGantt(dat, taskID = \"taskID\",taskName = \"taskName\",resource = \"resource\",\n                         start = \"start\",end = \"end\",duration = \"duration\",percentComplete = \"percentComplete\",\n                         dependencies = \"dependencies\",\n                         options = list(height = 300,\n                         gantt = \"{criticalPathEnabled:true,innerGridHorizLine: {\n                         stroke: '#e3f4ff',strokeWidth: 2},innerGridTrack: {fill: '#e8f3fa'},innerGridDarkTrack:\n                         {fill: '#c7e9ff'},labelStyle: {fontName: 'sans-serif',fontSize: 16}}\"))\n\nplot(Gantt_Tasks)"},{"path":"googlevis-in-r.html","id":"googlevis-merging-charts","chapter":"24 googleVis in R","heading":"24.3.12 googleVis Merging Charts","text":"googleVis Merge chart provides flexibility merging two gvis-objects, either next one gvis-object. objects arranged HTML table format.multiples charts view allows split individual charts Bar, Column, Line Geographic, Tabular etc. charts multiple charts, separated. numerous use cases like showing product sales per region providing information . gives lot flexibility report creation delivery aesthetically.","code":"\nGeographic <- gvisGeoChart(Exports,\n                           locationvar=\"Country\",colorvar=\"Profit\",\n                           options=list(width=400, height=200))\n\nTabular <- gvisTable(Exports,\n                     options=list(width=400, height=400))\n\nMerged_Charts <- gvisMerge(Geographic, Tabular, horizontal=FALSE, tableOptions=\"bgcolor=\\\"#7cdeb5\\\"\")\n\nplot(Merged_Charts)"},{"path":"googlevis-in-r.html","id":"use-googlevis-in-rstudio","chapter":"24 googleVis in R","heading":"24.4 Use googleVis in RStudio","text":"Using googleVis RStudio straightforward. default, RStudio renders charts new webpage -hand, view within RStudio,\n> View RStudio Viewer just use view locallyTo Knit Rmd Markdown file HTML, perform following command set Chunk option results asis {r ChartExample, results='asis', tidy=FALSE} plot(Chart, 'chart')","code":"plot(Chart)plot(Chart, browser=rstudioapi::viewer)"},{"path":"googlevis-in-r.html","id":"googlevis-references","chapter":"24 googleVis in R","heading":"24.4.1 googleVis References","text":"DocumentationGoogle ChartsDemoPaperCRAN-Stable VersionThank learning googleVis us !","code":""},{"path":"tutorial-for-vector-fields-in-r.html","id":"tutorial-for-vector-fields-in-r","chapter":"25 Tutorial for vector fields in r","heading":"25 Tutorial for vector fields in r","text":"Sebastian Steiner & Elliot Frank","code":""},{"path":"tutorial-for-vector-fields-in-r.html","id":"getting-started-with-vector-fields","chapter":"25 Tutorial for vector fields in r","heading":"25.1 Getting Started with Vector Fields","text":"Vector field graphs number important applications throughout science,\nengineering, math. tutorial, ’ll explain basic components \nvector field graphs, build R.research, documentation available use improvement, thus \ngoal provide accessible explanation build vector graphs R. \nkeep things clear, ’ll build example data set go, explaining \nrequired data components vector field graphs. Hopefully, reading \ntutorial, ’ll able easily put together vector graphs using \ndata sets.example data set, observation represents one arrow, vector, \nplotted graph. vectors communicate movement, arrow \nstarting point end point. dive , ’ve listed four\ndata columns required building vector graph, build\nexample data set.x_axis: horizontal value starting point given vectory_axis: vertical value starting point given vectorx_pull: strength force pulling vector horizontal directiony_pull: strength force pulling vector vertical directionFirst, require two data columns serving x y coordinates \nplacement arrow graph. initial coordinates, \ncall x_axis y_axis, place base arrow vector\ngraph. Stated otherwise, together x_axis y_axis columns provide \nstarting point arrow graph.code , start building example data set placing base\nvector positive integer values 10 X 10 grid, assigning\nvalues x_axis y_axis columns.graphing plot, ’ve assigned x_axis y_axis values axis values\nentire chart, , columns place arrow. geom_segment\nfunction plots lines, ’ve added endpoint 0.05 x_axis \nbase vector can seen .plotting starting points vectors, now need \ndetermine vectors end. determined \nx_pull y_pull columns. ’s important note columns\nprovide end coordinates, measure directions \nvector pulled. x_pull y_pull variables indicate far\nbase arrow extend given direction.’ll notice code , x_pull y_pull values added\nstarting point values (x_axis & y_axis) geom_segment function.\nvariables within ‘aes’, dictate end points vector, conveniently\nnamed ‘xend’ ‘yend’. altering ‘xend’ ‘yend’ values, place\ncoordinates vector’s endpoint.example, set y_pull equal 0.5, ’ll notice \narrows pulled upward direction.Conversely, set x_pull equal 0.5, ’ll notice \narrows pulled right direction.set x_pull y_pull equal 0.5, x y forces \noffsetting, arrows point 45 degree angle.provide another example, ’ve input random numbers x_pull y_pull\nvalues, show format completely flexible doesn’t require\nconsistent value changes x_pull y_pull variables.","code":"\n# Creating a blank data frame with four required columns\ndata_frame = data.frame(x_axis = numeric(), y_axis = numeric())\n\n# Generating evenly distributed values for x y coordinates\nfor(i in 1:10) {\n  for(j in 1:10) {\n    vec <- c(i, j) \n    data_frame[nrow(data_frame) + 1, ] <- vec\n  }\n}\n\n# Plotting points for illustrative purposes\nggplot(data_frame, aes(x = x_axis, y = y_axis)) +\n    scale_x_continuous(breaks = seq(0,10,1)) + \n    scale_y_continuous(breaks = seq(0,10,1)) +\n    geom_segment(aes(xend = x_axis+(0.05), yend = y_axis)) +\n    coord_fixed()\ndata_frame$x_pull <- 0\ndata_frame$y_pull <- 0.5\n\n# Plotting points for illustrative purposes\nggplot(data_frame, aes(x = x_axis, y = y_axis)) +\n    scale_x_continuous(breaks = seq(0,10,1)) + \n    scale_y_continuous(breaks = seq(0,10,1)) +\n    geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = y_axis + (y_pull)), \n                 arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n    coord_fixed()\ndata_frame$x_pull <- 0.5\ndata_frame$y_pull <- 0\n\n# Plotting points for illustrative purposes\nggplot(data_frame, aes(x = x_axis, y = y_axis)) +\n    scale_x_continuous(breaks = seq(0,10,1)) + \n    scale_y_continuous(breaks = seq(0,10,1)) +\n    geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = y_axis + (y_pull)), \n                 arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n    coord_fixed()\ndata_frame$x_pull <- 0.5\ndata_frame$y_pull <- 0.5\n\nggplot(data_frame, aes(x = x_axis, y = y_axis)) +\n    scale_x_continuous(breaks = seq(0,10,1)) + \n    scale_y_continuous(breaks = seq(0,10,1)) +\n    geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = y_axis + (y_pull)), \n                     arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n    coord_fixed()\ndata_frame$x_pull <- runif(nrow(data_frame), min=-0.5, max=0.5)\ndata_frame$y_pull <- runif(nrow(data_frame), min=-0.5, max=0.5)\n\nggplot(data_frame, aes(x = x_axis, y = y_axis)) +\n    scale_x_continuous(breaks = seq(0,10,1)) + \n    scale_y_continuous(breaks = seq(0,10,1)) +\n    geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = y_axis + (y_pull)), \n                     arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n    coord_fixed()"},{"path":"tutorial-for-vector-fields-in-r.html","id":"tips-and-tricks-to-plotting-vector-fields","chapter":"25 Tutorial for vector fields in r","heading":"25.2 Tips and Tricks to Plotting Vector Fields","text":"Now ’ve covered basics, ’ll provide guidance make\nhigh-quality vector field graphs.","code":""},{"path":"tutorial-for-vector-fields-in-r.html","id":"understanding-arrow-options","chapter":"25 Tutorial for vector fields in r","heading":"25.2.1 Understanding arrow options","text":"plotting arrows geom_segment, can control features arrow\nfollowing line:length = unit(0.1, “cm”) defines size arrow headsize = 0.25  defines arrow thicknessFor example, set 1, get following:arrow adjustments produce low-quality plot, highlight options\none represent arrows vector fields.","code":"arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25)\ndata_frame$x_pull <- 0.5\ndata_frame$y_pull <- 0.5\n\nggplot(data_frame, aes(x = x_axis, y = y_axis)) +\n    scale_x_continuous(breaks = seq(0,10,1)) + \n    scale_y_continuous(breaks = seq(0,10,1)) +\n    geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = y_axis + (y_pull)), \n                     arrow = arrow(length = unit(1, \"cm\")), size = 1) +\n    coord_fixed()"},{"path":"tutorial-for-vector-fields-in-r.html","id":"axis-scaling","chapter":"25 Tutorial for vector fields in r","heading":"25.2.2 Axis Scaling","text":"Vector fields commonly represent flows space. Therefore, moving 1 unit \nx-direction distance moving 1 unit y-direction. 1:1\nratio preserved, vector field becomes distorted difficult \ninterpret. Therefore, axis ggplot must fixed using following code:illustration discussion - note cases, vectors \nform (0.5,0.5). distortion appears small, easily avoidable\npreserves accurate relationship x y axes.","code":"coord_fixed()\n#Creating homogeneous vector field (1,1)\ndata_frame$x_pull <- 0.5\ndata_frame$y_pull <- 0.5\n\n#No coord_fixed\nggplot(data_frame, aes(x = x_axis, y = y_axis)) +\n  scale_x_continuous(breaks = seq(0,10,1)) + \n  scale_y_continuous(breaks = seq(0,10,1)) +\n  geom_segment(aes(xend = x_axis + (x_pull), \n                  yend = y_axis + (y_pull)), \n                  arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25)  +\n  ggtitle(\"Distorted plot without coord_fixed()\")\n#Coord_fixed\nggplot(data_frame, aes(x = x_axis, y = y_axis)) +\n        scale_x_continuous(breaks = seq(0,10,1)) + \n        scale_y_continuous(breaks = seq(0,10,1)) +\n        geom_segment(aes(xend = x_axis + (x_pull), \n                        yend = y_axis + (y_pull)), \n                        arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n        coord_fixed() +\n        ggtitle(\"High-quality plot with coord_fixed()\")"},{"path":"tutorial-for-vector-fields-in-r.html","id":"arrow-length","chapter":"25 Tutorial for vector fields in r","heading":"25.2.3 Arrow Length","text":"important consider length arrows plotting vector fields, \ninput data may cause arrows overlap, making plot difficult interpret\n(see example ).can see, even simple plot vectors pointing \ndirection, overlapping arrows makes impossible see origin point \narrows middle. Therefore, recommend scaling x_pull y_pull vectors,\nshown (note good practice scale x_pull y_pull \namount).","code":"\ndata_frame$x_pull <- 5\ndata_frame$y_pull <- 1\n\n# Plotting points for illustrative purposes\nggplot(data_frame, aes(x = x_axis, y = y_axis)) +\n    scale_x_continuous(breaks = seq(0,10,1)) + \n    scale_y_continuous(breaks = seq(0,10,1)) +\n    geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = y_axis + (y_pull)), \n                 arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n    coord_fixed()\n# Assume this is the raw x_pull & y_pull data\ndata_frame$x_pull <- 5\ndata_frame$y_pull <- 1\n\n# Scale vector data\ndata_frame$x_pull <- data_frame$x_pull / 10\ndata_frame$y_pull <- data_frame$y_pull / 10\n\n# Plotting points for illustrative purposes\nggplot(data_frame, aes(x = x_axis, y = y_axis)) +\n    scale_x_continuous(breaks = seq(0,10,1)) + \n    scale_y_continuous(breaks = seq(0,10,1)) +\n    geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = y_axis + (y_pull)), \n                 arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n    coord_fixed()"},{"path":"tutorial-for-vector-fields-in-r.html","id":"arrow-color","chapter":"25 Tutorial for vector fields in r","heading":"25.2.4 Arrow Color","text":"scaling arrows, absolute length arrows loses meaning,\ndistorts relative strength vectors graph. qualitative\nrepresentation acceptable cases plotting vector fields, \npossible add color arrows based magnitude - see . Moreover,\ncolor bar can add title explaining units colors\nrepresent (e.g. m/s). example, ’ll create new data set x_axis\ny_axis values ranging -10 10.following plot, changed arrow color based x_pull\nvalue, rather magnitude. useful flow one direction\nimportant .","code":"\nvector_frame = data.frame(x_axis = numeric(), y_axis = numeric())\n\n# Generating evenly distributed values for x y coordinates\nfor(i in -10:10) {\n  for(j in -10:10) {\n    vec <- c(i, j) \n    vector_frame[nrow(vector_frame) + 1, ] <- vec\n  }\n}\n\nvector_frame$x_pull <- with(vector_frame, -x_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\nvector_frame$y_pull <- with(vector_frame, 2*y_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\nvector_frame$mag    <- sqrt( (vector_frame$x_pull^2) + (vector_frame$y_pull)^2 ) \n\nggplot(vector_frame, aes(x = x_axis, y = y_axis, colour=mag) )+\n    scale_colour_continuous(name = \"*Units\") +\n    scale_x_continuous(breaks = seq(-10,10,1)) + \n    scale_y_continuous(breaks = seq(-10,10,1)) +\n    geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = y_axis + (y_pull)), \n                     arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n    geom_vline(xintercept=0, size=0.15) + geom_hline(yintercept=0, size=0.15) +\n    coord_fixed()\nvector_frame = data.frame(x_axis = numeric(), y_axis = numeric())\n\n# Generating evenly distributed values for x y coordinates\nfor(i in -10:10) {\n  for(j in -10:10) {\n    vec <- c(i, j) \n    vector_frame[nrow(vector_frame) + 1, ] <- vec\n  }\n}\n\nvector_frame$x_pull <- with(vector_frame, -x_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\nvector_frame$y_pull <- with(vector_frame, 2*y_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\n\nggplot(vector_frame, aes(x = x_axis, y = y_axis, colour=x_pull) )+\n    scale_colour_continuous(low = \"dodgerblue\", high = \"darkred\") +\n    scale_x_continuous(breaks = seq(-10,10,1)) + \n    scale_y_continuous(breaks = seq(-10,10,1)) +\n    geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = y_axis + (y_pull)), \n                     arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n    geom_vline(xintercept=0, size=0.15) + geom_hline(yintercept=0, size=0.15) +\n    coord_fixed()"},{"path":"tutorial-for-vector-fields-in-r.html","id":"spacing","chapter":"25 Tutorial for vector fields in r","heading":"25.3 Spacing","text":"Vector fields often describe flows continuous space, means \ninfinite number vectors plot. overcome , usually sample\nuniformly spaced points field plot vectors. Choosing point\nspacing important -sample much, lose information.\nhand , don’t -sample enough, vector field becomes cluttered.shown , extremes spacing, vector fields difficult interpret.\nHence, must optimal spacing two. illustrated , \noptimal spacing also depends arrow length closer spacing requires shorter\narrow lengths, larger spacing allows longer arrow lengths. words,\ntrade-vector spacing vector length.","code":"\nvector_frame1 = data.frame(x_axis = numeric(), y_axis = numeric())\n\n# Generating a dense vector field\nfor(i in seq(-10, 10, by=0.4)) {\n  for(j in seq(-10, 10, by=0.4)) {\n    vec <- c(i, j) \n    vector_frame1[nrow(vector_frame1) + 1, ] <- vec\n  }\n}\n\nvector_frame1$x_pull <- with(vector_frame1, -x_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\nvector_frame1$y_pull <- with(vector_frame1, 2*y_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\n\np1 <- ggplot(vector_frame1, aes(x = x_axis, y = y_axis) )+\n        scale_x_continuous(breaks = seq(-10,10,2)) + \n        scale_y_continuous(breaks = seq(-10,10,2)) +\n        geom_segment(aes(xend = x_axis + (x_pull), \n                         yend = y_axis + (y_pull)), \n                         arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n        geom_vline(xintercept=0, size=0.15) + geom_hline(yintercept=0, size=0.15) +\n        ggtitle(\"Not enough spacing\") +\n        theme(text=element_text(size=9)) +\n        coord_fixed()\n\nvector_frame2 = data.frame(x_axis = numeric(), y_axis = numeric())\n\n# Generating a sparse vector field\nfor(i in seq(-10, 10, by=5)) {\n  for(j in seq(-10, 10, by=5)) {\n    vec <- c(i, j) \n    vector_frame2[nrow(vector_frame2) + 1, ] <- vec\n  }\n}\n\nvector_frame2$x_pull <- with(vector_frame2, -x_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\nvector_frame2$y_pull <- with(vector_frame2, 2*y_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\n\np2 <- ggplot(vector_frame2, aes(x = x_axis, y = y_axis) )+\n        scale_x_continuous(breaks = seq(-10,10,2)) + \n        scale_y_continuous(breaks = seq(-10,10,2)) +\n        geom_segment(aes(xend = x_axis + (x_pull), \n                         yend = y_axis + (y_pull)), \n                         arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n        geom_vline(xintercept=0, size=0.15) + geom_hline(yintercept=0, size=0.15) +\n        ggtitle(\"Too much spacing\") +\n        theme(text=element_text(size=9)) +\n        coord_fixed()\n\np1 + p2\nvector_frame1 = data.frame(x_axis = numeric(), y_axis = numeric())\n\n# Vector field with larger spacing and arrows\n\nfor(i in seq(-10, 10, by=2)) {\n  for(j in seq(-10, 10, by=2)) {\n    vec <- c(i, j) \n    vector_frame1[nrow(vector_frame1) + 1, ] <- vec\n  }\n}\n\nvector_frame1$x_pull <- with(vector_frame1, -2*x_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\nvector_frame1$y_pull <- with(vector_frame1, 3*y_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\n\np1 <- ggplot(vector_frame1, aes(x = x_axis, y = y_axis) )+\n        scale_x_continuous(breaks = seq(-10,10,2)) + \n        scale_y_continuous(breaks = seq(-10,10,2)) +\n        geom_segment(aes(xend = x_axis + (x_pull), \n                         yend = y_axis + (y_pull)), \n                         arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n        geom_vline(xintercept=0, size=0.15) + geom_hline(yintercept=0, size=0.15) +\n        ggtitle(\"Opitmal with more spacing\") +\n        theme(text=element_text(size=9)) +\n        coord_fixed()\n\nvector_frame2 = data.frame(x_axis = numeric(), y_axis = numeric())\n\n# Vector field with smaller spacing and arrows\n\nfor(i in seq(-10, 10, by=1)) {\n  for(j in seq(-10, 10, by=1)) {\n    vec <- c(i, j) \n    vector_frame2[nrow(vector_frame2) + 1, ] <- vec\n  }\n}\n\nvector_frame2$x_pull <- with(vector_frame2, -x_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\nvector_frame2$y_pull <- with(vector_frame2, y_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\n\np2 <- ggplot(vector_frame2, aes(x = x_axis, y = y_axis) )+\n        scale_x_continuous(breaks = seq(-10,10,2)) + \n        scale_y_continuous(breaks = seq(-10,10,2)) +\n        geom_segment(aes(xend = x_axis + (x_pull), \n                         yend = y_axis + (y_pull)), \n                         arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n        geom_vline(xintercept=0, size=0.15) + geom_hline(yintercept=0, size=0.15) +\n        ggtitle(\"Optimal with less spacing\") +\n        theme(text=element_text(size=9)) +\n        coord_fixed()\n\np1 + p2"},{"path":"tutorial-for-vector-fields-in-r.html","id":"dealing-with-3d","chapter":"25 Tutorial for vector fields in r","heading":"25.4 Dealing with 3D","text":"far looked vector fields 2 dimensions. However, common \nvector fields 3 dimensions. tempting try building 3D\nplots vector fields, strongly recommend confusing. \nshown plot , taken page.Therefore, first thing consider plotting 3D vector\nfields : actually need plot 3 dimensions?cases, one dimension may small contribution overall flow, \nexcluded plot. check comparing contributions\ndirection magnitude vector using following formulas\nx^2 / (x^2 + y^2 + z^2) , y^2 / (x^2 + y^2 + z^2) & z^2 / (x^2 + y^2 +z^2 ).Another aspect consider symmetry environment flow. \nexample, flow cylindrical symmetric radial direction; words,\nsay flow z-direction, cross-sectional flows identical\nz-x z-y plots, see graph visual explanation.possible remove one axis due small contribution, symmetry\nvector field, recommend plotting 3 cross-sectional vector fields \nx-y, x-z y-z planes. plot, first thing consider \ncross-sections taken 3D space. recommend cross-sections\nmidpoint direction adjusting exact slices appropriate.\ncode , create 3D vector field, show extract midpoints\naxis, plot vector fields cross-sections midpoints.summary, visualizing 3D vector fields, avoid plotting 3D.\nInstead, consider one dimension small contribution , ,\neliminate plot. also think whether symmetries \nvector field allow exclude dimension plot. neither option \npossible, take cross-sections midpoints axes plot \nflows 2D.","code":"\nvector_frame = data.frame(x_axis = numeric(), y_axis = numeric(), z_axis = numeric()) \n\n# Generating a vector field in 3D space\nfor(i in -5:5) {\n  for(j in -5:5) {\n    for(k in -5:5) {\n      vec <- c(i, j, k) \n      vector_frame[nrow(vector_frame) + 1, ] <- vec\n    }\n  }\n}\n\nvector_frame$x_pull <- with(vector_frame, -x_axis/(sqrt((x_axis^2) + (y_axis^2) + (z_axis^2) ) + 1))\nvector_frame$y_pull <- with(vector_frame, y_axis/(sqrt((x_axis^2) + (y_axis^2) + (z_axis^2) ) ))\nvector_frame$z_pull <- with(vector_frame, -z_axis/(sqrt((x_axis^2) + (y_axis^2) + (z_axis^2) ) +0.5))\n\n# Finding midpoints of each axis\nx_range <- range(vector_frame$x_axis)\nmid_x <- (x_range[2] + x_range[1]) / 2\n\ny_range <- range(vector_frame$x_axis)\nmid_y <- (y_range[2] + y_range[1]) / 2\n\nz_range <- range(vector_frame$z_axis)\nmid_z <- (z_range[2] + z_range[1]) / 2\n\n# Extracting each cross-section\nxy_plot <- vector_frame[vector_frame$z_axis == mid_z,]\nxz_plot <- vector_frame[vector_frame$y_axis == mid_y,]\nyz_plot <- vector_frame[vector_frame$x_axis == mid_x,]\n\n#Plotting the 3 cross-sections\nxy <- ggplot(xy_plot, aes(x = x_axis, y = y_axis) )+\n        scale_x_continuous(breaks = seq(-5,5,1)) + \n        scale_y_continuous(breaks = seq(-5,5,1)) +\n        geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = y_axis + (y_pull)), \n                     arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n        geom_vline(xintercept=0, size=0.15) + geom_hline(yintercept=0, size=0.15) +\n        ggtitle(\"x-y cross-section\") +\n        coord_fixed() +\n        theme(text=element_text(size=8))\n\nxz <- ggplot(xz_plot, aes(x = x_axis, y = z_axis) )+\n        scale_x_continuous(breaks = seq(-5,5,1)) + \n        scale_y_continuous(breaks = seq(-5,5,1)) +\n        geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = z_axis + (z_pull)), \n                     arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n        geom_vline(xintercept=0, size=0.15) + geom_hline(yintercept=0, size=0.15) +\n        ggtitle(\"x-z cross-section\") +\n        coord_fixed() +\n        theme(text=element_text(size=8))\n\nyz <- ggplot(yz_plot, aes(x = y_axis, y = z_axis) )+\n        scale_x_continuous(breaks = seq(-5,5,1)) + \n        scale_y_continuous(breaks = seq(-5,5,1)) +\n        geom_segment(aes(xend = y_axis + (y_pull), \n                     yend = z_axis + (z_pull)), \n                     arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n        geom_vline(xintercept=0, size=0.15) + geom_hline(yintercept=0, size=0.15) +\n        ggtitle(\"y-z cross-section\") +\n        coord_fixed() +\n        theme(text=element_text(size=8))\n\nxy + xz + yz + plot_layout(ncol=2)"},{"path":"tutorial-for-vector-fields-in-r.html","id":"application","chapter":"25 Tutorial for vector fields in r","heading":"25.5 Application","text":"Now ’ve covered fundamentals plotting vector fields, briefly\ndiscuss applications.","code":""},{"path":"tutorial-for-vector-fields-in-r.html","id":"flow","chapter":"25 Tutorial for vector fields in r","heading":"25.5.1 Flow","text":"Using vector fields visualize flow extremely important fluid mechanics.\nExamples application include:Identifying regions turbulence designing airplanes race carsDiscovering sources sinks pressure fields meteorologyModelling flow blood vessels stents","code":""},{"path":"tutorial-for-vector-fields-in-r.html","id":"stability-analysis","chapter":"25 Tutorial for vector fields in r","heading":"25.5.2 Stability Analysis","text":"Stability analysis models long-term affects small perturbations initial\nconditions dynamical systems. Using differential equations describe\nenvironments, can plot vector fields system understand \nnature fixed points system (ie fixed points, stable, unstable,\netc…). figure illustrates several types fixed points:Source: https://www.sciencedirect.com/science/article/pii/S0021929018302239Examples applications stability analysis include modelling:Firing rates computational neuroscienceGene regulation networksPopulation dynamicsTherefore, given differential equations describing system, can generate\ndata frame vectors points environment. , using skills\nlearnt tutorial can plot vector fields, find fixed points \ndetermine nature.","code":""},{"path":"tutorial-for-vector-fields-in-r.html","id":"conclusion-2","chapter":"25 Tutorial for vector fields in r","heading":"25.5.3 Conclusion","text":"tutorial covered following topics:generate vector fields adjust componentsthe trade-arrow length arrow spacingmethods plotting vector fields 3 dimensionsapplications vector fields flow analysis stability analysisUsing tools, hope reader better understanding \nconstruct vector fields, elements consider creating high-quality vector\nfields applied real-world.","code":""},{"path":"tutorial-for-vector-fields-in-r.html","id":"reflection","chapter":"25 Tutorial for vector fields in r","heading":"25.5.4 Reflection","text":"research, limited documentation plotting vector fields R.\n, resources exist, Vectorfield Recipe,\nfeel tutorial dives deep quickly. Therefore, goal \ntutorial provide clear, easily accessible introduction building graphs\nvector fields R. Moreover, engineering students familiar \nvector fields, wanted share advice produce high-quality plots\nbriefly explain plots used research development., believe successfully captured basic principles \naccessible format readers hope tutorial contributes better\nunderstanding vector fields R. experience plotting vector\nfields Python, learned R. also learned \nexplicitly express elements consider plotting high-quality vector fields.terms work, think may useful use actual vector\nfield data sets, rather synthetically produced data, may provide\nreader practical exposure handling data. said ,\nmajority vector field data simulated, believe omission \nsignificant. Another area work include illustrative\ntutorials walking reader produce vector field plots specific\nflow analysis stability analysis. However, must careful making\nadditions analyses applications requires deep knowledge \ntopic, confuse reader fall trap tutorials\ndive “deep quick”.","code":""},{"path":"tutorial-for-vector-fields-in-r.html","id":"extra","chapter":"25 Tutorial for vector fields in r","heading":"25.5.5 Extra","text":"Overall, motivation project provide clear, easily accessible\ntutorial building vector graphs. engineering students, encountered\nvector graphs many classes, found existing documentation (Vectorfield Recipe) unnecessarily\nconfusing, also overlooking basics getting started. , learned\nmake vector graphs, feel successfully capture basic principles\naccessible format readers. don’t feel \nmajor changes ’d make improving article.","code":""},{"path":"data-cleaning-with-r.html","id":"data-cleaning-with-r","chapter":"26 Data cleaning with r","heading":"26 Data cleaning with r","text":"Jincheng Liu Yudu Chen","code":"\n#install.packages(openintro)\nlibrary(openintro)\n\n#install.packages('dplyr')\nlibrary(dplyr)\n\n#install.packages('tidyr')\nlibrary(tidyr)\n\n#install.packages(imputeTS)\nlibrary(imputeTS)\n\n#install.packages(caret)\nlibrary(caret)"},{"path":"data-cleaning-with-r.html","id":"data-cleaning-with-r-1","chapter":"26 Data cleaning with r","heading":"26.1 Data cleaning with R","text":"real world, data sets handling often data scientists can readily use. might contain duplicate entries entries supposed unique. might contain missing values, problematic tasks training predicative modeling. data features might vastly different scale, induces instability float point arithmatics inaccurate measurement feature importance training machine learning model. discuss use R clean Data sets situations take place.","code":""},{"path":"data-cleaning-with-r.html","id":"import-data-set","chapter":"26 Data cleaning with r","heading":"26.2 Import data set","text":"explore techniques data cleaning using “Airquality” dataset base R duke_forest data set openintro. data set available baseR require import outside source.","code":"\nairQuality_preProcessing <- airquality\n\nhead(airQuality_preProcessing, 10)##    Ozone Solar.R Wind Temp Month Day\n## 1     41     190  7.4   67     5   1\n## 2     36     118  8.0   72     5   2\n## 3     12     149 12.6   74     5   3\n## 4     18     313 11.5   62     5   4\n## 5     NA      NA 14.3   56     5   5\n## 6     28      NA 14.9   66     5   6\n## 7     23     299  8.6   65     5   7\n## 8     19      99 13.8   59     5   8\n## 9      8      19 20.1   61     5   9\n## 10    NA     194  8.6   69     5  10\nduke_forest_copy <- duke_forest"},{"path":"data-cleaning-with-r.html","id":"introduction-1","chapter":"26 Data cleaning with r","heading":"26.3 Introduction","text":"real world, data sets handling often data scientists can readily use. might contain duplicate entries entries supposed unique. might contain missing values, problematic tasks training predicative modeling. data features might vastly different scale, induces instability float point arithmatics inaccurate measurement feature importance training machine learning model. discuss use R clean Data sets situations take place.","code":""},{"path":"data-cleaning-with-r.html","id":"data-frame-modification-general-modification","chapter":"26 Data cleaning with r","heading":"26.4 Data frame modification (general modification)","text":"time, engineer data sets data analyzation . present tricks modify dat sets R.","code":""},{"path":"data-cleaning-with-r.html","id":"create-dataframe-by-arrays","chapter":"26 Data cleaning with r","heading":"26.4.1 Create DataFrame by arrays","text":"Creating DataFrame two arrays, two arrays columns df.Array length .order matters.another way create dataFrame, method produce result.","code":"\nfirst_column <- c(\"value_1\", \"value_2\", \"value_3\")\nsecond_column <- c(\"value_4\", \"value_5\", \"value_6\")\nthird_column <- c(\"value_7\", \"value_8\", \"value_9\")\ndf <- data.frame(first_column, second_column)\ndf##   first_column second_column\n## 1      value_1       value_4\n## 2      value_2       value_5\n## 3      value_3       value_6\ndf <- data.frame (first_column  = c(\"value_1\", \"value_2\", \"value_3\"),\n                  second_column = c(\"value_4\", \"value_5\", \"value_6\"),\n                  third_column = c(\"value_7\", \"value_8\", \"value_9\")\n                  )\ndf##   first_column second_column third_column\n## 1      value_1       value_4      value_7\n## 2      value_2       value_5      value_8\n## 3      value_3       value_6      value_9"},{"path":"data-cleaning-with-r.html","id":"create-dataframe-by-combining-two-dataframeusing-cbindrbind","chapter":"26 Data cleaning with r","heading":"26.4.2 Create DataFrame by combining two dataFrame(using cbind/rbind)","text":"combine two dataframe columns,order matters.combine two dataframe rows, two DataFrames must column name order.","code":"\n# create a new dataFrame\ndf1 <- data.frame (first_column  = c(\"value_1\", \"value_2\", \"value_3\"),\n                  second_column = c(\"value_4\", \"value_5\", \"value_6\"),\n                  third_column = c(\"value_7\", \"value_8\", \"value_9\")\n                  )\ndf2 <- data.frame (third_column  = c(\"value_7\", \"value_8\", \"value_9\"),\n                  fourth_column = c(\"value_10\", \"value_11\", \"value_12\")\n                  )\nbigger_df <- cbind(df1, df2)\nprint(bigger_df)##   first_column second_column third_column third_column fourth_column\n## 1      value_1       value_4      value_7      value_7      value_10\n## 2      value_2       value_5      value_8      value_8      value_11\n## 3      value_3       value_6      value_9      value_9      value_12\ndf3 <- data.frame (third_column = c(\"value_7\", \"value_8\", \"value_9\"),\n                  fourth_column = c(\"value_10\", \"value_11\", \"value_12\")\n                  )\nbigger_df2 <- rbind(df2,df3)\nprint(bigger_df2)##   third_column fourth_column\n## 1      value_7      value_10\n## 2      value_8      value_11\n## 3      value_9      value_12\n## 4      value_7      value_10\n## 5      value_8      value_11\n## 6      value_9      value_12"},{"path":"data-cleaning-with-r.html","id":"addingchange-row-name","chapter":"26 Data cleaning with r","heading":"26.4.3 adding/change row name","text":"","code":"\nprint(bigger_df2)##   third_column fourth_column\n## 1      value_7      value_10\n## 2      value_8      value_11\n## 3      value_9      value_12\n## 4      value_7      value_10\n## 5      value_8      value_11\n## 6      value_9      value_12\nrownames(bigger_df2) <- LETTERS[16:21]\n\nbigger_df2 <- data.frame(bigger_df2,\n                   row.names = LETTERS[16:21])\n# both ways produce the same result\nprint(bigger_df2)##   third_column fourth_column\n## P      value_7      value_10\n## Q      value_8      value_11\n## R      value_9      value_12\n## S      value_7      value_10\n## T      value_8      value_11\n## U      value_9      value_12\n# rownames can change the row name.\nrownames(bigger_df2) <- 1:6\nprint(bigger_df2)##   third_column fourth_column\n## 1      value_7      value_10\n## 2      value_8      value_11\n## 3      value_9      value_12\n## 4      value_7      value_10\n## 5      value_8      value_11\n## 6      value_9      value_12\n## get row names\nrownames(bigger_df2)## [1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\""},{"path":"data-cleaning-with-r.html","id":"changing-column-names","chapter":"26 Data cleaning with r","heading":"26.4.4 changing column names","text":"","code":"\nprint(df)##   first_column second_column third_column\n## 1      value_1       value_4      value_7\n## 2      value_2       value_5      value_8\n## 3      value_3       value_6      value_9\n# change all column names\ncolnames(df) <- c('C1','C2')\nprint(df)##        C1      C2      NA\n## 1 value_1 value_4 value_7\n## 2 value_2 value_5 value_8\n## 3 value_3 value_6 value_9\n# change specific column name\ncolnames(df)[1] <- c('new column name')\nprint(df)##   new column name      C2      NA\n## 1         value_1 value_4 value_7\n## 2         value_2 value_5 value_8\n## 3         value_3 value_6 value_9\n#get column names\ncolnames(df)## [1] \"new column name\" \"C2\"              NA"},{"path":"data-cleaning-with-r.html","id":"adding-array-to-to-dataframe-as-new-column","chapter":"26 Data cleaning with r","heading":"26.4.5 Adding array to to dataFrame as new column","text":"","code":"\nprint(bigger_df2)##   third_column fourth_column\n## 1      value_7      value_10\n## 2      value_8      value_11\n## 3      value_9      value_12\n## 4      value_7      value_10\n## 5      value_8      value_11\n## 6      value_9      value_12\nbigger_df_new = bigger_df2\n\nbigger_df_new$new <- c(3, 3, 6, 7, 8, 12)\n\nbigger_df_new['new'] <- c(3, 3, 6, 7, 8, 12)\n\nnew <- c(3, 3, 6, 7, 8, 12)\nbigger_df_new <- cbind(df, new)\n# three ways produce the same result.\nprint(bigger_df_new)##   new column name      C2      NA new\n## 1         value_1 value_4 value_7   3\n## 2         value_2 value_5 value_8   3\n## 3         value_3 value_6 value_9   6\n## 4         value_1 value_4 value_7   7\n## 5         value_2 value_5 value_8   8\n## 6         value_3 value_6 value_9  12"},{"path":"data-cleaning-with-r.html","id":"adding-column-by-combination-of-other-columns","chapter":"26 Data cleaning with r","heading":"26.4.6 adding column by combination of other columns","text":"","code":"\n# using mutate() from dplyr\ndf_math <- data.frame (first_column  = c(1, 2, 3),\n                  second_column = c(4, 5, 6)\n                  )\n\n## add new column as mathematical operation of other columns.\noutput <- mutate(df_math,\n                 sum = (first_column + second_column) / 2)\nprint(output)##   first_column second_column sum\n## 1            1             4 2.5\n## 2            2             5 3.5\n## 3            3             6 4.5\n## add new column by boolean operation of other columns.\noutput <- mutate(df_math,\n                 divisible_by2 = case_when(\n                   first_column%%2 == 0 ~ \"yes\",\n                   TRUE ~ \"No\" # otherwise not divisiable by 2.\n                 ))\nprint(output)##   first_column second_column divisible_by2\n## 1            1             4            No\n## 2            2             5           yes\n## 3            3             6            No"},{"path":"data-cleaning-with-r.html","id":"joining-two-table-2","chapter":"26 Data cleaning with r","heading":"26.4.7 joining two table [2]","text":"Bewaring order dataframe merge function matters(right join left)","code":"\nemp_df=data.frame(\n  emp_id=c(1,2,3,4,5,6),\n  name=c(\"Smith\",\"Rose\",\"Williams\",\"Jones\",\"Brown\",\"Brown\"),\n  superior_emp_id=c(-1,1,1,2,2,2),\n  dept_id=c(10,20,10,10,40,50),\n  dept_branch_id= c(101,102,101,101,104,105)\n)\n\ndept_df=data.frame(\n  dept_id=c(10,20,30,40),\n  dept_name=c(\"Finance\",\"Marketing\",\"Sales\",\"IT\"),\n  dept_branch_id= c(101,102,103,104)\n)\n# inner join\nprint(merge(x = emp_df, y = dept_df, by = \"dept_id\"))##   dept_id emp_id     name superior_emp_id dept_branch_id.x dept_name\n## 1      10      1    Smith              -1              101   Finance\n## 2      10      3 Williams               1              101   Finance\n## 3      10      4    Jones               2              101   Finance\n## 4      20      2     Rose               1              102 Marketing\n## 5      40      5    Brown               2              104        IT\n##   dept_branch_id.y\n## 1              101\n## 2              101\n## 3              101\n## 4              102\n## 5              104\n# outer join \nprint(merge(x = emp_df, y = dept_df, by = \"dept_id\", all = TRUE)) # all means containing all rows##   dept_id emp_id     name superior_emp_id dept_branch_id.x dept_name\n## 1      10      1    Smith              -1              101   Finance\n## 2      10      3 Williams               1              101   Finance\n## 3      10      4    Jones               2              101   Finance\n## 4      20      2     Rose               1              102 Marketing\n## 5      30     NA     <NA>              NA               NA     Sales\n## 6      40      5    Brown               2              104        IT\n## 7      50      6    Brown               2              105      <NA>\n##   dept_branch_id.y\n## 1              101\n## 2              101\n## 3              101\n## 4              102\n## 5              103\n## 6              104\n## 7               NA\n# left join\nprint(merge(x = emp_df, y = dept_df, by = \"dept_id\", all.x = TRUE)) # all.x means containing all rows in x##   dept_id emp_id     name superior_emp_id dept_branch_id.x dept_name\n## 1      10      1    Smith              -1              101   Finance\n## 2      10      3 Williams               1              101   Finance\n## 3      10      4    Jones               2              101   Finance\n## 4      20      2     Rose               1              102 Marketing\n## 5      40      5    Brown               2              104        IT\n## 6      50      6    Brown               2              105      <NA>\n##   dept_branch_id.y\n## 1              101\n## 2              101\n## 3              101\n## 4              102\n## 5              104\n## 6               NA\n# right join\nprint(merge(x = emp_df, y = dept_df, by = \"dept_id\", all.y = TRUE)) # all.y means containing all rows in y##   dept_id emp_id     name superior_emp_id dept_branch_id.x dept_name\n## 1      10      1    Smith              -1              101   Finance\n## 2      10      3 Williams               1              101   Finance\n## 3      10      4    Jones               2              101   Finance\n## 4      20      2     Rose               1              102 Marketing\n## 5      30     NA     <NA>              NA               NA     Sales\n## 6      40      5    Brown               2              104        IT\n##   dept_branch_id.y\n## 1              101\n## 2              101\n## 3              101\n## 4              102\n## 5              103\n## 6              104"},{"path":"data-cleaning-with-r.html","id":"pivoting-dataframe-understanding-pivot_longer","chapter":"26 Data cleaning with r","heading":"26.4.8 pivoting dataframe, understanding pivot_longer","text":"","code":"\n# pivot_longer transform a data frame from a wide format to a long format by converting feature names to a categorical feature.\n\n# pivot_longer is from the package tidyr.\nneeds_pivoting <- data.frame(sticker_type=c('A', 'B', 'C', 'D'),\n                 sparrow=c(12, 15, 19, 19),\n                 eagle=c(22, 29, 18, 12))\n\nprint(needs_pivoting)##   sticker_type sparrow eagle\n## 1            A      12    22\n## 2            B      15    29\n## 3            C      19    18\n## 4            D      19    12\nneeds_pivoting %>% pivot_longer(cols=c('sparrow', 'eagle'), # the columns(feature names) to be pivoted\n                                names_to='species', # the name of the column of features\n                                values_to='price') # the name of the column of values## # A tibble: 8 × 3\n##   sticker_type species price\n##   <chr>        <chr>   <dbl>\n## 1 A            sparrow    12\n## 2 A            eagle      22\n## 3 B            sparrow    15\n## 4 B            eagle      29\n## 5 C            sparrow    19\n## 6 C            eagle      18\n## 7 D            sparrow    19\n## 8 D            eagle      12"},{"path":"data-cleaning-with-r.html","id":"binning-using-cut","chapter":"26 Data cleaning with r","heading":"26.4.8.1 binning using cut()","text":"also good way categorize feature","code":"\nhour_df <- data.frame(shop_name=c('MAC', 'Tangro', 'cummington', 'Burger King', 'judgement', 'KFC', 'ye', 'Dungeon', 'Razer', 'yeah sir', 'Koban wife', 'string'),\n                 operating_hours=c(2, 5, 4, 7, 7, 8, 5, 4, 5, 11, 13, 8),\n                 rebounds=c(7, 7, 4, 6, 3, 8, 9, 9, 12, 11, 8, 9))\nhour_df##      shop_name operating_hours rebounds\n## 1          MAC               2        7\n## 2       Tangro               5        7\n## 3   cummington               4        4\n## 4  Burger King               7        6\n## 5    judgement               7        3\n## 6          KFC               8        8\n## 7           ye               5        9\n## 8      Dungeon               4        9\n## 9        Razer               5       12\n## 10    yeah sir              11       11\n## 11  Koban wife              13        8\n## 12      string               8        9\nnew_hour_df <- hour_df %>% mutate(operating_hours_bin = cut(operating_hours, breaks=c(0,5,9,13)))\nnew_hour_df##      shop_name operating_hours rebounds operating_hours_bin\n## 1          MAC               2        7               (0,5]\n## 2       Tangro               5        7               (0,5]\n## 3   cummington               4        4               (0,5]\n## 4  Burger King               7        6               (5,9]\n## 5    judgement               7        3               (5,9]\n## 6          KFC               8        8               (5,9]\n## 7           ye               5        9               (0,5]\n## 8      Dungeon               4        9               (0,5]\n## 9        Razer               5       12               (0,5]\n## 10    yeah sir              11       11              (9,13]\n## 11  Koban wife              13        8              (9,13]\n## 12      string               8        9               (5,9]"},{"path":"data-cleaning-with-r.html","id":"binning-using-case","chapter":"26 Data cleaning with r","heading":"26.4.8.2 binning using case()","text":"","code":"\nhour_df2 <- data.frame(shop_name=c('MAC', 'Tangro', 'cummington', 'Burger King', 'judgement', 'KFC', 'ye', 'Dungeon', 'Razer', 'yeah sir', 'Koban wife', 'string'),\n                 operating_hours=c(2, 5, 4, 7, 7, 8, 5, 4, 5, 11, 13, 8),\n                 rebounds=c(7, 7, 4, 6, 3, 8, 9, 9, 12, 11, 8, 9))\nhour_df##      shop_name operating_hours rebounds\n## 1          MAC               2        7\n## 2       Tangro               5        7\n## 3   cummington               4        4\n## 4  Burger King               7        6\n## 5    judgement               7        3\n## 6          KFC               8        8\n## 7           ye               5        9\n## 8      Dungeon               4        9\n## 9        Razer               5       12\n## 10    yeah sir              11       11\n## 11  Koban wife              13        8\n## 12      string               8        9\nnew_hour_df <- hour_df2 %>% mutate(operating_hours_bin = case_when( # logistics\n                   operating_hours <= 3 ~ 'very short',\n                   operating_hours <= 6 & operating_hours > 3 ~ 'short',\n                   operating_hours <= 10 & operating_hours > 6 ~ 'median',\n                   operating_hours > 10 ~ 'long',\n                   TRUE ~ 'what else will this be?'\n                 ))\nnew_hour_df##      shop_name operating_hours rebounds operating_hours_bin\n## 1          MAC               2        7          very short\n## 2       Tangro               5        7               short\n## 3   cummington               4        4               short\n## 4  Burger King               7        6              median\n## 5    judgement               7        3              median\n## 6          KFC               8        8              median\n## 7           ye               5        9               short\n## 8      Dungeon               4        9               short\n## 9        Razer               5       12               short\n## 10    yeah sir              11       11                long\n## 11  Koban wife              13        8                long\n## 12      string               8        9              median"},{"path":"data-cleaning-with-r.html","id":"ordering","chapter":"26 Data cleaning with r","heading":"26.4.9 ordering","text":"","code":"\nstudent_result_wild=data.frame(name=c(\"Ram\",\"Geeta\",\"John\",\"Paul\",\n                                 \"Cassie\",\"Geeta\",\"Paul\"),\n                          maths=c(7,8,8,9,10,8,9),\n                          science=c(5,7,6,8,9,7,8),\n                          history=c(7,7,7,7,7,7,7),\n                          id = c(9,2,3,5,13,2,5))\nstudent_result_wild # data with out ordering##     name maths science history id\n## 1    Ram     7       5       7  9\n## 2  Geeta     8       7       7  2\n## 3   John     8       6       7  3\n## 4   Paul     9       8       7  5\n## 5 Cassie    10       9       7 13\n## 6  Geeta     8       7       7  2\n## 7   Paul     9       8       7  5\n# order data by certain variable (ascending)\nstudent_result_wild[order(student_result_wild$id),]##     name maths science history id\n## 2  Geeta     8       7       7  2\n## 6  Geeta     8       7       7  2\n## 3   John     8       6       7  3\n## 4   Paul     9       8       7  5\n## 7   Paul     9       8       7  5\n## 1    Ram     7       5       7  9\n## 5 Cassie    10       9       7 13\n# order data by certain variable (descending)\nstudent_result_wild[order(-student_result_wild$id),]##     name maths science history id\n## 5 Cassie    10       9       7 13\n## 1    Ram     7       5       7  9\n## 4   Paul     9       8       7  5\n## 7   Paul     9       8       7  5\n## 3   John     8       6       7  3\n## 2  Geeta     8       7       7  2\n## 6  Geeta     8       7       7  2\n# order data by multiple variable, the second orders the duplicates in first variable\nstudent_result_wild[order(-student_result_wild$id, student_result_wild$science),]##     name maths science history id\n## 5 Cassie    10       9       7 13\n## 1    Ram     7       5       7  9\n## 4   Paul     9       8       7  5\n## 7   Paul     9       8       7  5\n## 3   John     8       6       7  3\n## 2  Geeta     8       7       7  2\n## 6  Geeta     8       7       7  2"},{"path":"data-cleaning-with-r.html","id":"duplicated-values","chapter":"26 Data cleaning with r","heading":"26.5 Duplicated Values","text":"many cases, observe duplicated values data set every instances supposed unique. now discussed handle duplicates.","code":""},{"path":"data-cleaning-with-r.html","id":"data-set-with-row-wise-duplicates","chapter":"26 Data cleaning with r","heading":"26.5.1 Data set with row-wise duplicates","text":"Although dataset use example, airquality data, duplicate, use illustrate techniques handling duplicate data.see data frame duplicate rows begin .Now randomly pick 5 instances data frame insert data frame induce duplicationWe now 5 duplicated instances:“duplicated(df)” returns boolean array value index indicates row index original data frame duplicated . can use ’duplicated(df)” extract duplicated rows:adding “!” “duplicated(df)”, can negate logics “duplicated(df)” access non duplicate rows data frame:duplicatesWe can create new reference data set duplicates. purpose reusing “data_duplicated_values”, just assign new reference :Now “data_duplicated_values” duplicates:Another way use “distinct()” function tidyverse package.","code":"\ndata_duplicated_values = airQuality_preProcessing\nprint(paste0('number of duplicated rows in the data is ',sum(duplicated(data_duplicated_values))))## [1] \"number of duplicated rows in the data is 0\"\nfor (i in 1: 5)\n{\n  data_duplicated_values[nrow(data_duplicated_values)+1,] = data_duplicated_values[floor(runif(1, min = 1, max = nrow(data_duplicated_values))),]\n}\nprint(paste0('number of duplicated rows in the data after insertion is ',sum(duplicated(data_duplicated_values))))## [1] \"number of duplicated rows in the data after insertion is 5\"\ndata_duplicated_values[duplicated(data_duplicated_values),]##     Ozone Solar.R Wind Temp Month Day\n## 154    NA     291 14.9   91     7  14\n## 155    36     118  8.0   72     5   2\n## 156    50     275  7.4   86     7  29\n## 157    NA     264 14.3   79     6   6\n## 158   118     225  2.3   94     8  29\nhead(data_duplicated_values[!duplicated(data_duplicated_values),], 10)##    Ozone Solar.R Wind Temp Month Day\n## 1     41     190  7.4   67     5   1\n## 2     36     118  8.0   72     5   2\n## 3     12     149 12.6   74     5   3\n## 4     18     313 11.5   62     5   4\n## 5     NA      NA 14.3   56     5   5\n## 6     28      NA 14.9   66     5   6\n## 7     23     299  8.6   65     5   7\n## 8     19      99 13.8   59     5   8\n## 9      8      19 20.1   61     5   9\n## 10    NA     194  8.6   69     5  10\nprint(paste0('duplicates in data frame formed by unique rows in original data frame is ',sum(duplicated(data_duplicated_values[!duplicated(data_duplicated_values),]))))## [1] \"duplicates in data frame formed by unique rows in original data frame is 0\"\ndata_duplicated_values <- data_duplicated_values[!duplicated(data_duplicated_values),]\nprint(paste0('number of duplicated value after re setting reference is ', sum(duplicated(data_duplicated_values))))## [1] \"number of duplicated value after re setting reference is 0\"\nfor (i in 1: 5)\n{\n  data_duplicated_values[nrow(data_duplicated_values)+1,] = data_duplicated_values[floor(runif(1, min = 1, max = nrow(data_duplicated_values))),]\n}\nprint(paste0('number of duplicate value in data frame before calling distinct is', sum(duplicated(data_duplicated_values))))## [1] \"number of duplicate value in data frame before calling distinct is5\"\ndata_duplicated_values <- data_duplicated_values %>% distinct()\nprint(paste0('number of duplicate value in data frame after calling distinct is', sum(duplicated(data_duplicated_values))))## [1] \"number of duplicate value in data frame after calling distinct is0\""},{"path":"data-cleaning-with-r.html","id":"duplicates-based-on-specific-columns","chapter":"26 Data cleaning with r","heading":"26.5.2 Duplicates based on specific columns","text":"times duplication elements certain column/columns desirable. want able remove rows duplication specified columns. first insert 5 rows duplicates “Day” “Month”, see “158” rows insertion:can remove rows duplicated “Day” “Month” combination using “distinct()”. “.keep_all = TRUE” argument makes sure duplicated combination Day Month, keep first row every duplicated combination keep variables Data. observe number rows dropping dupicated columns going back “153”, size data frame insert rows duplicated “Day” “Month” combinations.","code":"\nprint(paste0('number of rows before adding 5 duplicated rows on Day and Month is', nrow(data_duplicated_values)))## [1] \"number of rows before adding 5 duplicated rows on Day and Month is153\"\nfor (i in 1: 5)\n{\n  Ozone <- floor(runif(1,min = 0, max = 50))\n  Solor <- floor(runif(1,min = 0, max = 300))\n  Wind <- round(runif(1,min = 0, max = 20), 2)\n  Temp <- floor(runif(1,min = 0, max = 20))\n  \n  random_index = floor(runif(1, min = 1, max = nrow(data_duplicated_values)))\n  \n  Month <- data_duplicated_values[random_index, 'Month']\n  Day <- data_duplicated_values[random_index, 'Day']\n  data_duplicated_values[nrow(data_duplicated_values)+1,] = c(Ozone, Solor, Wind, Temp, Month, Day)\n  \n}\n\nprint(paste0('number of rows after adding 5 duplicated rows on Day and Month is', nrow(data_duplicated_values)))## [1] \"number of rows after adding 5 duplicated rows on Day and Month is158\"\ndata_duplicated_values<- data_duplicated_values %>% distinct(Day, Month, .keep_all = TRUE)\nprint(paste0('number of rows after removing 5 duplicated rows on Dat and Month is', nrow(data_duplicated_values)))## [1] \"number of rows after removing 5 duplicated rows on Dat and Month is153\""},{"path":"data-cleaning-with-r.html","id":"missing-values","chapter":"26 Data cleaning with r","heading":"26.6 Missing Values","text":"Encountering missing data dataset uncommon. collecting temperature data sensor might broken unable measure temperature. conduct public opinion surveying, interviewee might forget filling entires questionaires. Many data science job requires completeness data, training predicative model based numerical/categorical data. now discuss data cleaning R.","code":""},{"path":"data-cleaning-with-r.html","id":"na-in-r","chapter":"26 Data cleaning with r","heading":"26.6.1 NA in R","text":"R, missing value represented symbol “NA”.can observe, index 5, instance missing value “Ozone” “Solar.R” feasure, represented symbol “NA”.\n### Dropping rows missing data\nsimple approach drop rows missing data. task requires using features dataset, can opt drop rows least one missing value. However, task required using features, need drop rows missing values feastures specify. discuss cases.\n#### Drop rows containing missing Data\nSuppose want use features “airquality” dataset want drop rows least 1 missing values. first check number rows missing data dataset. ‘.na(df)’ command returns boolean array, truth value index indicate data element index data frame df NA . summing number “TRUE” values ‘.na(df)’, know total number missing values data set.see totally 44 missing least 1 missing values.\nknow number rows missing value, use “complete.cases(df)”. return array boolean boolean value index indicate data instance/case corresponding row index data frame df complete (NA missing value) . summing number false array, get total number rows least 1 missing value.visualize rows missing value, can access using boolean array “complete.cases()”. Due large number rows missing value, put 10 :now remove rows na. use “drop_na()” function tidyr library remove rows missing values. multiple ways drop rows missing value, go exhaustive .removing 42 rows missing values, 111 rows left, 42 rows removed total 153 rows original data frame.","code":"\nhead(airquality, 10)##    Ozone Solar.R Wind Temp Month Day\n## 1     41     190  7.4   67     5   1\n## 2     36     118  8.0   72     5   2\n## 3     12     149 12.6   74     5   3\n## 4     18     313 11.5   62     5   4\n## 5     NA      NA 14.3   56     5   5\n## 6     28      NA 14.9   66     5   6\n## 7     23     299  8.6   65     5   7\n## 8     19      99 13.8   59     5   8\n## 9      8      19 20.1   61     5   9\n## 10    NA     194  8.6   69     5  10\nprint(paste0('total number of missing values is ', sum(is.na(airQuality_preProcessing))))## [1] \"total number of missing values is 44\"\nprint(paste0(\"number of rows with at least 1 missing value is \", sum(!complete.cases(airQuality_preProcessing))))## [1] \"number of rows with at least 1 missing value is 42\"\nhead(airQuality_preProcessing[!complete.cases(airQuality_preProcessing), ], 10)\nairQuality_na_droped <- airQuality_preProcessing %>% drop_na()\nprint(paste0(\"number of missing values after we drop all NA is \",sum(is.na(airQuality_na_droped))))## [1] \"number of missing values after we drop all NA is 0\"\nprint(paste0(\"number of rows in data frame after extracting rows with NA is \", nrow(airQuality_na_droped)))## [1] \"number of rows in data frame after extracting rows with NA is 111\""},{"path":"data-cleaning-with-r.html","id":"drop-rows-with-missing-data-in-specified-columns","chapter":"26 Data cleaning with r","heading":"26.6.1.1 Drop rows with missing data in specified columns","text":"","code":""},{"path":"data-cleaning-with-r.html","id":"na-in-single-column","chapter":"26 Data cleaning with r","heading":"26.6.1.1.1 NA in single column","text":"can see dropping columns missing data wasteful don’t use features dataset. example, job require feature “Ozone”, don’t really care “Ozone” value missing irrelevant task. explore drop rows na specific columns.see “.na(df)” evaluate presence NA(missing value) entire data set. change argument df, entire data set, specific column, function evaluate specific columns data set return boolean array reflects presence missing value columns. example, want see number rows missing value “Ozone” column:37 rows NA “Ozone” column. drop rows missing value “Ozone”, put column name “Ozone” argument:37 rows extracted 153 columns, resulting 116 rows.","code":"\nprint(paste0('number of NA in Ozone column is ', sum(is.na(airQuality_preProcessing$Ozone))))## [1] \"number of NA in Ozone column is 37\"\nairQuality_na_droped_Ozone <- airQuality_preProcessing %>% drop_na(Ozone)\nprint(nrow(airQuality_na_droped_Ozone))## [1] 116"},{"path":"data-cleaning-with-r.html","id":"na-in-several-columns-and","chapter":"26 Data cleaning with r","heading":"26.6.1.1.2 NA in several columns (And)","text":"want drop rows NA specific column, example, rows NA “Ozone” “Solar.R” columns, can take advantage fact “.na(df)” boolean array, can perform element wise boolean operation two arrays dimension:see number rows missing value “Ozone” “Solar.R” 2. now visualize two rows:boolean array, can negate logics obtain rows don’t missing values “Ozone” “Solar.R” time.droped two rows 153 rows original dataset, 151 rows left.","code":"\nsum(is.na(airQuality_preProcessing$Ozone) & is.na(airQuality_preProcessing$Solar.R))## [1] 2\nairQuality_preProcessing[is.na(airQuality_preProcessing$Ozone) & is.na(airQuality_preProcessing$Solar.R), ]##    Ozone Solar.R Wind Temp Month Day\n## 5     NA      NA 14.3   56     5   5\n## 27    NA      NA  8.0   57     5  27\nairQuality_NA_Ozone_and_Solor <- airQuality_preProcessing[!(is.na(airQuality_preProcessing$Ozone) & is.na(airQuality_preProcessing$Solar.R)), ]\nnrow(airQuality_NA_Ozone_and_Solor)## [1] 151"},{"path":"data-cleaning-with-r.html","id":"na-in-several-columns-or","chapter":"26 Data cleaning with r","heading":"26.6.1.1.3 NA in several columns (Or)","text":"want drop columns NA “Ozone” “Solar.R”, two columns contains missing values. used boolean operation several boolean arrays , tedious feastures consider. use “complete.cases()” use specific columns dataset input. “complete.cases()” returns false single column rows data frame pass missing value, perform “” operation .42 rows missing values either “Ozone” “Solar.R”, . fact, missing values appear two columns, get total number rows missing values data set. can obtain datas missing value “Ozone” “Solar.R”.111 rows removing 42 rows missing value either “Ozone” “Solar.R”can also use drop_na() columns specified function arguement:","code":"\nsum(!(complete.cases(airQuality_preProcessing[,c(\"Ozone\",\"Solar.R\")])))## [1] 42\nairQuality_NA_Ozone_or_Solor<- airQuality_preProcessing[(complete.cases(airQuality_preProcessing[,c(\"Ozone\",\"Solar.R\")])), ]\nnrow(airQuality_NA_Ozone_or_Solor)## [1] 111\nairQuality_NA_Ozone_or_Solor<- airQuality_preProcessing %>% drop_na(c(\"Ozone\", \"Solar.R\"))\nnrow(airQuality_NA_Ozone_or_Solor)## [1] 111"},{"path":"data-cleaning-with-r.html","id":"drop-columns-with-certain-number-of-na","chapter":"26 Data cleaning with r","heading":"26.6.1.1.4 Drop columns with certain number of NA","text":"might uncommon, might want drop rows certain number NA, rows 3 NA even rows number NA equal number columns, means row column value NA. know number NA row, can use “rowSums()” functions:means row 1 row 10, row 5 2 NA row 6 10 1 NA. can drop rows number NA certain threshold. now insert 3 rows 3 NA data frame.Now last 3 rows 3 NA tail airQuality_3NA_Inserted. now check number rows equal 3 NA:Now 3 rows least 3 NA. now drop rows least 3 NA airQuality_3NA_Inserted selecting rows NA less 3 reset reference airQuality_3NA_Inserted:drop 3 rows least 3 NA, size airQuality_3NA_Inserted drops 156 153.","code":"\nhead(rowSums(is.na(airQuality_preProcessing)),10)##  [1] 0 0 0 0 2 1 0 0 0 1\nairQuality_3NA_Inserted <- airQuality_preProcessing\nfor (i in 1: 3)\n{\n  Ozone <- floor(runif(1,min = 0, max = 50))\n  Solor <- floor(runif(1,min = 0, max = 300))\n  Wind <- round(runif(1,min = 0, max = 20), 2)\n  Temp <- floor(runif(1,min = 0, max = 20))\n  \n  random_index = floor(runif(1, min = 1, max = nrow(airQuality_3NA_Inserted)))\n  \n  Month <- airQuality_3NA_Inserted [random_index, 'Month']\n  Day <- airQuality_3NA_Inserted [random_index, 'Day']\n  \n  \n  toInsert <- c(Ozone, Solor, Wind, Temp, Month, Day)\n  for (i in 1:3) {\n    random_index = floor(runif(1, min = 1, max = 6))\n    if (is.na(toInsert[random_index])) {\n      random_index = floor(runif(1, min = 1, max = 6))\n    }\n    toInsert <- replace(toInsert, random_index, NA)\n  }\n\n  airQuality_3NA_Inserted [nrow(airQuality_3NA_Inserted )+1,] <- toInsert\n  \n  \n}\ntail(airQuality_3NA_Inserted, 3)##     Ozone Solar.R Wind Temp Month Day\n## 154     2      NA 2.96   15    NA  23\n## 155    NA     103   NA   NA     9  29\n## 156    NA      NA 9.43    4    NA   8\nprint(paste('number of rows in airQuality_3NA_Inserted is', nrow(airQuality_3NA_Inserted)))## [1] \"number of rows in airQuality_3NA_Inserted is 156\"\nprint(paste0(\"number of rows with at least 3 NA in airQuality_3NA_Inserted is \", sum(rowSums(is.na(airQuality_3NA_Inserted)) >= 3)))## [1] \"number of rows with at least 3 NA in airQuality_3NA_Inserted is 2\"\nairQuality_3NA_Inserted <- airQuality_3NA_Inserted[!(rowSums(is.na(airQuality_3NA_Inserted)) >= 3), ]\n\nprint(paste0(\"number of rows in airQuality_3NA_Inserted after dropping is \", nrow(airQuality_3NA_Inserted)))## [1] \"number of rows in airQuality_3NA_Inserted after dropping is 154\""},{"path":"data-cleaning-with-r.html","id":"impute","chapter":"26 Data cleaning with r","heading":"26.6.2 Impute","text":"Dropping rows straight forward approach. However, dropping many drows induce huge data loss, detrimental task. can insert values specify entries missing values. minimize data loss. Though never know imputed values reflect true pattern data.","code":""},{"path":"data-cleaning-with-r.html","id":"impute-numerical-values","chapter":"26 Data cleaning with r","heading":"26.6.2.1 Impute numerical values","text":"","code":""},{"path":"data-cleaning-with-r.html","id":"impute-with-constant","chapter":"26 Data cleaning with r","heading":"26.6.2.1.1 Impute with constant","text":"column missing value,select constant think reflect real data fill missing entry column constant selected.Let’s say use constant 30 fill “Ozone” entries missing values.now fill:see NA Ozone column row 5 imputed constant 30.Pro\nEasy understand.\nEasy understand.Con\nValue assigned human intuition arbitrary might unrealistic.\nValue assigned human intuition arbitrary might unrealistic.","code":"\nairQuality_const_filled <- airQuality_preProcessing\nprint(paste0(\"number of missing values in Ozone column is \", sum(is.na(airQuality_const_filled$Ozone))))## [1] \"number of missing values in Ozone column is 37\"\nhead(airQuality_const_filled , 5)##   Ozone Solar.R Wind Temp Month Day\n## 1    41     190  7.4   67     5   1\n## 2    36     118  8.0   72     5   2\n## 3    12     149 12.6   74     5   3\n## 4    18     313 11.5   62     5   4\n## 5    NA      NA 14.3   56     5   5\nairQuality_const_filled$Ozone <- na_replace(airQuality_const_filled$Ozone, fill = 30, maxgap = Inf)\nprint(paste0(\"number of missing values in Ozone column is \", sum(is.na(airQuality_const_filled$Ozone))))## [1] \"number of missing values in Ozone column is 0\"\nhead(airQuality_const_filled , 5)##   Ozone Solar.R Wind Temp Month Day\n## 1    41     190  7.4   67     5   1\n## 2    36     118  8.0   72     5   2\n## 3    12     149 12.6   74     5   3\n## 4    18     313 11.5   62     5   4\n## 5    30      NA 14.3   56     5   5"},{"path":"data-cleaning-with-r.html","id":"impute-with-sample-statistics","chapter":"26 Data cleaning with r","heading":"26.6.2.1.2 Impute with Sample statistics","text":"can impute missing values column using sample statistics columns mean median. way fill missing value alter data statistics column. now fill “Ozone” column current “Mean” “Ozone” column “Solar.R” median “Solar.R” column. Note “na.rm = True” arguement neglect missing value computing sample statistics get real value answer.see longer missing values columns mean “Ozone” column median “Solar.R” unchanging impute.Pro\nEasy understand.\nchange sample statistics\nEasy understand.change sample statisticsCon\nMaybe distribution valid values columns skewed left right many outliers sample statistics reflect actual pattern data\nMaybe distribution valid values columns skewed left right many outliers sample statistics reflect actual pattern data","code":"\nairQuality_sstat_impute <- airQuality_preProcessing\nprint(paste0(\"Mean of Ozone before impute is \", mean(airQuality_sstat_impute$Ozone, na.rm = TRUE)))## [1] \"Mean of Ozone before impute is 42.1293103448276\"\nprint(paste0(\"Median of Solar.R before impute is \", median(airQuality_sstat_impute$Solar.R, na.rm = TRUE)))## [1] \"Median of Solar.R before impute is 205\"\nprint(paste0(\"number of missing values in Ozone column before impute is \", sum(is.na(airQuality_sstat_impute$Ozone))))## [1] \"number of missing values in Ozone column before impute is 37\"\nprint(paste0(\"number of missing values in Solar.R column before impute is \", sum(is.na(airQuality_sstat_impute$Solar.R))))## [1] \"number of missing values in Solar.R column before impute is 7\"\nairQuality_sstat_impute$Ozone <- na_mean(airQuality_sstat_impute$Ozone, option = \"mean\", maxgap = Inf)\nairQuality_sstat_impute$Solar.R <- na_mean(airQuality_sstat_impute$Solar, option = \"median\", maxgap = Inf)\n\nprint(paste0(\"Mean of Ozone after impute is \", mean(airQuality_sstat_impute$Ozone)))## [1] \"Mean of Ozone after impute is 42.1293103448276\"\nprint(paste0(\"Median of Solar.R after impute is \", median(airQuality_sstat_impute$Solar.R)))## [1] \"Median of Solar.R after impute is 205\"\nprint(paste0(\"number of missing values in Ozone column after impute is \", sum(is.na(airQuality_sstat_impute$Ozone))))## [1] \"number of missing values in Ozone column after impute is 0\"\nprint(paste0(\"number of missing values in Solar.R column after impute is \", sum(is.na(airQuality_sstat_impute$Solar.R))))## [1] \"number of missing values in Solar.R column after impute is 0\""},{"path":"data-cleaning-with-r.html","id":"impute-by-value-adjacent-to-missing-value","chapter":"26 Data cleaning with r","heading":"26.6.2.1.3 Impute by value adjacent to Missing value","text":"Often time-series data, value time point highly assosicated adjacent valid values. Therefore, want fill missing entries using valid data values .first fill missing values Ozone column last observed valid value:fill missing values Solar.R column next observed valid valueIf missing value beginning end data frame, rare, can remedy using “na_remaining” argument “na_locf()” handle remaining missing value filling. details can found imputeTS documentation.Pro\ntime series data, using adjacent value close capture time dependent pattern\ntime series data, using adjacent value close capture time dependent patternCon\ncloested adjacent value far away, filling adjacent value sustain time dependent pattern.\ncloested adjacent value far away, filling adjacent value sustain time dependent pattern.","code":"\nairQuality_Adjacent_impute <- airQuality_preProcessing\nhead(airQuality_Adjacent_impute, 10)##    Ozone Solar.R Wind Temp Month Day\n## 1     41     190  7.4   67     5   1\n## 2     36     118  8.0   72     5   2\n## 3     12     149 12.6   74     5   3\n## 4     18     313 11.5   62     5   4\n## 5     NA      NA 14.3   56     5   5\n## 6     28      NA 14.9   66     5   6\n## 7     23     299  8.6   65     5   7\n## 8     19      99 13.8   59     5   8\n## 9      8      19 20.1   61     5   9\n## 10    NA     194  8.6   69     5  10\nairQuality_Adjacent_impute$Ozone <- na_locf(airQuality_Adjacent_impute$Ozone, option = \"locf\")\nairQuality_Adjacent_impute$Solar.R <- na_locf(airQuality_Adjacent_impute$Solar.R, option = \"nocb\")\nhead(airQuality_Adjacent_impute, 10)##    Ozone Solar.R Wind Temp Month Day\n## 1     41     190  7.4   67     5   1\n## 2     36     118  8.0   72     5   2\n## 3     12     149 12.6   74     5   3\n## 4     18     313 11.5   62     5   4\n## 5     18     299 14.3   56     5   5\n## 6     28     299 14.9   66     5   6\n## 7     23     299  8.6   65     5   7\n## 8     19      99 13.8   59     5   8\n## 9      8      19 20.1   61     5   9\n## 10     8     194  8.6   69     5  10"},{"path":"data-cleaning-with-r.html","id":"impute-using-predicative-model","chapter":"26 Data cleaning with r","heading":"26.6.2.1.4 Impute using predicative model","text":"believe certain features data frame depends features, can use features fit predicative model predict missing value certain feature. need make sure “features” prediction based valid entries, need impute using method. now assume “Ozone” “Solar.R” features depends climate related features “Wind” “Temperature”, build linear regression models “Ozone” “Solar.R” based “Wind” “Temperature”.Since original data integer, now convert “Ozone” “Solar.R” back integer double.now filled missing values using linear regresison model prediction valid values columns.Pro\nfeature predict indeed depend features data set, missing value replace values reflect true pattern.\nfeature predict indeed depend features data set, missing value replace values reflect true pattern.Con\nDepending predicative model use, training prediction computationally expensive.\nfeature predict depend features data set select construct predicative model, filled missing value mis leading future analysis.\nDepending predicative model use, training prediction computationally expensive.feature predict depend features data set select construct predicative model, filled missing value mis leading future analysis.","code":"\nairQuality_lm_impute <- airQuality_preProcessing\n\n\nhead(airQuality_lm_impute, 10)##    Ozone Solar.R Wind Temp Month Day\n## 1     41     190  7.4   67     5   1\n## 2     36     118  8.0   72     5   2\n## 3     12     149 12.6   74     5   3\n## 4     18     313 11.5   62     5   4\n## 5     NA      NA 14.3   56     5   5\n## 6     28      NA 14.9   66     5   6\n## 7     23     299  8.6   65     5   7\n## 8     19      99 13.8   59     5   8\n## 9      8      19 20.1   61     5   9\n## 10    NA     194  8.6   69     5  10\nprint(paste0('number of missing values in Ozone column before impute is ', sum(is.na(airQuality_lm_impute$Ozone))))## [1] \"number of missing values in Ozone column before impute is 37\"\nprint(paste0('number of missing values in Solar.R column before impute is ', sum(is.na(airQuality_lm_impute$Solar.R))))## [1] \"number of missing values in Solar.R column before impute is 7\"\nlinear_model_Ozone <- lm(Ozone ~ Wind + Temp, data = airQuality_lm_impute)\nlinear_model_Solar.R <- lm(Solar.R ~ Wind + Temp, data = airQuality_lm_impute)\n\nairQuality_lm_impute$Ozone[is.na(airQuality_lm_impute$Ozone)] <- predict(linear_model_Ozone, \n                                                                         newdata = airQuality_lm_impute[is.na(airQuality_lm_impute$Ozone),c('Wind', 'Temp')])\n\n\nairQuality_lm_impute$Solar.R[is.na(airQuality_lm_impute$Solar.R)] <- predict(linear_model_Solar.R, \n                                                                         newdata = airQuality_lm_impute[is.na(airQuality_lm_impute$Solar.R),c('Wind', 'Temp')])\n\nhead(airQuality_lm_impute, 10)##        Ozone  Solar.R Wind Temp Month Day\n## 1   41.00000 190.0000  7.4   67     5   1\n## 2   36.00000 118.0000  8.0   72     5   2\n## 3   12.00000 149.0000 12.6   74     5   3\n## 4   18.00000 313.0000 11.5   62     5   4\n## 5  -11.67673 127.4317 14.3   56     5   5\n## 6   28.00000 159.5042 14.9   66     5   6\n## 7   23.00000 299.0000  8.6   65     5   7\n## 8   19.00000  99.0000 13.8   59     5   8\n## 9    8.00000  19.0000 20.1   61     5   9\n## 10  29.66190 194.0000  8.6   69     5  10\nairQuality_lm_impute$Ozone <- as.integer(airQuality_lm_impute$Ozone)\nairQuality_lm_impute$Solar.R <- as.integer(airQuality_lm_impute$Solar.R)\nhead(airQuality_lm_impute, 10)##    Ozone Solar.R Wind Temp Month Day\n## 1     41     190  7.4   67     5   1\n## 2     36     118  8.0   72     5   2\n## 3     12     149 12.6   74     5   3\n## 4     18     313 11.5   62     5   4\n## 5    -11     127 14.3   56     5   5\n## 6     28     159 14.9   66     5   6\n## 7     23     299  8.6   65     5   7\n## 8     19      99 13.8   59     5   8\n## 9      8      19 20.1   61     5   9\n## 10    29     194  8.6   69     5  10\nlinear_model_Ozone ## \n## Call:\n## lm(formula = Ozone ~ Wind + Temp, data = airQuality_lm_impute)\n## \n## Coefficients:\n## (Intercept)         Wind         Temp  \n##     -71.033       -3.055        1.840\nlinear_model_Solar.R## \n## Call:\n## lm(formula = Solar.R ~ Wind + Temp, data = airQuality_lm_impute)\n## \n## Coefficients:\n## (Intercept)         Wind         Temp  \n##     -76.362        2.211        3.075\nprint(paste0('number of missing values in Ozone column after impute is ', sum(is.na(airQuality_lm_impute$Ozone))))## [1] \"number of missing values in Ozone column after impute is 0\"\nprint(paste0('number of missing values in Solar.R column after impute is ', sum(is.na(airQuality_lm_impute$Solar.R))))## [1] \"number of missing values in Solar.R column after impute is 0\""},{"path":"data-cleaning-with-r.html","id":"impute-categorical-features","chapter":"26 Data cleaning with r","heading":"26.6.2.2 Impute categorical features","text":"imputing categorical features,Replacing NA categorical features stringReplacing NA categorical features mode [1]","code":"\nna_df <- data.frame(A = c(NA, 7, 8, 5, 3),\n                     B = c(4, 10, NA, 7, 4), \n                    fantasy = c(\"sad\", \"we\", NA, 'adf', 'NA'),\n                     C = c(1, 0, NA, 9, NA), \n                     D = c(\"tangro\", \"ok\", NA, 'yes', 'NA'))\ni1 <- !sapply(na_df, is.numeric)\n\nna_df[i1] <- lapply(na_df[i1], function(x)\n              replace(x, is.na(x), 'MISSING'))\nna_df##    A  B fantasy  C       D\n## 1 NA  4     sad  1  tangro\n## 2  7 10      we  0      ok\n## 3  8 NA MISSING NA MISSING\n## 4  5  7     adf  9     yes\n## 5  3  4      NA NA      NA\nna_df <- data.frame(A = c(NA, 7, 8, 5, 3),\n                     B = c(4, 10, NA, 7, 4), \n                    fantasy = c(\"sad\", \"we\", NA, 'adf', 'NA'),\n                     C = c(1, 0, NA, 9, NA), \n                     D = c(\"tangro\", \"ok\", NA, 'yes', 'NA'))\ni1 <- !sapply(na_df, is.numeric)\nMode <- function(x) { \n      ux <- sort(unique(x))\n      ux[which.max(tabulate(match(x, ux)))] \n}\nna_df[i1] <- lapply(na_df[i1], function(x)\n              replace(x, is.na(x), Mode(x[!is.na(x)])))\nna_df##    A  B fantasy  C      D\n## 1 NA  4     sad  1 tangro\n## 2  7 10      we  0     ok\n## 3  8 NA     adf NA     NA\n## 4  5  7     adf  9    yes\n## 5  3  4      NA NA     NA"},{"path":"data-cleaning-with-r.html","id":"data-scaling","chapter":"26 Data cleaning with r","heading":"26.7 Data scaling","text":"Scaling important data analysis exploration, Data without scaling can produce misleading result. Also scaling beneficial mathematics computation machine learning.","code":""},{"path":"data-cleaning-with-r.html","id":"standardization","chapter":"26 Data cleaning with r","heading":"26.7.1 standardization","text":"Standardization unify samples’ mean std 0 1, compare distribution among scaled samples.","code":"\n# we can use the scale from dplyr\nlibrary(dplyr)\nlibrary(openintro)\nscale_df = duke_forest_copy[, 2:5]\n# what happen if we did not scale.\nboxplot(scale_df)\n# after scaling\nscale_df %>% mutate_all(~(scale(.) %>% as.vector)) %>% boxplot() # apply scale() to every column"},{"path":"data-cleaning-with-r.html","id":"min-max-scaling","chapter":"26 Data cleaning with r","heading":"26.7.2 Min Max scaling","text":"MINMAX scaling good svm, don’t hesistate use :)","code":"\nminmax <- function(x, na.rm = TRUE) {\n    return((x- min(x)) /(max(x)-min(x)))\n}\nscale_df = duke_forest_copy[, 2:5]\n# what happen if we did not scale.\nboxplot(scale_df)\nscale_df %>% mutate_all(~(minmax(.) %>% as.vector)) %>% boxplot() # apply minmax() to every column"},{"path":"data-cleaning-with-r.html","id":"data-encoding","chapter":"26 Data cleaning with r","heading":"26.8 data encoding","text":"","code":""},{"path":"data-cleaning-with-r.html","id":"ordinal-encoding","chapter":"26 Data cleaning with r","heading":"26.8.1 ordinal encoding","text":"","code":"\nordinal <- function(x, order= unique(x)) {\n  x <- as.numeric(factor(x, levels = order, exclude = NULL))\n  x\n}\nencode_df <- data.frame(A = c(1, 7, 8, 5, 3),\n                     B = c(4, 10, NA, 7, 4), \n                    fantasy = c(\"sad\", \"we\", NA, 'adf', 'NA'),\n                     C = c(1, 0, NA, 9, NA), \n                     D = c(\"tangro\", \"ok\", 'NA', 'yes', 'NA'))\nprint('origin:')## [1] \"origin:\"\nc(encode_df[\"D\"])## $D\n## [1] \"tangro\" \"ok\"     \"NA\"     \"yes\"    \"NA\"\nprint('encoded:')## [1] \"encoded:\"\nordinal(encode_df[[\"D\"]])## [1] 1 2 3 4 3"},{"path":"data-cleaning-with-r.html","id":"one-hot-encoding-3","chapter":"26 Data cleaning with r","heading":"26.8.2 one-hot encoding [3]","text":"","code":"\nencode_df <- data.frame(A = c(1, 7, 8, 5, 3),\n                     B = c(4, 10, NA, 7, 4), \n                    fantasy = c(\"sad\", \"we\", \"seaweed\", 'adf', 'NA'),\n                     C = c(1, 0, NA, 9, NA), \n                     D = c(\"tangro\", \"ok\", 'NA', 'yes', 'NA'))\n\n\ndummy <- dummyVars(\" ~ .\", data=encode_df)\nnewdata <- data.frame(predict(dummy, newdata = encode_df)) \nnewdata##   A  B fantasyadf fantasyNA fantasysad fantasyseaweed fantasywe  C DNA Dok\n## 1 1  4          0         0          1              0         0  1   0   0\n## 2 7 10          0         0          0              0         1  0   0   1\n## 3 8 NA          0         0          0              1         0 NA   1   0\n## 4 5  7          1         0          0              0         0  9   0   0\n## 5 3  4          0         1          0              0         0 NA   1   0\n##   Dtangro Dyes\n## 1       1    0\n## 2       0    0\n## 3       0    0\n## 4       0    1\n## 5       0    0"},{"path":"data-cleaning-with-r.html","id":"reference-1","chapter":"26 Data cleaning with r","heading":"26.9 Reference","text":"[1] https://sparkbyexamples.com/r-programming/---left-join--r/#:~:text=%20to%20do%20left%20join%20on%20data%20frames%20in%20R,join%20data%20frames%20in%20R.[2]https://stackoverflow.com/questions/36377813/impute--frequent-categorical-value---columns--data-frame[3] https://datatricks.co.uk/one-hot-encoding--r-three-simple-methods","code":""},{"path":"introduction-to-shiny-web-apps.html","id":"introduction-to-shiny-web-apps","chapter":"27 Introduction to Shiny Web Apps","heading":"27 Introduction to Shiny Web Apps","text":"Wenxi Zhang (wz2615)","code":""},{"path":"introduction-to-shiny-web-apps.html","id":"motivation-3","chapter":"27 Introduction to Shiny Web Apps","heading":"27.1 Motivation","text":"Shiny R package R studio makes easy build interactive web apps straight R. Shiny allows create highly effective data reports visualization user can explore data set specifying data subset.think important address shiny tutorial following ways:Package analysis format users can easily ingest. data scientist, common present work progress within organization. Shiny makes easy deliver data-driven conclusions users don’t use R without technical background.Package analysis format users can easily ingest. data scientist, common present work progress within organization. Shiny makes easy deliver data-driven conclusions users don’t use R without technical background.official tutorial isn’t user friendly someone first learns shiny. provides comprehensive deep concepts Shiny take long finish whole lessons. Thus include heavily used functions concepts shiny give viewers general practical exposure using Shiny.official tutorial isn’t user friendly someone first learns shiny. provides comprehensive deep concepts Shiny take long finish whole lessons. Thus include heavily used functions concepts shiny give viewers general practical exposure using Shiny.Prototyping. Shiny provides rapid way prototyping shiny dashboards. user interface requirements often changes project progress, often easier determine requirements using prototype. Shiny, developers can quickly change edit Shiny dashboards, Shiny framework includes set pre-configured functions generate panels widgets. requirements settled, move Shiny app formal web development framework.Prototyping. Shiny provides rapid way prototyping shiny dashboards. user interface requirements often changes project progress, often easier determine requirements using prototype. Shiny, developers can quickly change edit Shiny dashboards, Shiny framework includes set pre-configured functions generate panels widgets. requirements settled, move Shiny app formal web development framework.Performing daily data analytics task. can save R code data analytics job Shiny. users background R, can also perform data manipulation visualization task. allows daily analytics task move away data scientists focus new challenges business decisions.Performing daily data analytics task. can save R code data analytics job Shiny. users background R, can also perform data manipulation visualization task. allows daily analytics task move away data scientists focus new challenges business decisions.","code":""},{"path":"introduction-to-shiny-web-apps.html","id":"how-to-host","chapter":"27 Introduction to Shiny Web Apps","heading":"27.2 How to host","text":"multiple way host shiny app.Install shiny server computerUpload app https://www.shinyapps.io/\nmonth 25 free active hours\nMaximum 5 apps free\nmonth 25 free active hoursMaximum 5 apps freeGoogle Cloud Run: https://code.markedmondson./shiny-cloudrun/","code":""},{"path":"introduction-to-shiny-web-apps.html","id":"create-your-first-shiny-app","chapter":"27 Introduction to Shiny Web Apps","heading":"27.3 Create your first Shiny app","text":"focus creating Shiny app Rstudio.\nseveral ways :File -> New File -> Shiny Web App\nSingle file ( tutorial uses)\nMultiple files\nSingle file ( tutorial uses)Multiple filesFile -> New File -> R Markdown -> ShinyFile -> New File -> R Markdown -> Template -> Flex Dashboard","code":""},{"path":"introduction-to-shiny-web-apps.html","id":"basic-struture","chapter":"27 Introduction to Shiny Web Apps","heading":"27.4 Basic Struture","text":"Shiny applications divided three parts:User Interface (UI)User Interface (UI)ServerServera call shinyApp functiona call shinyApp functionThe user interface (ui) responsible app presentation, server responsible app logic. Finally shinyApp() function creates Shiny app objects explicit UI/server pair.UI controls lay components displayed application page, like text, plots, widgets take user input.\nserver controls data displayed UI. ’s load wrangle data, transforming input UI outputs.simple example","code":"\nlibrary(shiny)\nui <- fluidPage(\n  # controls the layout and content of the application\n)\nserver <- function(input, output) {\n   # controls the interaction, modify output based on user input\n}\n#run app\nshinyApp(ui = ui, server = server)"},{"path":"introduction-to-shiny-web-apps.html","id":"the-ui","chapter":"27 Introduction to Shiny Web Apps","heading":"27.5 The UI","text":"may lay user interface app placing elements fluidPage function. Elements can seperated inputs outputs function.Next create visualization tool display k-means clustering Iris dataset. , need define UI panel return scatterplot.\nIris data set contains 3 classes 50 instances , class refers type iris plant. feature set contains “Sepal.Length”, “Sepal.Width”, “Petal.Length”, “Petal.Width”","code":""},{"path":"introduction-to-shiny-web-apps.html","id":"layout","chapter":"27 Introduction to Shiny Web Apps","heading":"27.5.1 layout","text":"First, let’s create UI panel left side. need define three panels: title, sidebar users’ input, main visualization work go.titlePanel sidebarLayout two popular elements create layout.titlePanel basically put App title .\nsidebarLayout two arguments: sidebarPanel mainPanel, place content sidebar main panels shown example1. can put values like text input/output functions Panels.","code":"\nui <- fluidPage(\n  titlePanel(\"title panel\"),\n\n  sidebarLayout(\n    sidebarPanel(\"sidebar panel\"),\n    mainPanel(\"main panel\")\n  )\n)"},{"path":"introduction-to-shiny-web-apps.html","id":"input-functions","chapter":"27 Introduction to Shiny Web Apps","heading":"27.5.2 Input functions","text":"can also add control widgets provide way users send messages Shiny app. Shiny widgets collect value user. user changes widget, value change well.many input functions create widgets. examples basic widgets.Next introduce common input function selectInput() shows box choices select .\nselectInput() 3 required arguments:\ninputId: input name access value (internal ues–> use input$inputId refer input value server)\nlabel: Label shown UI, NULL label.\nchoices: List values select .\ncan also use selected set initial selected value user specify.include select widget allows user select y variable output plot. also define variables choose attributes feature set, set default choice ‘Sepal.Width’. Similarly, can define selection box x variable numeric Input box user can define cluster numbers clustering example.","code":"\nselectInput(inputID='ycol', label='Y Variable', \n            choices=names(iris)[1:4],#\"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\" \n      selected = names(iris)[[2]])# set \"Petal.Length\" to be default"},{"path":"introduction-to-shiny-web-apps.html","id":"output-functions","chapter":"27 Introduction to Shiny Web Apps","heading":"27.5.3 Output functions","text":"display output, add output function fluidPage() *Output() function. list output functions turn R objects specific type outputJust like elements ui, can add output user interface inside sidebarPanel mainPanel.\nexample, place output mainPanel output scatter plot user defined x, y variables. may add plotOutput('plot') mainPanel create plot object, \"plot\" name given output plot. numericInput() used choosing cluster numbers, users allowed enter cluster numbers 1 9, default 3 clusters.complete ui portion Iris clustering problem.","code":"\nui <- fluidPage(\n  headerPanel('Iris k-means clustering'),\n  sidebarPanel(\n    selectInput('xcol', 'X Variable', names(iris)[1:4]),\n    selectInput('ycol', 'Y Variable', names(iris)[1:4],\n                selected = names(iris)[[2]]),\n    numericInput('clusters', 'Cluster count', value=3,#value: initial value\n                 min = 1, max = 9)#Minimum allowed value, Maximum allowed value\n  ),\n  mainPanel(\n    plotOutput('plot')\n  )\n)"},{"path":"introduction-to-shiny-web-apps.html","id":"the-server","chapter":"27 Introduction to Shiny Web Apps","heading":"27.6 The Server","text":"adding output object ui, need tell Shiny build object server function. , need toAccess input values input$ (example, input$xcol, input$ycol, input$clusters)Access input values input$ (example, input$xcol, input$ycol, input$clusters)builds list-like object named output contains code needed update R objects app. (Save objects display output$. case output output$plot)builds list-like object named output contains code needed update R objects app. (Save objects display output$. case output output$plot)Define text/charts/plots server function (Build objects display render*())Define text/charts/plots server function (Build objects display render*())entry output contain output one Shiny’s render* functions. functions capture R expression light pre-processing expression. render*() function creates type output wish make. Shiny re-run function every time needs update object.\ncommon render functions descriptions.following rules , reactivity automatically occurs whenever use input value build rendered Outputs.","code":""},{"path":"introduction-to-shiny-web-apps.html","id":"reactivity","chapter":"27 Introduction to Shiny Web Apps","heading":"27.6.1 Reactivity","text":"know input value changes whenever user changes input. input values notify render* functions R expression inside render*() input changes. notified input changes, object created render*() function return entire code block update .However, computational costy update R expression inside render*() whenever input updates. example, want use modified input several times, duplicate R expressions modifying input slows program. Thus better save reusable R expressions modifies input function. Luckily, reactive() function shiny job!reactive() builds reactive object respond every input value code. can save R expressions modifies input values reactive() call reactive expression like function.example, can save user specified two feature columns Iris dataset selectedData calculate k-mans model clusters base selectedData user specified cluster numbers.\nMoreover, selectedData won’t change input$xcol input$ycol aren’t changed. can avoid unnecessary computation putting expressions final render*() function, don’t need update iris[, c(input$xcol, input$ycol)] whenever input$clusters changes.","code":"\n  selectedData <- reactive({\n    iris[, c(input$xcol, input$ycol)]\n  })\n\n  clusters <- reactive({\n    kmeans(selectedData(), input$clusters)\n  })"},{"path":"introduction-to-shiny-web-apps.html","id":"display-output-with-render","chapter":"27 Introduction to Shiny Web Apps","heading":"27.6.2 Display output with render*()","text":"example, need define plot output. Thus use render function renderPlot() .discussed , renderPlot() respond every reactive value code, selectedData() clusters(). use two functions input data renderPlot(). two functions change, renderPlot() rerun r expressions inside.Now everything connected! overall server section looks like . Finally, need one statement app running –>shinyApp(ui = ui, server = server)","code":"\n  output$plot <- renderPlot({\n    par(mar = c(5.1, 4.1, 0, 1))\n    plot(selectedData(),\n         col = clusters()$cluster,\n         pch = 20, cex = 3)\n    points(clusters()$centers, pch = 4, cex = 4, lwd = 4)\n  })\nserver <- function(input, output) {\n\n  selectedData <- reactive({\n    iris[, c(input$xcol, input$ycol)]\n  })\n\n  clusters <- reactive({\n    kmeans(selectedData(), input$clusters)\n  })\n\n  output$plot <- renderPlot({\n    par(mar = c(5.1, 4.1, 0, 1))\n    plot(selectedData(),\n         col = clusters()$cluster,\n         pch = 20, cex = 3)\n    points(clusters()$centers, pch = 4, cex = 4, lwd = 4)\n  })\n\n\n}"},{"path":"introduction-to-shiny-web-apps.html","id":"conclusion-3","chapter":"27 Introduction to Shiny Web Apps","heading":"27.7 Conclusion","text":"general, Shiny great option presenting results combining visualization tool using R expressions. Though example went , covered basic structure Shiny, manipulate layout UI, connect logic behind input s outputs server function.Functions introduced used functions shiny. powerful interesting aspects shiny app like customizing appearance internal functions. highly recommend visit https://shiny.rstudio.com/ documentation gallery.","code":""},{"path":"introduction-to-shiny-web-apps.html","id":"evaluation","chapter":"27 Introduction to Shiny Web Apps","heading":"27.8 Evaluation","text":"experience dive deep learning shiny experimenting different data sets, find shiny powerful easy learn package generate intuitive interactive web apps, especially interactive plots. want report ongoing project, shiny great tool visualizing impact work. Just like example visualizing k-means clusters, gives users intuitive way understanding k-means clustering.also learnt Shiny unlimited possibilities improving efficiency work, creating interactive presentations project result solving daily analytic task. Moreover, Shiny enables users R background data analyzing work, manipulating widgets UI.Things might differently next time including complex useful internal functions server part, customizing appearance Shiny’s UI components using differnt themes.","code":""},{"path":"introduction-to-shiny-web-apps.html","id":"reference-2","chapter":"27 Introduction to Shiny Web Apps","heading":"27.9 Reference","text":"https://towardsdatascience.com/beginners-guide--creating--r-shiny-app-1664387d95b3https://shiny.rstudio.com/tutorial/","code":""},{"path":"python-missingno-library-tutorial.html","id":"python-missingno-library-tutorial","chapter":"28 Python missingno library tutorial","heading":"28 Python missingno library tutorial","text":"Yuyang Zhuo, Jiahao LaiCleaning data always take half time model-fitting project. deal missing data major problem cleaning data.\n, missingno Python library provides various ways visualize missing data help users dig reasons data complete.Please check github repo see tutorial:https://github.com/Jiahao-B-Lai/EDAV-CC-Group16/blob/main/5702_CC_Group16.ipynb","code":""},{"path":"data-explorer-tutorial-and-automation.html","id":"data-explorer-tutorial-and-automation","chapter":"29 Data Explorer Tutorial and Automation","heading":"29 Data Explorer Tutorial and Automation","text":"Akshay IyengarI created video tutorial shortcut known data explorer. package talked class, emphasized useful fast data exploration package. video teach classmates automation can useful useful time.especially one many packages help creating general visualizations data scientists.Youtube LinkSources:CRAN\nRpubs","code":""},{"path":"d-visualization-in-r.html","id":"d-visualization-in-r","chapter":"30 3D Visualization in R","heading":"30 3D Visualization in R","text":"Tianyu Han Shijia Huang","code":""},{"path":"d-visualization-in-r.html","id":"motivation-4","chapter":"30 3D Visualization in R","heading":"30.1 Motivation","text":"important part data visualization, 3D plotting makes data exploration part easier users allow visual display datasets.plotting data points three axes, 3D plots describe relationship three variables useful identify underlying patterns interactions shown 2D graphs.tutorial, introduce different packages 3D plots, including package Scatterplot3D, package plot3D, also plotly. end tutorial, one able choose suitable package /project.can learn project always different ways tackle problem using different R libraries packages. require extensive research trials order determine one works best given scenario.","code":""},{"path":"d-visualization-in-r.html","id":"scatterplot3d","chapter":"30 3D Visualization in R","heading":"30.2 Scatterplot3D","text":"Scatterplot3d R package displays multidimensional data 3D space.one function scatterplot3d() package.usage scatterplot3d() discussed examples .","code":""},{"path":"d-visualization-in-r.html","id":"load-data","chapter":"30 3D Visualization in R","heading":"30.2.1 Load Data","text":"use preloaded dataset USArrests example show information can draw 3D plot using scatterplot3d.","code":"\ndata(\"USArrests\")\nhead(USArrests)##            Murder Assault UrbanPop Rape\n## Alabama      13.2     236       58 21.2\n## Alaska       10.0     263       48 44.5\n## Arizona       8.1     294       80 31.0\n## Arkansas      8.8     190       50 19.5\n## California    9.0     276       91 40.6\n## Colorado      7.9     204       78 38.7"},{"path":"d-visualization-in-r.html","id":"create-matrix","chapter":"30 3D Visualization in R","heading":"30.2.2 Create matrix","text":"Scatterplot3d, dataframe provided must converted matrix. select Assault, Urban Population, Rape three axes.","code":"\nUSArrestsMatrix <- as.matrix(USArrests)\nx1 <- USArrestsMatrix[,2] ## Assault\ny1 <- USArrestsMatrix[,3] ## Urban Population\nz1 <- USArrestsMatrix[,4] ## Rape"},{"path":"d-visualization-in-r.html","id":"generate-3d-scatter-plot","chapter":"30 3D Visualization in R","heading":"30.2.3 Generate 3d scatter plot","text":"Creating graph using scatterplot3d. “highlight” gives color scale enables users understand relative position data point. “pch” specifies plotting shape, set pch = 16, small dot.can also remove box (grid) graph change color points. Note setting color, “highlight.3d” argument specified FALSEAdding labels graph, “cex” specifies font size.","code":"\nsp1 <- scatterplot3d(x1,y1,z1, highlight.3d = TRUE, pch = 16, angle = 45,\n                     xlab = \"Assault\",\n                     ylab = \"Urban Population\",\n                     zlab = \"Rape\")\nsp2 <- scatterplot3d(x1,y1,z1,  pch = 16, angle = 45,highlight.3d = FALSE,\n                     xlab = \"Assault\",\n                     ylab = \"Urban Population\",\n                     zlab = \"Rape\",\n                     grid = TRUE,\n                     box = FALSE,\n                     color = c(\"pink\")) \nsp3 <- scatterplot3d(x1,y1,z1, highlight.3d = TRUE, pch = 18, angle = 45,\n                     xlab = \"Assault\",\n                     ylab = \"Urban Population\",\n                     zlab = \"Rape\")\ntext(sp3$xyz.convert(USArrests[2:4]),labels = rownames(USArrests), cex = 0.5, color = 'pink')"},{"path":"d-visualization-in-r.html","id":"d-scatter-plot-with-x-y-plane-position","chapter":"30 3D Visualization in R","heading":"30.2.4 3D scatter plot with x-y plane position","text":"Use “Type = ‘h’” create vertical lines data point x-y plane.","code":"\nsp4 <- scatterplot3d(x1,y1,z1, highlight.3d = TRUE, pch = 18, angle = 45,\n                     xlab = \"Assault\",\n                     ylab = \"Urban Population\",\n                     zlab = \"Rape\",\n                     type = \"h\")"},{"path":"d-visualization-in-r.html","id":"d-plot-and-pca","chapter":"30 3D Visualization in R","heading":"30.3 3D plot and PCA","text":"data science, 3D plot can also used machine learning steps. example, plotting principal components 3D space, efficiently observe interaction important vectors input data.use preloaded data “Glass” perform principal component analysis 3D visualization components.","code":""},{"path":"d-visualization-in-r.html","id":"load-package-mlbench-and-use-the-glass-dataset.","chapter":"30 3D Visualization in R","heading":"30.3.1 Load Package “mlbench” and use the Glass dataset.","text":"","code":"\ndata(Glass)\nhead(Glass)##        RI    Na   Mg   Al    Si    K   Ca Ba   Fe Type\n## 1 1.52101 13.64 4.49 1.10 71.78 0.06 8.75  0 0.00    1\n## 2 1.51761 13.89 3.60 1.36 72.73 0.48 7.83  0 0.00    1\n## 3 1.51618 13.53 3.55 1.54 72.99 0.39 7.78  0 0.00    1\n## 4 1.51766 13.21 3.69 1.29 72.61 0.57 8.22  0 0.00    1\n## 5 1.51742 13.27 3.62 1.24 73.08 0.55 8.07  0 0.00    1\n## 6 1.51596 12.79 3.61 1.62 72.97 0.64 8.07  0 0.26    1"},{"path":"d-visualization-in-r.html","id":"data-cleaning","chapter":"30 3D Visualization in R","heading":"30.3.2 Data Cleaning","text":"Perform PCA dataset convert pca result dataframe. plot three components PCA results.Specify three colors .”shape” specifies three different shapes component.","code":"\nresults <- prcomp(Glass[,2:4], scale = TRUE)\nresults$rotation <- -1*results$rotation\nresults$rotation##           PC1        PC2       PC3\n## Na  0.4381565 -0.8763587 0.2000358\n## Mg -0.6582364 -0.1612544 0.7353380\n## Al  0.6121632  0.4538639 0.6475058\nresults$x <- -1*results$x\nhead(results$x)##          PC1         PC2       PC3\n## 1 -1.1222507 -0.76451910 0.5299814\n## 2 -0.2631728 -0.69696062 0.4746961\n## 3 -0.2128159 -0.14139775 0.5944634\n## 4 -0.7549327 -0.04089696 0.2632213\n## 5 -0.7521008 -0.14291459 0.1773877\n## 6 -0.5391614  0.71876865 0.5475329\npca.result <- results$x\npca.result <-data.frame(pca.result)\nhead(pca.result)##          PC1         PC2       PC3\n## 1 -1.1222507 -0.76451910 0.5299814\n## 2 -0.2631728 -0.69696062 0.4746961\n## 3 -0.2128159 -0.14139775 0.5944634\n## 4 -0.7549327 -0.04089696 0.2632213\n## 5 -0.7521008 -0.14291459 0.1773877\n## 6 -0.5391614  0.71876865 0.5475329\npca.result$Type <- (Glass$Type)"},{"path":"d-visualization-in-r.html","id":"define-color-and-shape-parameter.","chapter":"30 3D Visualization in R","heading":"30.3.3 Define color and shape parameter.","text":"","code":"\n## choose 6 colors for 6 glass types\ncolors <- c(\"#E69F00\", \"#56B4E9\",\"#B2182B\",\"#D1E5F0\",\"#92C5DE\",\"#2166AC\")\ncolors <- colors[as.numeric(pca.result$Type)]\n## choose 6 shapes for 6 glass types\nshape<-10:15\nshape<-shape[as.numeric(pca.result$Type)]"},{"path":"d-visualization-in-r.html","id":"generate-graph","chapter":"30 3D Visualization in R","heading":"30.3.4 Generate graph","text":"Plot result PCA analysis following step previous part. Adjust angle best visualization. example 3D plot can help us see contribution component classifying types glass.","code":"\nPCA3D <- scatterplot3d(pca.result[,1:3],\n                     color=colors,\n                     pch = shape, \n                     cex.symbols = 3,\n                     angle = 100)\nlegend(\"top\", legend = levels(pca.result$Type),\n       col =   c(\"#E69F00\", \"#56B4E9\",\"#B2182B\",\"#D1E5F0\",\"#92C5DE\",\"#2166AC\"),\n       pch = c(10,11,12,13,14), \n       inset = -0.1, xpd = TRUE, horiz = TRUE)"},{"path":"d-visualization-in-r.html","id":"other-usage-of-the-scatterplot3d-function","chapter":"30 3D Visualization in R","heading":"30.4 Other usage of the scatterplot3d function","text":"Sometimes hard imagine relationship two functions graph, plotting 3D space, visualize interaction dynamic environment.simple example graph interaction cos sin function.","code":"\nz <- seq(-15, 15, 0.05)\nx <- cos(z)\ny <- sin(z)\nscatterplot3d(x, y, z, highlight.3d=TRUE, col.axis =\"blue\",col.grid =\"lightblue\", main=\"an example of cosine and sine interaction\", pch=20)"},{"path":"d-visualization-in-r.html","id":"d-histogram","chapter":"30 3D Visualization in R","heading":"30.5 3D Histogram","text":"generate histogram 3d, can use plot 3D package. first initiate x-axis y-axis. , need create z matrix dimension |x| * |y|. can use hist3D function package help us generate 3D histogram need.","code":"\nx = c(1, 2)\ny = c(1, 2)\nz = c(1, 2, 2, 3)\nmat1 <- matrix(z,nrow=2,ncol=2,byrow=TRUE)\nhist3D(z=mat1, x = x, y= y)"},{"path":"d-visualization-in-r.html","id":"d-scatter-plot-using-plotly","chapter":"30 3D Visualization in R","heading":"30.6 3D scatter plot using plotly","text":"","code":""},{"path":"d-visualization-in-r.html","id":"demo-data","chapter":"30 3D Visualization in R","heading":"30.6.1 Demo Data","text":"order better demonstrate different features plotly 3D Scatterplot, selected sample data includes 40 observations household expenditure single men women. 5 variables observation:Housing: money(usd) spent housingFood: money(usd) spent foodGoods: money(usd) spent goodsService: money(usd) spent serviceGender: female male","code":"\nhousehold##    housing food goods service gender\n## 1      820  114   183     154 female\n## 2      184   74     6      20 female\n## 3      921   66  1686     455 female\n## 4      488   80   103     115 female\n## 5      721   83   176     104 female\n## 6      614   55   441     193 female\n## 7      801   56   357     214 female\n## 8      396   59    61      80 female\n## 9      864   65  1618     352 female\n## 10     845   64  1935     414 female\n## 11     404   97    33      47 female\n## 12     781   47  1906     452 female\n## 13     457  103   136     108 female\n## 14    1029   71   244     189 female\n## 15    1047   90   653     298 female\n## 16     552   91   185     158 female\n## 17     718  104   583     304 female\n## 18     495  114    65      74 female\n## 19     382   77   230     147 female\n## 20    1090   59   313     177 female\n## 21     497  591   153     291   male\n## 22     839  942   302     365   male\n## 23     798 1308   668     584   male\n## 24     892  842   287     395   male\n## 25    1585  781  2476    1740   male\n## 26     755  764   428     438   male\n## 27     388  655   153     233   male\n## 28     617  879   757     719   male\n## 29     248  438    22      65   male\n## 30    1641  440  6471    2063   male\n## 31    1180 1243   768     813   male\n## 32     619  684    99     204   male\n## 33     253  422    15      48   male\n## 34     661  739    71     188   male\n## 35    1981  869  1489    1032   male\n## 36    1746  746  2662    1594   male\n## 37    1865  915  5184    1767   male\n## 38     238  522    29      75   male\n## 39    1199 1095   261     344   male\n## 40    1524  964  1739    1410   male"},{"path":"d-visualization-in-r.html","id":"the-classic-3d-scatterplot","chapter":"30 3D Visualization in R","heading":"30.6.2 The classic 3D Scatterplot","text":"","code":"\nfig <- plot_ly(household, x = ~housing, y = ~food, z = ~goods + service)\nfig <- fig %>% layout(scene = list(xaxis = list(title = 'housing'),\n                                   yaxis = list(title = 'food'),\n                                   zaxis = list(title = 'goods and services')))\nfig"},{"path":"d-visualization-in-r.html","id":"adding-colors-to-3d-scatterplot","chapter":"30 3D Visualization in R","heading":"30.6.3 Adding colors to 3D Scatterplot","text":"order differentiate observations opposite genders, need add colors 3D scatter plot. done followed:","code":"\nfig <- plot_ly(household, x = ~housing, y = ~food, z = ~goods + service,\n               color = ~gender, colors = c('#17becf', '#d62728'))\nfig <- fig %>% add_markers()\nfig <- fig %>% layout(scene = list(xaxis = list(title = 'housing'),\n                                   yaxis = list(title = 'food'),\n                                   zaxis = list(title = 'goods and services')))\nfig"},{"path":"d-visualization-in-r.html","id":"adding-sizes-to-3d-scatterplot","chapter":"30 3D Visualization in R","heading":"30.6.4 Adding sizes to 3D Scatterplot","text":"interesting note size available fifth parameter helps us plot findings. example, used size plot overall expenditure household. help us visualize overall trend better.","code":"\nfig <- plot_ly(household, x = ~housing, y = ~food, z = ~goods + service,\n               color = ~gender, colors = c('#2ca02c', '#8c564b'), size = ~ housing + food + goods + service, sizes = c(500, 5000))\nfig <- fig %>% add_markers()\nfig <- fig %>% layout(scene = list(xaxis = list(title = 'housing'),\n                                   yaxis = list(title = 'food'),\n                                   zaxis = list(title = 'goods and services')))\nfig"},{"path":"d-visualization-in-r.html","id":"conclusion-4","chapter":"30 3D Visualization in R","heading":"30.7 Conclusion","text":"tutorial, introduced scatterplot3d, plot3d, also plotly. respective advantages. need interactive graph allows zooming rotating, plotly better choice. However, perform principal component analysis better visualize results, easier use scatterplot3d.","code":""},{"path":"d-visualization-in-r.html","id":"works-cited","chapter":"30 3D Visualization in R","heading":"30.8 Works Cited","text":"Ligges, Uwe, Martin Mächler. “Scatterplot3d - R Package Visualizing Multivariate Data.” Journal Statistical Software, vol. 8, . 11, Foundation Open Access Statistic, 2003, https://doi.org/10.18637/jss.v008.i11.http://www.sthda.com/english/wiki/scatterplot3d-3d-graphics-r-software--data-visualizationhttp://www.sthda.com/english/wiki/colors--r#:~:text=%20R%2C%20colors%20can%20be,taken%20from%20the%20RColorBrewer%20package.https://www.statology.org/principal-components-analysis--r/https://plotly.com/r/3d-scatter-plots/http://www.countbio.com/web_pages/left_object/R_for_biology/R_fundamentals/3D_histograms_R.html","code":""},{"path":"visualizing-time-series-data.html","id":"visualizing-time-series-data","chapter":"31 Visualizing Time Series Data","heading":"31 Visualizing Time Series Data","text":"Kate Lassiter","code":""},{"path":"visualizing-time-series-data.html","id":"starting-point","chapter":"31 Visualizing Time Series Data","heading":"31.0.0.1 Starting Point","text":"exploratory questions new data set:Strongly correlated columnsVariable meansSample variance, etc.Use familiar techniques:Summary statisticsHistogramsScatter plots, etc.careful lookahead!Incorporating information future past smoothing, prediction, etc. shouldn’t know yetCan happen time-shifting, smoothing, imputing dataCan bias model make predictions worthless","code":""},{"path":"visualizing-time-series-data.html","id":"working-with-time-series-ts-objects","chapter":"31 Visualizing Time Series Data","heading":"31.0.0.2 Working with time series (ts) objects","text":"Integration ts() objects ggplot2:ggfortify package\nautoplot()\ncustomizations ggplot2\nDon’t convert ts dataframe format\nautoplot()customizations ggplot2Don’t convert ts dataframe formatgridExtra package\nArrange 4 ggplot plots 4-panel grid\nArrange 4 ggplot plots 4-panel gridgrid package\nAdd title grid arrangement\nAdd title grid arrangement","code":"\ndax=autoplot(EuStockMarkets[,\"DAX\"])+\n  ylab(\"Price\")+\n  xlab(\"Time\")\n\ncac=autoplot(EuStockMarkets[,\"CAC\"])+\n  ylab(\"Price\")+\n  xlab(\"Time\")\n\nsmi=autoplot(EuStockMarkets[,\"SMI\"])+\n  ylab(\"Price\")+\n  xlab(\"Time\")\n\nftse=autoplot(EuStockMarkets[,\"FTSE\"])+\n  ylab(\"Price\")+\n  xlab(\"Time\")\ngrid.arrange(dax,cac,smi,ftse,top=textGrob(\"Stock market prices\"))"},{"path":"visualizing-time-series-data.html","id":"time-series-relevant-plotting","chapter":"31 Visualizing Time Series Data","heading":"31.0.0.3 Time series relevant plotting:","text":"Working data:Directly transforming ts() objects use ggplot2:complete.cases() easily remove NA rows - prevent ggplot warning\navoid irritations working ts objects\ncomplete.cases() easily remove NA rows - prevent ggplot warningavoid irritations working ts objectsLooking changes time:Plot differenced values\nHistogram/scatter plot lagged data\nShows change values, values change together\nTrend can hide true relationship, make two series appear highly predictive one another move together\nUse base package diff(), calculates difference point time t t+1\nHistogram/scatter plot lagged dataShows change values, values change togetherTrend can hide true relationship, make two series appear highly predictive one another move togetherUse base package diff(), calculates difference point time t t+1Exploring Time Lags:Lagged differences:Time series analysis: focused predicting future values past\nConcerned whether change one variable time t predicts change another variable time t+1\nlag() shift forward one\nShowing density using alpha\nTime series analysis: focused predicting future values pastConcerned whether change one variable time t predicts change another variable time t+1lag() shift forward oneShowing density using alphaNow apparent relationship: positive change SMI today won’t predict positive change DAX tomorrow. positive trend long term, little predict short termObservations:careful time series data: use techniques, reshape dataChange values one time another vital concept","code":"\nnew=as.data.frame(EuStockMarkets)\nnew$SMI_diff=c(NA,diff(new$SMI))\nnew$DAX_diff=c(NA,diff(new$DAX))\n\np1 <- ggplot(new, aes(SMI,DAX))+\n  geom_point(shape = 21, colour = \"black\", fill = \"white\")\np2 <- ggplot(new[complete.cases(new),], aes(SMI_diff,DAX_diff))+\n  geom_point(shape = 21, colour = \"black\", fill = \"white\")\n\ngrid.arrange(p1,p2,top=textGrob(\"SMI vs DAX\"))\nnew$SMI_lag_diff=c(NA,lag(diff(new$SMI),1))\nggplot(new[complete.cases(new),], aes(SMI_lag_diff,DAX_diff))+\n  geom_point(shape = 21, colour = \"black\", fill = \"white\",alpha=0.4,size=2)"},{"path":"visualizing-time-series-data.html","id":"dynamics-of-time-series-data","chapter":"31 Visualizing Time Series Data","heading":"31.0.0.4 Dynamics of Time Series Data","text":"Three aspects time series data:Seasonal:\nRecurs fixed period\nRecurs fixed periodCycle:\nRecurrent behaviors, variable time period\nRecurrent behaviors, variable time periodTrend:\nOverall drift higher/lower values long time period\nOverall drift higher/lower values long time period","code":""},{"path":"visualizing-time-series-data.html","id":"line-plots","chapter":"31 Visualizing Time Series Data","heading":"31.0.0.4.1 Line plots","text":"Discover patterns visual inspection:Observations:\nClear trend\nConsider log transform differencing\n\nIncreasing variance\nConsider log square root transform\n\nMultiplicative seasonality\nSeasonal swings grow along overall values\n\nClear trend\nConsider log transform differencing\nConsider log transform differencingIncreasing variance\nConsider log square root transform\nConsider log square root transformMultiplicative seasonality\nSeasonal swings grow along overall values\nSeasonal swings grow along overall values","code":"\nautoplot(AirPassengers)+\n  xlab(\"Year\")+\n  ylab(\"Passengers\")"},{"path":"visualizing-time-series-data.html","id":"time-series-decomposition","chapter":"31 Visualizing Time Series Data","heading":"31.0.0.4.2 Time series decomposition:","text":"Break data seasonal, trend, remainder componentsSeasonal component:\nLOESS smoothing January values, February values, etc.\nMoving window estimate smoothed value based point’s neighbors\nLOESS smoothing January values, February values, etc.Moving window estimate smoothed value based point’s neighborsstats package\nstl()\nstl()Observations\nClear rising trend\nObvious seasonality\nDifference two methods:\nparticular decomposition shows additive, multiplicative seasonality\nstart end time series highest residuals\nSettled average seasonal variance\n\n\nreveal information patterns need identified potentially dealt forecasting can occur\nClear rising trendObvious seasonalityDifference two methods:\nparticular decomposition shows additive, multiplicative seasonality\nstart end time series highest residuals\nSettled average seasonal variance\n\nparticular decomposition shows additive, multiplicative seasonality\nstart end time series highest residuals\nSettled average seasonal variance\nstart end time series highest residualsSettled average seasonal varianceBoth reveal information patterns need identified potentially dealt forecasting can occur","code":"\nautoplot(stl(AirPassengers, s.window = 'periodic'), ts.colour = 'red')+\n  xlab(\"Year\")"},{"path":"visualizing-time-series-data.html","id":"plotting-exploiting-the-temporal-axis","chapter":"31 Visualizing Time Series Data","heading":"31.0.0.5 Plotting: Exploiting the Temporal Axis","text":"","code":""},{"path":"visualizing-time-series-data.html","id":"gannt-charts","chapter":"31 Visualizing Time Series Data","heading":"31.0.0.5.1 Gannt charts","text":"Shows overlapping time periods, duration event relative otherstimevis package timevis()","code":"\ndates=sample(seq(as.Date('1998-01-01'), as.Date('2000-01-01'), by=\"day\"), 16)\ndates=dates[order(dates)]\nprojects = paste0(\"Project \",seq(1,8)) \n\ndata <- data.frame(content = projects, \n                    start = dates[1:8],\n                    end = dates[9:16])\ntimevis(data)"},{"path":"visualizing-time-series-data.html","id":"using-month-and-year-creatively-in-line-plots","chapter":"31 Visualizing Time Series Data","heading":"31.0.0.5.2 Using month and year creatively in line plots","text":"forecast package\nggseasonplot()\nX axis months\nY axis variable interest\nline represents year.\nShows months exhibited similar/different seasonal patterns years\n\nggmonthplot()\nX axis months\nY axis variable interest\nBlue line mean season\nBlack line values every year single month\n\nggseasonplot()\nX axis months\nY axis variable interest\nline represents year.\nShows months exhibited similar/different seasonal patterns years\nX axis monthsY axis variable interestEach line represents year.Shows months exhibited similar/different seasonal patterns yearsggmonthplot()\nX axis months\nY axis variable interest\nBlue line mean season\nBlack line values every year single month\nX axis monthsY axis variable interestBlue line mean seasonBlack line values every year single monthObservations\nmonths increased time others\nPassenger numbers peak July August\nLocal peak March years\nOverall increase across months years\nGrowth trend increasing (rate increase increasing)\nmonths increased time othersPassenger numbers peak July AugustLocal peak March yearsOverall increase across months yearsGrowth trend increasing (rate increase increasing)","code":"\nggseasonplot(AirPassengers)\nggmonthplot(AirPassengers)"},{"path":"visualizing-time-series-data.html","id":"d-visualizations-plotly-package","chapter":"31 Visualizing Time Series Data","heading":"31.0.0.6 3-D Visualizations: plotly package","text":"Convert format plotly understand\nAvoid using ts() object\nDataframe datetime, numeric columns\nlubridate package date manipulation\nyear()\nmonth()\n\nAvoid using ts() objectDataframe datetime, numeric columnslubridate package date manipulation\nyear()\nmonth()\nyear()month()Allows better view relationship month, year, number passengers","code":"\nnew = data.frame(AirPassengers)\nnew$year=year(seq(as.Date(\"1949-01-01\"),as.Date(\"1960-12-01\"),by=\"month\"))\nnew$month=lubridate::month(seq(as.Date(\"1949-01-01\"),as.Date(\"1960-12-01\"),by=\"month\"),label=TRUE)\nplot_ly(new, x = ~month, y = ~year, z = ~AirPassengers, \n             color = ~as.factor(month)) %>%\n    add_markers() %>%\n    layout(scene = list(xaxis = list(title = 'Month'),\n                        yaxis = list(title = 'Year'),\n                        zaxis = list(title = 'Passenger Count')))"},{"path":"visualizing-time-series-data.html","id":"data-smoothing","chapter":"31 Visualizing Time Series Data","heading":"31.0.0.7 Data Smoothing","text":"Usually need smooth data starting analysis visualization\nAllows better storytelling\nIrrelevant spikes dominate narrative\nAllows better storytellingIrrelevant spikes dominate narrativeMethods:\nMoving average/median\nGood noisy data\nRolling mean reduces variance\nKeep mind: affects accuracy, R² statistics, etc.\nZoo package rollmean() rollmedian()\nk = 7 7 month rolling window\n\n\nPrevent lookahead, use past values window (align=“right”)\ngsub() substitute series names clearer legend\ntidyr package gather()\nConvert wide long, use color/group ggplot2 geom_line()\n\n\nExponentially weighted moving average\nWeigh past values less recent\npracma package movavg() function\nUseful recent data less informative past\n\nGeometric mean\nCombats strong serial correlation\nGood series data compounds greatly time goes \nBase R exp(mean(log())) zoo package rollapply()\n\nMoving average/median\nGood noisy data\nRolling mean reduces variance\nKeep mind: affects accuracy, R² statistics, etc.\nZoo package rollmean() rollmedian()\nk = 7 7 month rolling window\n\n\nPrevent lookahead, use past values window (align=“right”)\ngsub() substitute series names clearer legend\ntidyr package gather()\nConvert wide long, use color/group ggplot2 geom_line()\n\nGood noisy dataRolling mean reduces variance\nKeep mind: affects accuracy, R² statistics, etc.\nZoo package rollmean() rollmedian()\nk = 7 7 month rolling window\n\nKeep mind: affects accuracy, R² statistics, etc.Zoo package rollmean() rollmedian()\nk = 7 7 month rolling window\nk = 7 7 month rolling windowPrevent lookahead, use past values window (align=“right”)gsub() substitute series names clearer legendtidyr package gather()\nConvert wide long, use color/group ggplot2 geom_line()\nConvert wide long, use color/group ggplot2 geom_line()Exponentially weighted moving average\nWeigh past values less recent\npracma package movavg() function\nUseful recent data less informative past\nWeigh past values less recentpracma package movavg() functionUseful recent data less informative pastGeometric mean\nCombats strong serial correlation\nGood series data compounds greatly time goes \nBase R exp(mean(log())) zoo package rollapply()\nCombats strong serial correlationGood series data compounds greatly time goes onBase R exp(mean(log())) zoo package rollapply()","code":"\nnew = data.frame(AirPassengers)\nnew$AirPassengers=as.numeric(new$AirPassengers)\nnew$year=seq(as.Date(\"1949-01-01\"),as.Date(\"1960-12-01\"),by=\"month\")\n\nnew = new %>%\n  mutate(roll_mean = rollmean(new$AirPassengers,k=7,align=\"right\",fill = NA),\n         roll_median = rollmedian(new$AirPassengers,k=7,align=\"right\",fill = NA))\n\ndf <- gather(new, key = year, value = Rate, \n                            c(\"roll_mean\",\"roll_median\", \"AirPassengers\"))\ndf$year=gsub(\"AirPassengers\",\"series\",df$year)\ndf$year=gsub(\"roll_mean\",\"rolling mean\",df$year)\ndf$year=gsub(\"roll_median\",\"rolling median\",df$year)\ndf$date = rep(new$year,3)\n\nggplot(df[complete.cases(df),], aes(x=date, y = Rate, group = year, colour = year)) + \n  geom_line()"},{"path":"visualizing-time-series-data.html","id":"checking-time-series-properties","chapter":"31 Visualizing Time Series Data","heading":"31.0.0.8 Checking Time Series Properties","text":"","code":""},{"path":"visualizing-time-series-data.html","id":"stationarity","chapter":"31 Visualizing Time Series Data","heading":"31.0.0.8.1 Stationarity","text":"Many time series models rely stationarity\nStable mean/variance/autocorrelation time\nExamine error term behavior\nCan visually:\nLook seasonality, trend, increasing variance\n\nStatistically:\nAugmented Dickey–Fuller (ADF) test\nNull hypothesis = unit root\nFocuses changing mean\ntseries package adf.test()\n\n\nVisual can better:\nADF tests perform poorly near unit roots, small sample size\nUse approaches\n\nRemedies:\nDifference data correct trend\nLogarithm square root correct variance\n\nStable mean/variance/autocorrelation timeExamine error term behaviorCan visually:\nLook seasonality, trend, increasing variance\nLook seasonality, trend, increasing varianceStatistically:\nAugmented Dickey–Fuller (ADF) test\nNull hypothesis = unit root\nFocuses changing mean\ntseries package adf.test()\n\nAugmented Dickey–Fuller (ADF) test\nNull hypothesis = unit root\nFocuses changing mean\ntseries package adf.test()\nNull hypothesis = unit rootFocuses changing meantseries package adf.test()Visual can better:\nADF tests perform poorly near unit roots, small sample size\nUse approaches\nADF tests perform poorly near unit roots, small sample sizeUse approachesRemedies:\nDifference data correct trend\nLogarithm square root correct variance\nDifference data correct trendLogarithm square root correct varianceObservations:\ndata clear trend increasing variance\nusing differencing, trend dampened\n’s still increasing variance, log transform might right choice\nThings start getting muddied second difference\nADF says original series stationary based small p-value\nVisual inspection says otherwise\n\ndata clear trend increasing varianceBy using differencing, trend dampenedThere’s still increasing variance, log transform might right choiceThings start getting muddied second differenceADF says original series stationary based small p-value\nVisual inspection says otherwise\nVisual inspection says otherwise","code":"\nnew = data.frame(AirPassengers)\nnew$AirPassengers=as.numeric(new$AirPassengers)\nnew$date=seq(as.Date(\"1949-01-01\"),as.Date(\"1960-12-01\"),by=\"month\")\nnew$diff_data=c(NA,diff(new$AirPassengers, differences=1))\nnew$diff_data2=c(NA,NA,diff(new$AirPassengers, differences=2))\n\ng=ggplot(new , aes(date,AirPassengers))+\n  geom_line(colour = \"black\")\n\ng_diff=ggplot(new , aes(date,diff_data))+\n  geom_line(colour = \"red\")+\n  ylab(\"Difference Data\")\n\ng_diff2=ggplot(new , aes(date,diff_data2))+\n  geom_line(colour = \"blue\")+\n  ylab(\"2nd Difference Data\")\n\ngrid.arrange(g,g_diff,g_diff2,top=textGrob(\"Airline Passengers\"))\nadf.test(new$AirPassengers,alternative='stationary')## \n##  Augmented Dickey-Fuller Test\n## \n## data:  new$AirPassengers\n## Dickey-Fuller = -7.3186, Lag order = 5, p-value = 0.01\n## alternative hypothesis: stationary\nadf.test(new[complete.cases(new$diff_data),3],alternative='stationary')## \n##  Augmented Dickey-Fuller Test\n## \n## data:  new[complete.cases(new$diff_data), 3]\n## Dickey-Fuller = -7.0177, Lag order = 5, p-value = 0.01\n## alternative hypothesis: stationary\nadf.test(new[complete.cases(new$diff_data2),4],alternative='stationary')## \n##  Augmented Dickey-Fuller Test\n## \n## data:  new[complete.cases(new$diff_data2), 4]\n## Dickey-Fuller = -8.0516, Lag order = 5, p-value = 0.01\n## alternative hypothesis: stationary"},{"path":"visualizing-time-series-data.html","id":"normality","chapter":"31 Visualizing Time Series Data","heading":"31.0.0.8.2 Normality:","text":"Many time series models assume normalityThis can observed histogram QQ plot\nggplot2 package geom_histogram()\nstats package qqnorm() qqline()\nggplot2 package geom_histogram()stats package qqnorm() qqline()normal:\nBox Cox transformation\nMASS package boxcox()\n\ncareful transformations!\npreserving important information?\n\nBox Cox transformation\nMASS package boxcox()\nMASS package boxcox()careful transformations!\npreserving important information?\npreserving important information?Observations:\nData look normal: skewed\nEven though doesn’t capture time aspect data, input variable must normal many models\nData look normal: skewedEven though doesn’t capture time aspect data, input variable must normal many models","code":"\nggplot(new, aes(x=AirPassengers)) +\n  geom_histogram(binwidth =20,fill = \"mediumpurple\") +             \n  xlab(\"Passengers\")\nqqnorm(new$AirPassengers, main=\"Airline Passengers\", xlab=\"\", ylab=\"\", pch=16)\nqqline(new$AirPassengers)"},{"path":"visualizing-time-series-data.html","id":"lagged-correlations","chapter":"31 Visualizing Time Series Data","heading":"31.0.0.8.3 Lagged Correlations","text":"Autocorrelation Function\nCorrelation two points time series fixed interval\nLinear relationship points function time difference\nCommon behaviors:\nACF stationary series drops zero quickly\nnonstationary series, value lag 1 large positive\nWhite noise 0 lags 0\n\nSignificance ACF estimate determined “critical region” bounds +/–1.96 × sqrt(n)\nforecast package Acf()\nY axis correlation\nX axis time lag\n\nCorrelation two points time series fixed intervalLinear relationship points function time differenceCommon behaviors:\nACF stationary series drops zero quickly\nnonstationary series, value lag 1 large positive\nWhite noise 0 lags 0\nACF stationary series drops zero quicklyFor nonstationary series, value lag 1 large positiveWhite noise 0 lags 0Significance ACF estimate determined “critical region” bounds +/–1.96 × sqrt(n)forecast package Acf()\nY axis correlation\nX axis time lag\nY axis correlationX axis time lagPartial Autocorrelation Function:\npartial correlation series time t series time t-k given information t-k….t\ncritical regions ACF\nACF vs PACF:\nRedundant correlations appear ACF\nPACF correlations show exactly kth lagged value related current point\nPACF helps know long time series needs capture dynamics want model\n\nforecast package Pacf()\npartial correlation series time t series time t-k given information t-k….tSame critical regions ACFACF vs PACF:\nRedundant correlations appear ACF\nPACF correlations show exactly kth lagged value related current point\nPACF helps know long time series needs capture dynamics want model\nRedundant correlations appear ACFPACF correlations show exactly kth lagged value related current pointPACF helps know long time series needs capture dynamics want modelforecast package Pacf()Observations:\nACF fails trail certain lag, indicating clear trend\nPACF shows strong significance around lag 12, coinciding Christmas, seasonal peak expected.\nBehavior PACF ACF vital determining parameters time series ARIMA models many others.\nACF fails trail certain lag, indicating clear trendPACF shows strong significance around lag 12, coinciding Christmas, seasonal peak expected.Behavior PACF ACF vital determining parameters time series ARIMA models many others.Now ready get started time series data!Referenceshttps://www.statology.org/exponential-moving-average--r/https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc442.htmhttps://learning.oreilly.com/library/view/practical-time-series/9781492041641/ch02.html","code":"\nAcf(new$AirPassengers,main='Passengers Autcorrelation Function')\nPacf(new$AirPassengers,main='Passengers Partial Autcorrelation Function')"},{"path":"igraph-in-r.html","id":"igraph-in-r","chapter":"32 igraph in r","heading":"32 igraph in r","text":"Mouwei Lin (lm3756) Linhao Yu (ly2590)brief introduction R package called igraph, complex network analysis tool R. contain commonly used important functions. believe look cheat sheet, can handle network visualization problems.","code":""},{"path":"igraph-in-r.html","id":"igraph-basics","chapter":"32 igraph in r","heading":"32.1 1. igraph Basics","text":"First download install package.use open source data set named phone.call navdata.","code":"\n#install.packages('igraph')\nlibrary(igraph) #this is a must install package\n\n#install.packages('devtools')\n#library(devtools) #this is a must install package\n#install_github('kassambara/navdata') \nlibrary(navdata) #this is a must install package\n\n#install.packages('tidyverse')\nlibrary(tidyverse) #this is a must install package\n\n#install.packages('igraphdata')\nlibrary(igraphdata) #this is a must install package\ndata(\"phone.call\")\nhead(phone.call)## # A tibble: 6 × 3\n##   source  destination n.call\n##   <chr>   <chr>        <dbl>\n## 1 France  Germany          9\n## 2 Belgium France           4\n## 3 France  Spain            3\n## 4 France  Italy            4\n## 5 France  Netherlands      2\n## 6 France  UK               3"},{"path":"igraph-in-r.html","id":"create-igraph-network-object","chapter":"32 igraph in r","heading":"32.1.1 1.1 Create igraph Network Object","text":"igraph specific network (graph) object called ‘igraph’. simplest way create ‘igraph’ object use graph.formula() specify every node edge individually.real world, usually lot data. Therefore, good idea define every node edge manually. generally, use two methods:","code":"\ngraph.formula(A-B-C-D,A-E-F,Z-D-X)## IGRAPH 4392828 UN-- 8 7 -- \n## + attr: name (v/c)\n## + edges from 4392828 (vertex names):\n## [1] A--B A--E B--C C--D D--Z D--X E--F\n# prepare the data\nname<-data.frame(c(phone.call$source,phone.call$destination))\nnodes<-name%>%\n    distinct()%>%\nmutate(location=c(\"western\",\"western\",\"central\",\"nordic\",\"southeastern\",\n     \"southeastern\",\"southeastern\",\"southern\",\"sourthern\",\n     \"western\",\"western\",\"central\",\"central\",\"central\",\"central\",\"central\"))\ncolnames(nodes)<-c(\"label\",\"location\")\n\nedges<-phone.call%>%\n    rename(from=source,to=destination,weight=n.call)"},{"path":"igraph-in-r.html","id":"build-igraph-object-from-dataframe","chapter":"32 igraph in r","heading":"32.1.1.1 (1) build igraph object from dataframe:","text":"graph_from_data_frame()\nuse method, need two dataframes, one edge frame, vertices frame.can see graph created. can use V() E() visit vertices edges, as_edgelist(net, names=T), as_adjacency_matrix(net, attr=“weight”) catch edges list adjacent matrix:","code":"\nnet_pc<-graph_from_data_frame(\n   d=edges,vertices=nodes,\n   directed=TRUE)\nV(net_pc)## + 16/16 vertices, named, from 6e93970:\n##  [1] France         Belgium        Germany        Danemark       Croatia       \n##  [6] Slovenia       Hungary        Spain          Italy          Netherlands   \n## [11] UK             Austria        Poland         Switzerland    Czech republic\n## [16] Slovania\nV(net_pc)$location##  [1] \"western\"      \"western\"      \"central\"      \"nordic\"       \"southeastern\"\n##  [6] \"southeastern\" \"southeastern\" \"southern\"     \"sourthern\"    \"western\"     \n## [11] \"western\"      \"central\"      \"central\"      \"central\"      \"central\"     \n## [16] \"central\"\nE(net_pc)## + 18/18 edges from 6e93970 (vertex names):\n##  [1] France  ->Germany        Belgium ->France         France  ->Spain         \n##  [4] France  ->Italy          France  ->Netherlands    France  ->UK            \n##  [7] Germany ->Austria        Germany ->Poland         Belgium ->Germany       \n## [10] Germany ->Switzerland    Germany ->Czech republic Germany ->Netherlands   \n## [13] Danemark->Germany        Croatia ->Germany        Croatia ->Slovania      \n## [16] Croatia ->Hungary        Slovenia->Germany        Hungary ->Slovania\nas_edgelist(net_pc, names=T) ##       [,1]       [,2]            \n##  [1,] \"France\"   \"Germany\"       \n##  [2,] \"Belgium\"  \"France\"        \n##  [3,] \"France\"   \"Spain\"         \n##  [4,] \"France\"   \"Italy\"         \n##  [5,] \"France\"   \"Netherlands\"   \n##  [6,] \"France\"   \"UK\"            \n##  [7,] \"Germany\"  \"Austria\"       \n##  [8,] \"Germany\"  \"Poland\"        \n##  [9,] \"Belgium\"  \"Germany\"       \n## [10,] \"Germany\"  \"Switzerland\"   \n## [11,] \"Germany\"  \"Czech republic\"\n## [12,] \"Germany\"  \"Netherlands\"   \n## [13,] \"Danemark\" \"Germany\"       \n## [14,] \"Croatia\"  \"Germany\"       \n## [15,] \"Croatia\"  \"Slovania\"      \n## [16,] \"Croatia\"  \"Hungary\"       \n## [17,] \"Slovenia\" \"Germany\"       \n## [18,] \"Hungary\"  \"Slovania\"\nas_adjacency_matrix(net_pc, attr=\"weight\")## 16 x 16 sparse Matrix of class \"dgCMatrix\"\n##                                                 \n## France         . . 9 . . . . 3 4 2 3 . . . . .  \n## Belgium        4 . 3 . . . . . . . . . . . . .  \n## Germany        . . . . . . . . . 2 . 2 2 2 2 .  \n## Danemark       . . 2 . . . . . . . . . . . . .  \n## Croatia        . . 2 . . . 2 . . . . . . . . 2.0\n## Slovenia       . . 2 . . . . . . . . . . . . .  \n## Hungary        . . . . . . . . . . . . . . . 2.5\n## Spain          . . . . . . . . . . . . . . . .  \n## Italy          . . . . . . . . . . . . . . . .  \n## Netherlands    . . . . . . . . . . . . . . . .  \n## UK             . . . . . . . . . . . . . . . .  \n## Austria        . . . . . . . . . . . . . . . .  \n## Poland         . . . . . . . . . . . . . . . .  \n## Switzerland    . . . . . . . . . . . . . . . .  \n## Czech republic . . . . . . . . . . . . . . . .  \n## Slovania       . . . . . . . . . . . . . . . ."},{"path":"igraph-in-r.html","id":"build-igraph-object-from-adjacent-matrix","chapter":"32 igraph in r","heading":"32.1.1.2 (2) build igraph object from adjacent matrix:","text":"graph_from_adjacency_matrix()\ngraph usingnow simply plot take look","code":"\nadjacent_matrix<-as_adjacency_matrix(net_pc, attr=\"weight\")\nnet_am<-graph_from_adjacency_matrix(adjacent_matrix)\nplot(net_pc)\nplot(net_am)"},{"path":"igraph-in-r.html","id":"basic-igraph-visualization-instructions","chapter":"32 igraph in r","heading":"32.1.2 1.2 Basic igraph Visualization Instructions","text":"plot function igraph strong lot parameters make network beautiful clear. section give general introduction important visualization methods, detailed introduction next section.large number parameters used display various properties nodes, edges graphs. parameters related nodes start vertex.XXX, parameters related edges start edge.XXXIn addition specifying parameters nodes edges plot(), can also use previously mentioned V() E() add corresponding properties directly igraph object. difference two methods parameters specified plot() change properties plot. example, first specify color node according position, width edge according weight (two attributes saved net_pc object), specify size node parameters plot() (proportional degree node, node degree number edges connected node), size position node marker, color edge, size arrow, degree curvature edge.","code":"\n# Calculate node's degree\ndeg<-degree(net_pc,mode=\"all\")\n# Set up the color\nvcolor<-c(\"orange\",\"red\",\"lightblue\",\"tomato\",\"yellow\")\n# Set specific node's Color\nV(net_pc)$color<-vcolor[factor(V(net_pc)$location)]\n# Set specific edge's weight\nE(net_pc)$width<-E(net_pc)$weight/2\n\n# Set up vertex.size, vertex.label.cex & dist, edge color & arrow size & curve in graph\nplot(net_pc,vertex.size=3*deg,\n     vertex.label.cex=.7,vertex.label.dist=1,\n     edge.color=\"gray50\",edge.arrow.size=.4, edge.curved=.1)\n# Add legend\nlegend(x=-1.5,y=1.5,levels(factor(V(net_pc)$location)),pch=21,col=\"#777777\",pt.bg=vcolor)"},{"path":"igraph-in-r.html","id":"network-layout","chapter":"32 igraph in r","heading":"32.1.3 1.3 Network Layout","text":"Network layout refers method determining coordinates node network.variety layout algorithms provided igraph. Among , Force-directed layout algorithms useful. Force-directed layouts try get aesthetically pleasing graph edges similar length cross little possible. model graphics physical system. Nodes “charged particles” repel get close. edges act springs, attracting connected nodes together. result, nodes evenly distributed illustrated area, layout intuitive nodes share connections closer . disadvantage algorithms slow therefore less frequently used graphs larger 1000 vertices.using force-directed layout, can use niter parameter control number iterations perform. default setting 500 iterations. large graphs, can lower number get results faster check reasonable.Fruchterman-Reingold widely used Force-directed layout method:Fruchterman Reingold layout random different every run result slightly different layout configurations. Saving layout object l allows us obtain exact result multiple times (also possible specify random state setting seed seed())18 methods layout igraph, won’t go detail layout method widely used except Fruchterman Reingold layout. However, give example show layouts look like:","code":"\n# Fruchterman-Reingold layout method\nl <- layout_with_fr(net_pc) \nplot(net_pc, layout=l)\n# All the layout methods in igraph\nlayouts <- grep(\"^layout_\", ls(\"package:igraph\"), value=TRUE)[-1]\nlayouts##  [1] \"layout_as_bipartite\"  \"layout_as_star\"       \"layout_as_tree\"      \n##  [4] \"layout_components\"    \"layout_in_circle\"     \"layout_nicely\"       \n##  [7] \"layout_on_grid\"       \"layout_on_sphere\"     \"layout_randomly\"     \n## [10] \"layout_with_dh\"       \"layout_with_drl\"      \"layout_with_fr\"      \n## [13] \"layout_with_gem\"      \"layout_with_graphopt\" \"layout_with_kk\"      \n## [16] \"layout_with_lgl\"      \"layout_with_mds\"      \"layout_with_sugiyama\"\nlayouts <- grep(\"^layout_\", ls(\"package:igraph\"), value=TRUE)[-1]\n# Remove layouts that do not apply to our graph.\nlayouts <- layouts[!grepl(\"bipartite|merge|norm|sugiyama|tree\", layouts)]\npar(mfrow=c(5,3), mar=c(1,1,1,1)) \nfor (layout in layouts) {\n  print(layout)\n  l <- do.call(layout, list(net_pc))\n  plot(net_pc, vertex.label=\"\",edge.arrow.mode=0,\n       layout=l,main=layout) }## [1] \"layout_as_star\"## [1] \"layout_components\"## [1] \"layout_in_circle\"## [1] \"layout_nicely\"## [1] \"layout_on_grid\"## [1] \"layout_on_sphere\"## [1] \"layout_randomly\"## [1] \"layout_with_dh\"## [1] \"layout_with_drl\"## [1] \"layout_with_fr\"## [1] \"layout_with_gem\"## [1] \"layout_with_graphopt\"## [1] \"layout_with_kk\"## [1] \"layout_with_lgl\"## [1] \"layout_with_mds\""},{"path":"igraph-in-r.html","id":"decorating-igraph-visualizations","chapter":"32 igraph in r","heading":"32.1.4 2. Decorating igraph visualizations","text":"","code":""},{"path":"igraph-in-r.html","id":"details-for-decorating-igraph-visualization","chapter":"32 igraph in r","heading":"32.1.5 2.1 Details for decorating igraph visualization","text":"section, still use example , customize several parameters see influence visualization.First, customize nodes parameters.","code":"\nset.seed(111)\nl <- layout_with_fr(net_pc) \nplot(net_pc, \n     vertex.color = 'pink',                   # The color of nodes\n     vertex.frame.color = 'lightblue',        # The color of node frames\n     vertex.shape = c('circle','rectangle'),  # The color of node shapes\n     vertex.size = 25,                        # The size of a node\n     vertex.size2 = 15,                       # For rectangle, we need two parameters to specify its shape\n     \n     layout = l)\nplot(net_pc, \n     vertex.color = 'pink',                    \n     vertex.frame.color = 'lightblue',        \n     vertex.shape = c('circle','rectangle'),  \n     vertex.size = 25,                        \n     vertex.size2 = 15,                       \n     \n     vertex.label = 1:length(V(net_pc)), # Change labels to numbers\n     vertex.label.family = 'Helvetica',  # Change the font family of labels\n     vertex.label.font = 3,              # Change the font to italic\n     vertex.label.cex = 0.8,             # Change the size of labels\n     vertex.label.dist = 0.5,            # Change the distance between labels and node frames\n  \n     layout = l)\nplot(net_pc, \n     vertex.color = 'pink',                    \n     vertex.frame.color = 'lightblue',        \n     vertex.shape = c('circle','rectangle'),  \n     vertex.size = 25,                        \n     vertex.size2 = 15,                       \n     \n     vertex.label = 1:length(V(net_pc)), \n     vertex.label.family = 'Helvetica',  \n     vertex.label.font = 3,              \n     vertex.label.cex = 0.8,             \n     vertex.label.dist = 0.5,   \n     \n     edge.color = 'lightblue',  # Color of edges\n     edge.width = 3,            # Width of edges\n     edge.arrow.size = 0.8,     # Size of arrows\n     edge.arrow.width = 0.8,    # Width of arrows\n     edge.lty = 4,              # Line types of edges (4: dot dash)\n     edge.curved = 0.5,         # Curvature of edges\n     \n     layout = l)"},{"path":"igraph-in-r.html","id":"example-for-advanced-igraph-visualization","chapter":"32 igraph in r","heading":"32.1.6 2.2 Example for advanced igraph visualization","text":"section, introduce advanced network visualization using Zachary’s karate club network dataset. widely-used dataset network analysis. dataset, every node represents member karate club edges represent members’ social connection. Zachary’s study, administrator “John. .” coach “Mr. Hi” conflict led split club. Now, want use igraph visualize social network karate club.First, plot network without decoration.Next, highlight two leaders (“John. .” “Mr. Hi”) network using rectangles.\nFinally, divide edges three categories use different colors . Remember karate club split two factions, edges can divided : edges inside faction 1, edges inside faction 2, edges connecting factions.\nBesides, also set edge width according weight.","code":"\nset.seed(111)\ndata(karate)\nl <- layout_with_fr(karate) \n\nigraph.options(vertex.size = 10)\npar(mfrow = c(1,1))\nplot(karate,\n     layout = l)\n# Decoration\nV(karate)$label <- sub(\"Actor \",\"\", V(karate)$name)\n\n# Two leaders get shapes different from club members\nV(karate)$shape <- \"circle\"\nV(karate)[c(\"Mr Hi\", \"John A\")]$shape <- \"rectangle\"\n\nV(karate)$size  <- 20\nV(karate)$size2 <- 15\n\nplot(karate, \n     vertex.label = V(karate)$label,\n     layout  = l)\n# Define factions\nF1<-V(karate)[Faction==1]\nF2<-V(karate)[Faction==2]\n\n# Set up edge colors according to factions\nE(karate)[F1 %--% F1]$color<-\"darkgoldenrod2\"\nE(karate)[F2 %--% F2]$color<-\"lightblue\"\nE(karate)[F1 %--% F2]$color<-\"brown\"\n\n# Set up edge width according to weights\nE(karate)$width=E(karate)$weight\n\n# Plot the decorated graph, using same layout.\nplot(karate,layout=l)"},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"geo-mapping-coordinate-extraction-using-api-calls","chapter":"33 Geo-Mapping Coordinate Extraction Using API Calls","heading":"33 Geo-Mapping Coordinate Extraction Using API Calls","text":"Wei Xiong Toh","code":""},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"introduction-2","chapter":"33 Geo-Mapping Coordinate Extraction Using API Calls","heading":"33.1 Introduction","text":"Leaflet popular library R allows us plot interactive maps. extremely easy use many applications visualizing geometric information. However, relatively prohibitive use without prior access required Shapefiles spatial data. project aims automate process generating spatial data incorporating information existing data frames ease plotting Leaflet. detailed guide using Leaflet can found .","code":""},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"motivation-5","chapter":"33 Geo-Mapping Coordinate Extraction Using API Calls","heading":"33.2 Motivation","text":"Consider following toy problem: following population information want display outlining country data frame, individual country’s population shown color map.","code":"\ninput_list <- c('US', 'UK', 'China', 'Japan', 'Brazil')\ninput_vals <- c(329064917,  67508936, 1425887337, 128547854, 215313498)\ninput_df <- data.frame(name=input_list, val=input_vals)\nprint(input_df)##     name        val\n## 1     US  329064917\n## 2     UK   67508936\n## 3  China 1425887337\n## 4  Japan  128547854\n## 5 Brazil  215313498"},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"public-datasets","chapter":"33 Geo-Mapping Coordinate Extraction Using API Calls","heading":"33.2.1 Public Datasets","text":"several open source datasets contain Shapefiles/coordinates geographical boundaries countries. example can found . challenge following downloading data integrate existing information (population example) spatial coordinates Shapefiles, one needs manually filter dataset obtain. Another significant challenge arises areas interest located within dataset. Extracting coordinates multiple sources becomes tedious. example, following comparison might need several disparate sources.","code":"\ninput_list_2 <- c('New York', 'London', 'Shanghai', 'Tokyo', 'Rio')\ninput_vals_2 <- c(8467513, 9541000, 20217748, 37274000, 13634000)\ninput_df_2 <- data.frame(name=input_list_2, val=input_vals_2)\nprint(input_df_2)##       name      val\n## 1 New York  8467513\n## 2   London  9541000\n## 3 Shanghai 20217748\n## 4    Tokyo 37274000\n## 5      Rio 13634000"},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"api-calls","chapter":"33 Geo-Mapping Coordinate Extraction Using API Calls","heading":"33.2.2 API Calls","text":"alternative relying publicly available datasets spatial coordinates obtain information API calls. several steps process:Obtain relation number point interest OpenStreetMap\nObtain relation number point interest OpenStreetMap\nInput relation number Polygon Search API\nInput relation number Polygon Search API\nExtract coordinates response\nbenefit using method accurate levels administration, whether one looking country, state even city.\nnext section details process automate search given input data frames . Several improvements arise contribution:Eliminate need download several datasets multiple sources.Reduce time taken filter dataset obtain relevant coordinates.Obtain either point polygon coordinates.","code":""},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"code","chapter":"33 Geo-Mapping Coordinate Extraction Using API Calls","heading":"33.3 Code","text":"user specify following input parameters:input_type parameter allows user select level administration input list.granularity parameter used speed plotting reducing number points final multipolygon shape.","code":"\ninput_list <- c('US', 'UK', 'China', 'Japan', 'Brazil')\ninput_vals <- c(329064917,  67508936,   1425887337, 128547854, 215313498)\ninput_df <- data.frame(name=input_list, val=input_vals)\ninput_type <- 'Country'\ninput_ll_type <- 'poly'\ngranularity <- 50"},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"multipolygon","chapter":"33 Geo-Mapping Coordinate Extraction Using API Calls","heading":"33.3.1 Multipolygon","text":"Main code obtain longitude latitudes areas interest:","code":"\ninput_list <- c('US', 'UK', 'China', 'Japan', 'Brazil')\ninput_vals <- c(329064917,  67508936,   1425887337, 128547854, 215313498)\ninput_df <- data.frame(name=input_list, val=input_vals)\ninput_type <- 'Country'\ninput_ll_type <- 'poly'\ngranularity <- 50\n\n# check inputs\nstopifnot(input_type %in% c('City', 'State', 'Country'))\nstopifnot(input_ll_type %in% c('poly', 'point'))\nstopifnot(granularity > 0)\n\nif (str_detect(input_ll_type, 'poly')) {\n  mpoly_result = c()\n  \n  for (input in input_list) {\n    # replace spaces with '+'\n    input <- input %>% str_replace_all(string=., ' ', '+')\n    \n    # concat to get final query url\n    base_url <- 'https://www.openstreetmap.org/geocoder/search_osm_nominatim?query='\n    query_url <- paste(c(base_url, input), collapse='')\n    \n    # get relation number from OSM API\n    res <- GET(query_url)\n    results <- content(res, as='text')\n    results <- results %>% str_extract_all(., 'li class=.+')\n    results <- results[[1]]\n    \n    # Part 1: extract relation number matching input type\n    for (result in results) {\n      data_pref <- str_extract(result, 'data-prefix=.+?(\\\\s)')\n      # check for match with query\n      if (!is.na(str_extract(data_pref, input_type))) {\n        rn <- str_extract(result, 'data-id=.+?(\\\\s)')\n        rn <- rn %>% str_replace(., 'data-id=\\\"', '') %>% \n                str_replace(., '\\\" ', '')\n        break\n      }\n    }\n    \n    # Part 2: get latitude and longitude from OSM API\n    poly_base_url <- 'https://polygons.openstreetmap.fr/get_poly.py?id='\n    params <- '&params=0'\n    poly_url <- paste(c(poly_base_url, rn, params), collapse='')\n    \n    poly_res <- GET(poly_url)\n    poly_ll <- read.table(text=content(poly_res,as='text'), sep='\\n')\n    \n    # Part 3: create polygons for plotting\n    mpoly_coords <- list()\n    curr_coords <- list()\n    count <- 1\n    \n    for (coords in poly_ll[2:NROW(poly_ll)-1,]) {\n      # add to current mpoly list\n      if (str_detect(coords, 'END')) {\n        if (length(curr_coords) > 1) {\n          # verify closed polygon\n          if (curr_coords[[length(curr_coords)-1]] != curr_coords[[1]] || \n              curr_coords[[length(curr_coords)]] != curr_coords[[2]]) {\n            curr_coords[[length(curr_coords)+1]] <- curr_coords[[1]]\n            curr_coords[[length(curr_coords)+1]] <- curr_coords[[2]]\n          }\n          \n          mpoly_coords[[length(mpoly_coords)+1]] <- matrix(as.numeric(curr_coords), ncol=2, byrow=TRUE)\n          curr_coords <- list()\n          count <- 1\n        }\n      }\n      # check if coords are valid\n      else if ((count-1) %% granularity == 0 && nchar(coords) > 1) {\n        long = as.numeric(str_split(coords, '\\t')[[1]][2])\n        lat = as.numeric(str_split(coords, '\\t')[[1]][3])\n        \n        if (!is.na(long) && !is.na(lat)) {\n          curr_coords <- append(curr_coords, long)\n          curr_coords <- append(curr_coords, lat)\n        }\n      }\n      \n      count <- count + 1\n    }\n    \n    mpoly_shape <- st_multipolygon(list(mpoly_coords))\n    \n    mpoly_result[[length(mpoly_result)+1]] <- mpoly_shape\n  \n  }\n  \n  # create sf object to plot\n  mpoly_sf <- st_sf(input_df, geometry=mpoly_result)\n  \n} else {\n  # single point\n  mpoint_list <- c()\n  \n  for (input in input_list) {\n    # replace spaces with '+'\n    input <- input %>% str_replace_all(string=., ' ', '+')\n    \n    # concat to get final query url\n    base_url <- 'https://www.openstreetmap.org/geocoder/search_osm_nominatim?query='\n    query_url <- paste(c(base_url, input), collapse='')\n    \n    # get relation number from OSM API\n    res <- GET(query_url)\n    results <- content(res, as='text')\n    results <- results %>% str_extract_all(., 'li class=.+')\n    results <- results[[1]]\n    \n    # Part 1: extract relation number matching input type\n    for (result in results) {\n      data_pref <- str_extract(result, 'data-prefix=.+?(\\\\s)')\n      # check for match with query\n      if (!is.na(str_extract(data_pref, input_type))) {\n        rn <- str_extract(result, 'data-id=.+?(\\\\s)')\n        rn <- rn %>% str_replace(., 'data-id=\\\"', '') %>% \n          str_replace(., '\\\" ', '')\n        break\n      }\n    }\n    \n    # Part 2: get latitude and longitude from OSM API\n    base_url <- 'https://nominatim.openstreetmap.org/details?osmtype=R&osmid='\n    query_url <- paste(c(base_url, rn, '&format=json'), collapse='')\n    \n    # extract info from OSM API\n    res <- fromJSON(query_url)\n    \n    if (\"centroid\" %in% names(res)) {\n      long <- res$centroid$coordinates[1]\n      lat <- res$centroid$coordinates[2]\n    } else {\n      long <- NULL\n      lat <- NULL\n    }\n    \n    if (!is.na(long) && !is.na(lat)) {\n      mpoint_list[[length(mpoint_list)+1]] <- st_point(c(long, lat))\n    }\n  }\n  \n  mpoint_sf <- st_sf(input_df, geometry=mpoint_list)\n  \n}"},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"multipolygon-plots","chapter":"33 Geo-Mapping Coordinate Extraction Using API Calls","heading":"33.3.1.1 Multipolygon Plots","text":"","code":"\ninput_colorP <- \"YlOrRd\"\n\nleaflet(mpoly_sf) %>% \n    addTiles() %>%\n    addPolygons(\n      fillOpacity = 1, smoothFactor = 0.75,\n      color=~colorNumeric(input_colorP, val)(val)\n    )"},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"points","chapter":"33 Geo-Mapping Coordinate Extraction Using API Calls","heading":"33.3.2 Points","text":"","code":"\ninput_list <- c('New York', 'London', 'Shanghai', 'Tokyo', 'Rio')\ninput_vals <- c(8467513,    9541000,    20217748, 37274000, 13634000)\ninput_df <- data.frame(name=input_list, val=input_vals)\ninput_type <- 'City'\ninput_ll_type <- 'point'\ngranularity <- 50\n\n# check inputs\nstopifnot(input_type %in% c('City', 'State', 'Country'))\nstopifnot(input_ll_type %in% c('poly', 'point'))\nstopifnot(granularity > 0)\n\nif (str_detect(input_ll_type, 'poly')) {\n  mpoly_result <- c()\n  \n  for (input in input_list) {\n    # replace spaces with '+'\n    input <- input %>% str_replace_all(string=., ' ', '+')\n    \n    # concat to get final query url\n    base_url <- 'https://www.openstreetmap.org/geocoder/search_osm_nominatim?query='\n    query_url <- paste(c(base_url, input), collapse='')\n    \n    # get relation number from OSM API\n    res <- GET(query_url)\n    results <- content(res, as='text')\n    results <- results %>% str_extract_all(., 'li class=.+')\n    results <- results[[1]]\n    \n    # Part 1: extract relation number matching input type\n    for (result in results) {\n      data_pref <- str_extract(result, 'data-prefix=.+?(\\\\s)')\n      # check for match with query\n      if (!is.na(str_extract(data_pref, input_type))) {\n        rn <- str_extract(result, 'data-id=.+?(\\\\s)')\n        rn <- rn %>% str_replace(., 'data-id=\\\"', '') %>% \n                str_replace(., '\\\" ', '')\n        break\n      }\n    }\n    \n    # Part 2: get latitude and longitude from OSM API\n    poly_base_url <- 'https://polygons.openstreetmap.fr/get_poly.py?id='\n    params <- '&params=0'\n    poly_url <- paste(c(poly_base_url, rn, params), collapse='')\n    \n    poly_res <- GET(poly_url)\n    poly_ll <- read.table(text=content(poly_res,as='text'), sep='\\n')\n    \n    # Part 3: create polygons for plotting\n    mpoly_coords <- list()\n    curr_coords <- list()\n    count <- 1\n    \n    for (coords in poly_ll[2:NROW(poly_ll)-1,]) {\n      # add to current mpoly list\n      if (str_detect(coords, 'END')) {\n        if (length(curr_coords) > 1) {\n          # verify closed polygon\n          if (curr_coords[[length(curr_coords)-1]] != curr_coords[[1]] || \n              curr_coords[[length(curr_coords)]] != curr_coords[[2]]) {\n            curr_coords[[length(curr_coords)+1]] <- curr_coords[[1]]\n            curr_coords[[length(curr_coords)+1]] <- curr_coords[[2]]\n          }\n          \n          mpoly_coords[[length(mpoly_coords)+1]] <- matrix(as.numeric(curr_coords), ncol=2, byrow=TRUE)\n          curr_coords <- list()\n          count <- 1\n        }\n      }\n      # check if coords are valid\n      else if ((count-1) %% granularity == 0 && nchar(coords) > 1) {\n        long = as.numeric(str_split(coords, '\\t')[[1]][2])\n        lat = as.numeric(str_split(coords, '\\t')[[1]][3])\n        \n        if (!is.na(long) && !is.na(lat)) {\n          curr_coords <- append(curr_coords, long)\n          curr_coords <- append(curr_coords, lat)\n        }\n      }\n      \n      count <- count + 1\n    }\n    \n    mpoly_shape <- st_multipolygon(list(mpoly_coords))\n    \n    mpoly_result[[length(mpoly_result)+1]] <- mpoly_shape\n  \n  }\n  \n  # create sf object to plot\n  mpoly_sf <- st_sf(input_df, geometry=mpoly_result)\n  \n} else {\n  # single point\n  mpoint_list <- c()\n  \n  for (input in input_list) {\n    # replace spaces with '+'\n    input <- input %>% str_replace_all(string=., ' ', '+')\n    \n    # concat to get final query url\n    base_url <- 'https://www.openstreetmap.org/geocoder/search_osm_nominatim?query='\n    query_url <- paste(c(base_url, input), collapse='')\n    \n    # get relation number from OSM API\n    res <- GET(query_url)\n    results <- content(res, as='text')\n    results <- results %>% str_extract_all(., 'li class=.+')\n    results <- results[[1]]\n    \n    # Part 1: extract relation number matching input type\n    for (result in results) {\n      data_pref <- str_extract(result, 'data-prefix=.+?(\\\\s)')\n      # check for match with query\n      if (!is.na(str_extract(data_pref, input_type))) {\n        rn <- str_extract(result, 'data-id=.+?(\\\\s)')\n        rn <- rn %>% str_replace(., 'data-id=\\\"', '') %>% \n          str_replace(., '\\\" ', '')\n        break\n      }\n    }\n    \n    # Part 2: get latitude and longitude from OSM API\n    base_url <- 'https://nominatim.openstreetmap.org/details?osmtype=R&osmid='\n    query_url <- paste(c(base_url, rn, '&format=json'), collapse='')\n    \n    # extract info from OSM API\n    res <- fromJSON(query_url)\n    \n    if (\"centroid\" %in% names(res)) {\n      long <- res$centroid$coordinates[1]\n      lat <- res$centroid$coordinates[2]\n    } else {\n      long <- NULL\n      lat <- NULL\n    }\n    \n    if (!is.na(long) && !is.na(lat)) {\n      mpoint_list[[length(mpoint_list)+1]] <- st_point(c(long, lat))\n    }\n  }\n  \n  mpoint_sf <- st_sf(input_df, geometry=mpoint_list)\n  \n}"},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"single-marker-plots","chapter":"33 Geo-Mapping Coordinate Extraction Using API Calls","heading":"33.3.3 Single Marker Plots","text":"","code":"\nleaflet(mpoint_sf) %>% \n    addTiles() %>% \n    addMarkers()"},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"circle-plots","chapter":"33 Geo-Mapping Coordinate Extraction Using API Calls","heading":"33.3.4 Circle Plots","text":"","code":"\nleaflet(mpoint_sf) %>% \n    addTiles() %>% \n    addCircles(radius=~sqrt(val)*50)"},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"evaluation-1","chapter":"33 Geo-Mapping Coordinate Extraction Using API Calls","heading":"33.4 Evaluation","text":"set code generates different simple feature collection objects depending input variables set user. allows lot flexibility obtaining spatial coordinates based user’s needs. compared traditional method downloading open source spatial data, approach convenient user obtain data, user need know manipulate data prior plotting.Since method relies two key API calls, accuracy limited accuracy data maintained API libraries used. feasible rely API calls administrators OpenStreetMap decide remove public access libraries charge fee usage.","code":""},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"future-work","chapter":"33 Geo-Mapping Coordinate Extraction Using API Calls","heading":"33.5 Future Work","text":"Given time work project, publish code library R overall code neater even error handling incorporated.","code":""},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"conclusion-5","chapter":"33 Geo-Mapping Coordinate Extraction Using API Calls","heading":"33.6 Conclusion","text":"project automates method API calls efficiently extract geospatial coordinates points interests (POIs), can merged existing data frames visualizing spatial information. Users need provide names POIs, administrative level POIs (Country/State/City), type plot required granularity selecting coordinates plotting polygons. Users can use combined data frame input Leaflet visualize data.","code":""},{"path":"edav-garden.html","id":"edav-garden","chapter":"34 EDAV Garden","heading":"34 EDAV Garden","text":"Chenyu Zhang🌐Address: edav-garden.netlify.appEDAV Garden digital garden EDAV.\nsimple words, collection published notes, like edav.info.\nHowever, EDAV Garden special features:captures journey learning EDAV. practice learning public, learning public.notes collection external information; instead, notes containing thoughts interpretation.Therefore, may mistakes question marks scattered around. may fixed come back later.frozen garden; evolving garden continue gardening learn.site reference, also roam.","code":""},{"path":"edav-garden.html","id":"how-to-explore-the-garden","chapter":"34 EDAV Garden","heading":"34.1 How to Explore the Garden","text":"significant difference garden traditional book/wiki/website notes linear: chronological logical order. Instead, connected contextual links form knowledge graph, making garden explorable. also matches fundamental problem EDAV: exploratory vs. explanatory. navigate different notes, can follow different types links, illustrated belowan outgoing link points another note;backlink points note mentions current note;interactive graph presents connections current note .","code":""},{"path":"edav-garden.html","id":"main-sections","chapter":"34 EDAV Garden","heading":"34.2 Main Sections","text":"Right now, three main sections EDAV garden:Plots Gallery: gallery graphs categorized data type.R Garden: notes R.Git Garden: notes Git.","code":""},{"path":"using-raster-data-in-r.html","id":"using-raster-data-in-r","chapter":"35 Using raster data in R","heading":"35 Using raster data in R","text":"Andre Evard","code":""},{"path":"using-raster-data-in-r.html","id":"introduction-to-rasters","chapter":"35 Using raster data in R","heading":"35.1 Introduction to rasters","text":"Community Contribution project gives overview online book\nGeocomputation R,\nspecifically regards raster type geographic data representation.\nPlease note, second version book active development, \npackages references may --date near future.Frequent references package called terra made, may read \ndetail . extra\ndata sources used detailed book, except snow cover data found website,\nZenodo, Geocomputation R already links .","code":""},{"path":"using-raster-data-in-r.html","id":"what-is-a-raster","chapter":"35 Using raster data in R","heading":"35.1.1 What is a Raster?","text":"raster \nspatial model crosswise divides area land regularized boxes,\ncells. raster strictly rectangular X Y dimensions, \ncell dimensions defined point one corner, \nstores associated values, say temperature, elevation, population density, \nlist goes . raster may multiple layers, cell exactly one\nvalue per layer.rasters extremely useful environmental scientific modeling outside\nconfines political boundaries, example capturing number frog\nspecies discovered square kilometer sections Amazon Rainforest, \ncan widespread RGB pixels screen viewing \n.reference, type geocomputational model book concerned\nvector.\ndefined cells specified shapes geometries, better\nable represent human political boundaries instance. Although project\ncentered raster, vectors referenced occasion data\ntranslation transformation.","code":""},{"path":"using-raster-data-in-r.html","id":"what-is-terra","chapter":"35 Using raster data in R","heading":"35.1.2 What is Terra?","text":"(covers sections 2.3.1 2.3.3)Terra primary package\nGeocomputation R cites raster processing. Reportedly, terra \ncommon usecase easier learn package, whilst stars’s niche\npremire specialized datasets. sf third final package book describes heavy detail, vector data.Terra built upon older raster package, internal foundation \nC++ efficiency. Raster representation, one book’s first\ncode snippets, can see default attributes.can see , raster defined :dimensions:\nnrow: number rows\nncol: number columns\nnlyr: number layers\nnrow: number rowsncol: number columnsnlyr: number layersresolution: space covered single cell, two dimensionsextent: boundaries coordinatecoordinate reference: distance’s data type (“EPSG:4326” )access additional fields source, name, class, \nsmall metadata.terra also provides dedicated functions return components:Dimensions: ndim()Cells: ncell()Spatial Extent: ext()Coordinate Reference System: crs()Geocomputation R makes frequent usage example, srtm, elevation map\nsurrounding Utah national park, though provide examples,\nsometimes quick reference provides best example.\nQuickly plotting shows following graphic:intuitively, cell value, represented visually pixels\ncolor scale.","code":"\nraster_filepath = system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nsrtm = rast(raster_filepath)\nplot(srtm)"},{"path":"using-raster-data-in-r.html","id":"raster-creation","chapter":"35 Using raster data in R","heading":"35.2 Raster Creation","text":"","code":""},{"path":"using-raster-data-in-r.html","id":"the-direct-approach","chapter":"35 Using raster data in R","heading":"35.2.1 The direct approach","text":"(covers sections 2.3.4, 3.3.1)\nknow rasters , basic data structure. Okay, well \ncreate, load, populate general rasters? ’s options, always.Naturally, can directly create raster rast() function follows:isn’t exactly terribly useful raster, intuitive.\ncan also extract replace values, though \naren’t recommended anything past small structures experiments. \nknowledge row/col values ’re targeting, can directly set values\nsimilar fashion, using c() functions one cell desired.\nMutliple layers may also modified time way.\nSee last section better approach raster manipulation.","code":"\nmanual_raster = rast(\n  nrows = 100,\n  ncols = 100,\n  nlyrs = 2,\n  resolution = .5,\n  xmin = -25,\n  xmax = 25,\n  ymin = -25,\n  ymax = 25,\n  vals = 1:20000\n)\nplot(manual_raster)\nterra::extract(manual_raster, 10, 10)##   lyr.1 lyr.2\n## 1    10 10010\nmanual_raster[15, -10] <- 15000\nmanual_raster[15, -10]##   lyr.1 lyr.2\n## 1    NA    NA\nmanual_raster[c(19, 20, 21), c(19, 20, 21)] <- cbind(c(5000:5008), c(15000:15008)) \nplot(manual_raster)\n# You can also use <raster>[] as a shortcut of .values()."},{"path":"using-raster-data-in-r.html","id":"raster-files-and-their-formats","chapter":"35 Using raster data in R","heading":"35.2.2 Raster files and their formats","text":"(covers sections 8.5 8.6.2)Loading files preferred approach far robust, \nimage file already handy (one producedyourself!).Firstly, ’s lot know raster files. Pulling section 8.5, can\nfind quick overview popular raster (vector) formats. short though,\nprimary file format GeoTIFF, embeds additional geospatial data\n(coordinate systems) .tif/.tiff image files. Among others, NASA \nGoogle data publicaly available COGs.Outside GeoTIFF, also Arc ASCII (.asc) text-based storage,\nSpatiaLite extension SQLite, ESRI FileGDB proprietary.\n, outside specific use cases, recommendation use GeoTIFF.Notably, GeoTIFF also supports Cloud Optimized GeoTIFFs (COG), allows\nrasters hosted HTTP servers enables users download \nsegment can rather large files.Given path, rast() function\ngood job loading rasters.previously mentioned, whole GeoTIFF downloaded \nway, merely attached . /vsicurl/ followed https url ’s\nnecessary download information, provided image provider\nlocated already course. relevant portions loaded utilize\nmanipulate manner, :discuss cropping later. Naturally, can also load local files using\nrast(). can produce save , use packages spData.","code":"\nsnowurl = \"/vsicurl/https://zenodo.org/record/5774954/files/clm_snow.cover_esa.modis_p05_1km_s0..0cm_2001.01_epsg4326_v1.tif\"\nsnow = rast(snowurl)\nsnow## class       : SpatRaster \n## dimensions  : 17924, 43200, 1  (nrow, ncol, nlyr)\n## resolution  : 0.008333333, 0.008333333  (x, y)\n## extent      : -180, 180, -61.99666, 87.37  (xmin, xmax, ymin, ymax)\n## coord. ref. : lon/lat WGS 84 (EPSG:4326) \n## source      : clm_snow.cover_esa.modis_p05_1km_s0..0cm_2001.01_epsg4326_v1.tif \n## name        : clm_snow.cover_esa.modis_p05_1km_s0..0cm_2001.01_epsg4326_v1\ndimension_raster = rast(\n  nrows = 1000,\n  ncols = 1000,\n  resolution = 0.008333333,\n  xmin = 2.5,\n  xmax = 17.5,\n  ymin = 42.5,\n  ymax = 57.5\n)\nsnow_clipped = crop(snow, dimension_raster)\nsnow_clipped## class       : SpatRaster \n## dimensions  : 1800, 1800, 1  (nrow, ncol, nlyr)\n## resolution  : 0.008333333, 0.008333333  (x, y)\n## extent      : 2.499993, 17.49999, 42.50334, 57.50333  (xmin, xmax, ymin, ymax)\n## coord. ref. : lon/lat WGS 84 (EPSG:4326) \n## source      : memory \n## name        : clm_snow.cover_esa.modis_p05_1km_s0..0cm_2001.01_epsg4326_v1 \n## min value   :                                                            0 \n## max value   :                                                          100\nplot(snow_clipped, main=\"Snow cover around the Alps in Jan 2001\")\n# what it would look like with a local file \n#snow_rast = rast(\"2019_jan_snow_cover.tif\") \nsnow_rast = rast(\"/vsicurl/https://zenodo.org/record/5774954/files/clm_snow.cover_esa.modis_p95_1km_s0..0cm_2019.01_epsg4326_v1.tif?download=1\")\nUSA_NE_raster = rast(\n  nrows = 1000,\n  ncols = 1000,\n  resolution = 0.008333333,\n  xmin = -83.5,\n  xmax = -68.5,\n  ymin = 35.5,\n  ymax = 50.5\n)\ncropped_NE_snow = crop(snow_rast, USA_NE_raster)\nplot(cropped_NE_snow, main=\"USA Northeast snow cover in jan 2019\")"},{"path":"using-raster-data-in-r.html","id":"rasterization","chapter":"35 Using raster data in R","heading":"35.2.3 Rasterization","text":"(covers section 6.4)’s one type raster creation: transformations vectors,\nrasterize. Usage \nrequires knowledge spatial vector data time go depth\n, short, set extremely flexible operations convert\nvarious statistics vector set transform sort grid.\nExamples provided state boundaries, aggregating count average values\npoint clusters, ’re confident right vector datasets \npossibilities quite endless.Geocomputation R’s section 6.4 ’re curious.","code":""},{"path":"using-raster-data-in-r.html","id":"raster-manipulations-and-their-applications","chapter":"35 Using raster data in R","heading":"35.3 Raster Manipulations, and their applications","text":"knowing create well good, can \nrasters? use ?","code":""},{"path":"using-raster-data-in-r.html","id":"spatial-operations","chapter":"35 Using raster data in R","heading":"35.3.0.1 Spatial operations","text":"good number geometrically-based operations terra supports\nbox. sections Geocomputation R lays Subsetting 4.3.1,\nLocal, Focal,\nZonal,\nGlobal\nmap algebra operations sections 4.3.3 4.3.6, raster Merging\nsection 4.3.8","code":""},{"path":"using-raster-data-in-r.html","id":"subsetting","chapter":"35 Using raster data in R","heading":"35.3.1 Subsetting","text":"(covers section 4.3.1)ways (cleanly) draw values individual cells, cells,\nrows, columns ids coordinates, slice apart data.\ncellFromXY() assorted functions, \nterra::extract two good ones,\nthough beware overlap tidyverse’s extract. can also define smaller\nrasters clip crop , done examples prior.well direct access, know either desired row & column numbers\n1D cell ids:","code":"\nxy <- matrix(c(-73.94, 40.73), ncol = 2)\nmatrix_cells = cellFromXY(cropped_NE_snow, xy)\ncropped_NE_snow[matrix_cells]##   clm_snow.cover_esa.modis_p95_1km_s0..0cm_2019.01_epsg4326_v1\n## 1                                                           31\nterra::extract(cropped_NE_snow, xy)##   clm_snow.cover_esa.modis_p95_1km_s0..0cm_2019.01_epsg4326_v1\n## 1                                                           31\nfilter = rast(\n  xmin = -74.5,\n  xmax = -73.5,\n  ymin = 40.2,\n  ymax = 41.2,\n  resolution = 0.0083\n)\n\nNYC_sieve = terra::extract(cropped_NE_snow, ext(filter))\nhead(NYC_sieve)##   clm_snow.cover_esa.modis_p95_1km_s0..0cm_2019.01_epsg4326_v1\n## 1                                                           62\n## 2                                                           61\n## 3                                                           68\n## 4                                                           74\n## 5                                                           75\n## 6                                                           81\n# extract does a lot more things more flexibly, cellfromxy and related\n# better for method header precision and location manipulation\n# NYC was not very snowy that winter, it seems...\nsrtm[50, 50]##   srtm\n## 1 1885\nsrtm[c(49, 50, 51), c(49, 50, 51)]##   srtm\n## 1 1854\n## 2 1919\n## 3 1986\n## 4 1835\n## 5 1885\n## 6 1962\n## 7 1819\n## 8 1869\n## 9 1929\nsrtm[100]##   srtm\n## 1 2034"},{"path":"using-raster-data-in-r.html","id":"cropping-and-masking","chapter":"35 Using raster data in R","heading":"35.3.2 Cropping and Masking","text":"(covers section 6.2)Two similar concepts, crop() mask()\nperform task downsizing raster, typically tandem, require\nusage additional spatial object, practice typically vector.\ndiffer cropping reduces dimensions original raster\nbased second object’s boundaries, whereas masking sets everything\noutside second object’s boundaries zero, leaves structure intact.course can combine :","code":"\nzion = read_sf(system.file(\"vector/zion.gpkg\", package = \"spDataLarge\"))\nzion = st_transform(zion, crs(srtm))\nsrtm_cropped = crop(srtm, zion)\nsrtm_masked = mask(srtm, zion)\nplot(zion[0], main=\"Outline of Zion national park\")\nplot(srtm_cropped, main=\"SRTM cropped by Zion national park\")\nplot(srtm_masked, main=\"SRTM masked by Zion national park\")\nplot(crop(srtm_masked, zion))"},{"path":"using-raster-data-in-r.html","id":"map-algebra","chapter":"35 Using raster data in R","heading":"35.3.3 Map Algebra","text":"(covers sections 4.3.2, 4.3.3, 4.3.4,\n4.3.5, \n4.3.6)can see , many algebraic calculations can performed rasters local level.well boolean operationsAnd use app() efficiency large data.Given right attributes data (book lists coil class, pH, etc),\ncan used predictive modeling.Focal transformations:Similar sliding windows machine learning algorithms, focal operations\ntransform cells neighborhoods, working process extremes whilst\npreserving form original data. Another potential focal operator \nterrain(), calculates\nterrain characteristics elevation data.Zonal similar focal one\nlevel aggregation . uses second raster define zones group \nfirst , applying statistical measurements divided population.Global cross-raster statistical\ncalculations, can used distance\npossibilities–namely, summary functions ’d see type \ndataset.summary() describes archetypal\nstatistical bands, freq() counts \noccurrences class, boxplot(),\ndensity(),\nhist(),\npairs() can run ---box\nimmediate exploration. global() remains robust approach, can\nself-define statistical function apply particular dataset,\nmuch can aforementioned app() entourage.","code":"\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nplot(srtm)\nplot(log(srtm))\nplot(srtm ^ 2)\nplot(snow_clipped > 20, main=\"Areas with more than 20% snow cover\")\nplot(snow_clipped == 0, main=\"Areas with no snow cover\")\napp(snow_clipped, fun=function(i) { 2 * log(i) })## class       : SpatRaster \n## dimensions  : 1800, 1800, 1  (nrow, ncol, nlyr)\n## resolution  : 0.008333333, 0.008333333  (x, y)\n## extent      : 2.499993, 17.49999, 42.50334, 57.50333  (xmin, xmax, ymin, ymax)\n## coord. ref. : lon/lat WGS 84 (EPSG:4326) \n## source      : memory \n## name        :   lyr.1 \n## min value   :    - ?  \n## max value   : 9.21034\nmean_srtm = focal(srtm, w = matrix(1, nrow = 5, ncol = 5), fun = mean)\nplot(mean_srtm)\nsummary(snow_clipped) # 3rd quartile of 1, but mean of 10?##  clm_snow.cover_esa.modis_p05_1km_s0..0cm_2001.01_epsg4326_v1\n##  Min.   :  0.00                                              \n##  1st Qu.:  0.00                                              \n##  Median :  0.00                                              \n##  Mean   : 10.27                                              \n##  3rd Qu.:  1.00                                              \n##  Max.   :100.00                                              \n##  NA's   :28886\nsnow_tenths = app(snow_clipped, fun=function(i) {ceiling(i / 10) * 10})\nfreq(snow_tenths) # Indeed, 0 is by far the most common##    layer value   count\n## 1      1     0 1696965\n## 2      1    10  190843\n## 3      1    20   61456\n## 4      1    30   44132\n## 5      1    40   42039\n## 6      1    50   43384\n## 7      1    60   46092\n## 8      1    70   47859\n## 9      1    80   46767\n## 10     1    90   43627\n## 11     1   100   46772"},{"path":"using-raster-data-in-r.html","id":"extraction","chapter":"35 Using raster data in R","heading":"35.3.4 Extraction","text":"(covers section 6.3)Extraction process gathering targetted subset values raster,\nuse terra::extract().\ntake use vector spatial objects, result rather\npowerful specialized inferences. ’ll keep brief, please visit 6.3\n’re interested .Example use cases raster extraction can vary wildly creativity.\nquick examples, import boundaries European country\naround alps aggregate snow cover , find highest/lowest points\nalong Zion National Park’s boundaries. demonstrations book provides \npicking elevation selection points, elevation graph along \nline segment planning hike. Categorical continuous data can \nextracted, sometimes across different layers.One quick note–book points can performance issues scale,\nuse exact_extract() exactextract package possible.","code":""},{"path":"using-raster-data-in-r.html","id":"vectorization","chapter":"35 Using raster data in R","heading":"35.3.5 Vectorization","text":"(covers section 6.5)opposite rasterization, spatial vectorization transforms rasters vectors.\n’re familiar vectors, abundance flexibility , lying .spatvector class \nfunctions. One particularly eye-catching use quick visualization \ncontour lines using .contour() follows:","code":"\ncl = as.contour(srtm)\nplot(cl, axes=FALSE)"},{"path":"using-raster-data-in-r.html","id":"file-output","chapter":"35 Using raster data in R","heading":"35.3.6 File Output","text":"(covers section 8.7.2)\nOkay, ’ve done experiments, like preserve rasters\nshare results save later computational power. ?Simple. writeRaster().little involved, selecting bit datatype storage,\nfiletypes writeRaster can’t infer filename, names layer names,\nassortment memory-related variables debugging throughput \nparticularly large rasters. Compared loading rasters, though,\nfunction conveniently straightforward, yet still plenty capable terms\nflexibility; like rasters hole, rigid, yet surprisingly flexible just one layer beyond surface.","code":"\n# Is how I would do it, if I wanted to dirty the CC project\n# writeRaster(snow_clipped, filename=\"2007_jan_eu_snow.tif\", datatype=\"INT1U\",\n#             overwrite=TRUE)"},{"path":"make-geographical-maps-with-different-packages.html","id":"make-geographical-maps-with-different-packages","chapter":"36 Make Geographical Maps with Different Packages","heading":"36 Make Geographical Maps with Different Packages","text":"Xindi Deng","code":"\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(osmextract)"},{"path":"make-geographical-maps-with-different-packages.html","id":"introduction-3","chapter":"36 Make Geographical Maps with Different Packages","heading":"36.1 Introduction","text":"spacial analysis, always focus map USA extract dataset maps package R. However, number datasets maps package limited contain information sub-regions many countries, China, Japan .tutorial, first introduce common way plot map extracting coordinates “maps” package, talk “osmextract” package contains information geographical maps.","code":""},{"path":"make-geographical-maps-with-different-packages.html","id":"make-geographical-maps-with-map-package","chapter":"36 Make Geographical Maps with Different Packages","heading":"36.2 Make geographical maps with “map” package","text":"make geographical map, first need know coordinates region boundaries target country. introduce two ways extract data. first one user-friendly second one contains data sets.","code":""},{"path":"make-geographical-maps-with-different-packages.html","id":"import-dataset-with-function-map_data","chapter":"36 Make Geographical Maps with Different Packages","heading":"36.2.1 Import dataset with function “map_data”","text":"One common user-friendly functions extract map data maps package “map_data”. can turn data maps package data frame suitable plotting ggplot2.","code":""},{"path":"make-geographical-maps-with-different-packages.html","id":"usage-of-map_data","chapter":"36 Make Geographical Maps with Different Packages","heading":"36.2.1.1 Usage of “map_data”","text":"map_data(map, region = “.”, exact = FALSE, …)map: name map provided maps package.\n choices map include world map, three USA databases (usa, state, county), (including Italy database, France database, New Zealand database ). \n can use help(package='maps') check map data sets can used \"map_data\".map: name map provided maps package.region: select sub-regions map. Default “.”.region: select sub-regions map. Default “.”.","code":" The choices of map include a world map, three USA databases (usa, state, county), and more (including Italy database, France database, New Zealand database and so on). \n We can use help(package='maps') to check what map data sets can be used by \"map_data\"."},{"path":"make-geographical-maps-with-different-packages.html","id":"dataset-exploration","chapter":"36 Make Geographical Maps with Different Packages","heading":"36.2.1.2 Dataset exploration","text":"use map=“state” example see format function returnThe data set “states” 15537 coordinates 6 features.\n- long: longitude coordinate boundaries\n- lat: latitude coordinate boundaries\n- group: minimum closed region unique group number\n- region: name region","code":"\nstatesMap <- map_data(\"state\")\nhead(statesMap)##        long      lat group order  region subregion\n## 1 -87.46201 30.38968     1     1 alabama      <NA>\n## 2 -87.48493 30.37249     1     2 alabama      <NA>\n## 3 -87.52503 30.37249     1     3 alabama      <NA>\n## 4 -87.53076 30.33239     1     4 alabama      <NA>\n## 5 -87.57087 30.32665     1     5 alabama      <NA>\n## 6 -87.58806 30.32665     1     6 alabama      <NA>\nunique(statesMap$region)##  [1] \"alabama\"              \"arizona\"              \"arkansas\"            \n##  [4] \"california\"           \"colorado\"             \"connecticut\"         \n##  [7] \"delaware\"             \"district of columbia\" \"florida\"             \n## [10] \"georgia\"              \"idaho\"                \"illinois\"            \n## [13] \"indiana\"              \"iowa\"                 \"kansas\"              \n## [16] \"kentucky\"             \"louisiana\"            \"maine\"               \n## [19] \"maryland\"             \"massachusetts\"        \"michigan\"            \n## [22] \"minnesota\"            \"mississippi\"          \"missouri\"            \n## [25] \"montana\"              \"nebraska\"             \"nevada\"              \n## [28] \"new hampshire\"        \"new jersey\"           \"new mexico\"          \n## [31] \"new york\"             \"north carolina\"       \"north dakota\"        \n## [34] \"ohio\"                 \"oklahoma\"             \"oregon\"              \n## [37] \"pennsylvania\"         \"rhode island\"         \"south carolina\"      \n## [40] \"south dakota\"         \"tennessee\"            \"texas\"               \n## [43] \"utah\"                 \"vermont\"              \"virginia\"            \n## [46] \"washington\"           \"west virginia\"        \"wisconsin\"           \n## [49] \"wyoming\""},{"path":"make-geographical-maps-with-different-packages.html","id":"plot-maps","chapter":"36 Make Geographical Maps with Different Packages","heading":"36.2.2 Plot maps","text":"","code":""},{"path":"make-geographical-maps-with-different-packages.html","id":"the-logic-of-ploting-maps","chapter":"36 Make Geographical Maps with Different Packages","heading":"36.2.2.1 The logic of ploting maps","text":"plot maps, take minimum closed region polygon. Since know coordinates boundaries minimum closed region, can plot polygon taking longitude x-axis taking latitude y-axis. Since minimum closed region unique group number, can group coordinates group number plot polygons one graph.","code":""},{"path":"make-geographical-maps-with-different-packages.html","id":"plot-the-map","chapter":"36 Make Geographical Maps with Different Packages","heading":"36.2.2.2 Plot the map","text":"","code":"\nggplot(statesMap,aes(x=long,y=lat,group=group))+\n  geom_polygon(fill=\"white\",color=\"black\")\nNewYorkMap <- subset(statesMap, region %in% c(\"new jersey\", \"new york\",\"connecticut\"))\nggplot(NewYorkMap,aes(x=long,y=lat, group=group, fill = region))+\n  geom_polygon(color=\"white\")"},{"path":"make-geographical-maps-with-different-packages.html","id":"data-visualization-on-the-map","chapter":"36 Make Geographical Maps with Different Packages","heading":"36.2.2.3 Data visualization on the map","text":"Step 1: prepare data frame.Import dataset need visualized data frame. Combine new data frame data frame map region names. can get data frame suitable plotting.Step 2: plot visualization result","code":"\nmurders <- read_csv(\"./resources/make_geographical_maps_with_different_packages/murders.csv\", show_col_types = FALSE)\nmurders$region=tolower(murders$State)\nmurderMap <- merge(statesMap,murders,by=\"region\")\nhead(murderMap)##    region      long      lat group order subregion   State Population\n## 1 alabama -87.46201 30.38968     1     1      <NA> Alabama    4779736\n## 2 alabama -87.48493 30.37249     1     2      <NA> Alabama    4779736\n## 3 alabama -87.52503 30.37249     1     3      <NA> Alabama    4779736\n## 4 alabama -87.53076 30.33239     1     4      <NA> Alabama    4779736\n## 5 alabama -87.57087 30.32665     1     5      <NA> Alabama    4779736\n## 6 alabama -87.58806 30.32665     1     6      <NA> Alabama    4779736\n##   PopulationDensity Murders GunMurders GunOwnership\n## 1             94.65     199        135        0.517\n## 2             94.65     199        135        0.517\n## 3             94.65     199        135        0.517\n## 4             94.65     199        135        0.517\n## 5             94.65     199        135        0.517\n## 6             94.65     199        135        0.517\nggplot(murderMap,aes(x=long, y=lat, group=group,fill=GunMurders))+\n  geom_polygon(color=\"black\")+\n  scale_fill_gradient(low=\"white\",high=\"red\",guide=\"legend\")+\n  coord_fixed(1.5)"},{"path":"make-geographical-maps-with-different-packages.html","id":"make-geographical-maps-with-osmextract-package","chapter":"36 Make Geographical Maps with Different Packages","heading":"36.3 Make geographical maps with “osmextract” package","text":"Even though defined function “map_data” easy use, maps package contains data coordinates countries. extract coordinates boundaries countries, China, can use “osmextract” package.","code":""},{"path":"make-geographical-maps-with-different-packages.html","id":"import-dataset-with-object-openstreetmap_fr_zones","chapter":"36 Make Geographical Maps with Different Packages","heading":"36.3.1 Import dataset with object “openstreetmap_fr_zones”","text":"“openstreetmap_fr_zones” sf object geographical zones taken download.openstreetmap.fr.","code":""},{"path":"make-geographical-maps-with-different-packages.html","id":"define-a-function-to-extract-the-coordinations-data","chapter":"36 Make Geographical Maps with Different Packages","heading":"36.3.1.1 Define a function to extract the coordinations data","text":"define function called “extract_map_data”, can turn data openstreetmap_fr_zones data frame suitable plotting ggplot2.","code":"\nlibrary(sf)\nlibrary(osmextract) \n#> OpenStreetMap® is open data, licensed under the Open Data Commons Open Database License (ODbL) by the OpenStreetMap Foundation (OSMF).\n#> Data (c) OpenStreetMap contributors, ODbL 1.0. https://www.openstreetmap.org/copyright.\n#> Check the package website, https://docs.ropensci.org/osmextract/, for more details.\n\nextract_map_data <- function(region_name){\n  # get polygons in the target region\n  poly_region <- openstreetmap_fr_zones[which(tolower(openstreetmap_fr_zones$parent) == region_name), ]\n\n  # extract the coordinations and save the coordinations in a data.frame\n  poly_region_coords <- as.data.frame(st_coordinates(poly_region))\n\n  # extract the region name\n  my_times <- vapply(st_geometry(poly_region), function(x) nrow(st_coordinates(x)), numeric(1))\n  poly_region_coords$region_name <- rep(tolower(poly_region$name), times = my_times)\n  \n  # make the format of data frame similar to that from map_data and make it suitable for plotting with ggplot2\n  poly_region_coords$group <- paste(poly_region_coords$L1,\",\",poly_region_coords$L2,\",\",poly_region_coords$L3)\n  colnames(poly_region_coords) <- c(\"long\",\"lat\",\"group1\",\"group2\",\"group3\", \"region\",\"group\")\n  \n  # explore the data frame\n  print(head(poly_region_coords))\n  \n  return <- poly_region_coords \n}"},{"path":"make-geographical-maps-with-different-packages.html","id":"usage-of-extract_map_data","chapter":"36 Make Geographical Maps with Different Packages","heading":"36.3.1.2 Usage of “extract_map_data”","text":"extract_map_data(map)map: name region. available regions shown .","code":"\nunique(tolower(openstreetmap_fr_zones$parent))##  [1] NA                                 \"africa\"                          \n##  [3] \"spain\"                            \"asia\"                            \n##  [5] \"china\"                            \"india\"                           \n##  [7] \"indonesia\"                        \"japan\"                           \n##  [9] \"central-america\"                  \"europe\"                          \n## [11] \"austria\"                          \"belgium\"                         \n## [13] \"czech_republic\"                   \"finland\"                         \n## [15] \"france\"                           \"alsace\"                          \n## [17] \"aquitaine\"                        \"auvergne\"                        \n## [19] \"basse_normandie\"                  \"bourgogne\"                       \n## [21] \"bretagne\"                         \"centre\"                          \n## [23] \"champagne_ardenne\"                \"corse\"                           \n## [25] \"franche_comte\"                    \"haute_normandie\"                 \n## [27] \"ile_de_france\"                    \"languedoc_roussillon\"            \n## [29] \"limousin\"                         \"lorraine\"                        \n## [31] \"midi_pyrenees\"                    \"nord_pas_de_calais\"              \n## [33] \"pays_de_la_loire\"                 \"picardie\"                        \n## [35] \"poitou_charentes\"                 \"provence_alpes_cote_d_azur\"      \n## [37] \"rhone_alpes\"                      \"germany\"                         \n## [39] \"nordrhein_westfalen\"              \"italy\"                           \n## [41] \"netherlands\"                      \"norway\"                          \n## [43] \"poland\"                           \"portugal\"                        \n## [45] \"seas\"                             \"slovakia\"                        \n## [47] \"sweden\"                           \"switzerland\"                     \n## [49] \"ukraine\"                          \"united_kingdom\"                  \n## [51] \"england\"                          \"north-america\"                   \n## [53] \"canada\"                           \"ontario\"                         \n## [55] \"quebec\"                           \"us-west\"                         \n## [57] \"california\"                       \"oceania\"                         \n## [59] \"australia\"                        \"russia\"                          \n## [61] \"central_federal_district\"         \"far_eastern_federal_district\"    \n## [63] \"north_caucasian_federal_district\" \"northwestern_federal_district\"   \n## [65] \"siberian_federal_district\"        \"southern_federal_district\"       \n## [67] \"ural_federal_district\"            \"volga_federal_district\"          \n## [69] \"south-america\"                    \"argentina\"                       \n## [71] \"brazil\"                           \"central-west\"                    \n## [73] \"north\"                            \"northeast\"                       \n## [75] \"south\"                            \"southeast\""},{"path":"make-geographical-maps-with-different-packages.html","id":"plot-the-map-of-china","chapter":"36 Make Geographical Maps with Different Packages","heading":"36.3.2 Plot the map of China","text":"","code":"\nmap2 = extract_map_data(\"china\")##      long    lat group1 group2 group3 region     group\n## 1 114.900 32.950      1      1      1  anhui 1 , 1 , 1\n## 2 114.930 32.915      1      1      1  anhui 1 , 1 , 1\n## 3 115.000 32.920      1      1      1  anhui 1 , 1 , 1\n## 4 115.000 32.905      1      1      1  anhui 1 , 1 , 1\n## 5 115.015 32.890      1      1      1  anhui 1 , 1 , 1\n## 6 115.120 32.885      1      1      1  anhui 1 , 1 , 1\nggplot(map2,aes(x=long,y=lat,group=group))+\n  geom_polygon(fill=\"white\",color=\"black\")"},{"path":"make-geographical-maps-with-different-packages.html","id":"plot-the-map-of-japan","chapter":"36 Make Geographical Maps with Different Packages","heading":"36.3.3 Plot the map of Japan","text":"References:https://ggplot2.tidyverse.org/reference/map_data.htmlhttps://cran.r-project.org/web/packages/osmextract/vignettes/osmextract.html","code":"\nmap2 = extract_map_data(\"japan\")##      long    lat group1 group2 group3 region     group\n## 1 135.430 35.510      1      1      1  chubu 1 , 1 , 1\n## 2 135.450 35.495      1      1      1  chubu 1 , 1 , 1\n## 3 135.440 35.485      1      1      1  chubu 1 , 1 , 1\n## 4 135.445 35.450      1      1      1  chubu 1 , 1 , 1\n## 5 135.460 35.435      1      1      1  chubu 1 , 1 , 1\n## 6 135.470 35.435      1      1      1  chubu 1 , 1 , 1\nggplot(map2,aes(x=long,y=lat,group=group))+\n  geom_polygon(fill=\"white\",color=\"black\")"},{"path":"how-to-obtain-data-from-api-and-process-raw-json-data.html","id":"how-to-obtain-data-from-api-and-process-raw-json-data","chapter":"37 How to obtain data from API and process raw JSON data","heading":"37 How to obtain data from API and process raw JSON data","text":"Boping Xia Mingkang Yuan","code":"\n# load required packages, can be installed by calling install.packages()\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)"},{"path":"how-to-obtain-data-from-api-and-process-raw-json-data.html","id":"motivation-6","chapter":"37 How to obtain data from API and process raw JSON data","heading":"37.1 Motivation","text":"Obtaining data crucial part explore visualize . Although processed data provided R packages, realistic project, need collect data reliable source yet processed. ’ve able scrap data site. scratching data site amazingly questionable since simple alteration web page break whole scratching calculation. Moreover, data get may one need. sources may provide data CSV file formats. Still, sources use application programming interface (API) allow users access data. However, working final project proposal, took us long time get data website. One reason resources tutorials using APIs obscure people limited knowledge. Another reason source processes distributes data distinct ways. addition, data receive API commonly JSON file. tabular file format, need extra steps clean . tutorial guide using httr package request data API transcribe JSON files data frame table.","code":""},{"path":"how-to-obtain-data-from-api-and-process-raw-json-data.html","id":"what-is-api-and-json","chapter":"37 How to obtain data from API and process raw JSON data","heading":"37.2 What is API and JSON?","text":"API (application programming interface) method two programs share data. Usually, API unique settings (parameters). Today API usually refers web API, programs communicate Hypertext Transfer Protocol (HTTP). Online data sources typically use web API allows users obtain data remotely. APIs also require personalized token protect monitor data transmittance. API uses designated URLs make unique Internet. Compared data already file, API allows users download parts work .JSON (JavaScript Object Notation) file format aims exchange data efficiently Internet. JSON simple, human-readable writable, lightweight, widely accepted become standard. JSON stores data key/value pairs. six value types JSON: number, boolean, string, null, array, object. object collection key/value pairs. sample JSON file:example, department location string variables, yearEst number variable, employees array objects. element employees contains two string variables, name title, number variable, age, boolean variable, onDuty. null Wei’s title NA, means empty. Notice whole chunk data also object. nested structure objects allows JSON files efficiently store data complicated relations.","code":"{\n  \"department\": \"Technical\",\n  \"location\": \"New York\",\n  \"yearEst\": 2016,\n  \"employees\": [\n    {\n      \"name\": \"Tom\",\n      \"age\": 34,\n      \"title\": \"Manager\",\n      \"onDuty\": true\n    },\n    {\n      \"name\": \"Kate\",\n      \"age\": 24,\n      \"title\": \"Backend Developer\",\n      \"onDuty\": true\n    },\n    {\n      \"name\": \"Wei\",\n      \"age\": 21,\n      \"title\": null,\n      \"onDuty\": false\n    }\n  ]\n}"},{"path":"how-to-obtain-data-from-api-and-process-raw-json-data.html","id":"using-httr-package-to-request-data-from-api","chapter":"37 How to obtain data from API and process raw JSON data","heading":"37.3 Using httr package to request data from API","text":"","code":""},{"path":"how-to-obtain-data-from-api-and-process-raw-json-data.html","id":"introduction-to-http-verb-and-httr","chapter":"37 How to obtain data from API and process raw JSON data","heading":"37.3.1 Introduction to HTTP verb and httr","text":"Since tutorial faces people limited CS/Internet knowledge, dig little bit . key idea HTTP verb, action perform server (prefer HTTP method instead HTTP verb). five mainly used HTTP verbs: POST, GET, PUT, PATCH, DELETE. purpose obtaining data, usually use GET (contexts, use POST), getting specific resources server.httr package developed RStudio, easy powerful tool dealing HTTP. contains wrapper functions allow make designated requests server decode response server.","code":""},{"path":"how-to-obtain-data-from-api-and-process-raw-json-data.html","id":"example-of-use-httrget-to-request-data-from-api","chapter":"37 How to obtain data from API and process raw JSON data","heading":"37.3.2 Example of use httr::GET to request data from API","text":"WARNING: example illustrating workflow retrieving data API. Since API unique parameters, work differently. Therefore, want utilize API, first need go reference usage page understand communicate correctly.use example getting warships’ information game World Warships Wargaming developer API. Notice download data Wargaming API, must register account create application id. id unique , share id anyone else (others may use id something illegal like DDOS). demonstration purposes, created temporary id stored JSON file. temporary id DEACTIVATED Dec. 15, 2022. Please use id code.use GET() function obtain data API. two fields need pass function: url, address API, query, parameters need pass API. parsing response, checking status code good habit ensure successfully get data want. GET() method, code 200 means server received request sent corresponding data. received status codes, please check setup consult resources.receiving server’s response, need decode extract data want. Thankfully, content() automatically dig data inside response object.","code":"\nid_json <- read_json(\"resources/api_json_tutorial/application_id.json\")\napp_id <- id_json$id\n# The application id will be DEACTIVATED after Dec. 15, 2022 and will cause error in GET call.\napi_response <- GET(url = \"https://api.worldofwarships.eu/wows/encyclopedia/ships/\",\n                    query = list(application_id = app_id))\napi_response$status_code## [1] 200\nraw_json <- content(api_response)\nglimpse(raw_json, list.len = 5)## List of 3\n##  $ status: chr \"ok\"\n##  $ meta  :List of 5\n##   ..$ count     : int 100\n##   ..$ page_total: int 7\n##   ..$ total     : int 608\n##   ..$ limit     : int 100\n##   ..$ page      : int 1\n##  $ data  :List of 100\n##   ..$ 3542005744:List of 19\n##   .. ..$ description     : chr \"Brave and energetic like her elder sister, Cleveland. Loves battles and stands up to even the strongest of foes\"| __truncated__\n##   .. ..$ price_gold      : int 11000\n##   .. ..$ ship_id_str     : chr \"PASC718\"\n##   .. ..$ has_demo_profile: logi FALSE\n##   .. ..$ images          :List of 4\n##   .. .. [list output truncated]\n##   ..$ 3553572848:List of 19\n##   .. ..$ description     : chr \"A Tennessee-class battleship armed with twelve 356 mm guns and possessing quite modest speed characteristics. A\"| __truncated__\n##   .. ..$ price_gold      : int 9500\n##   .. ..$ ship_id_str     : chr \"PASB707\"\n##   .. ..$ has_demo_profile: logi FALSE\n##   .. ..$ images          :List of 4\n##   .. .. [list output truncated]\n##   ..$ 3541972176:List of 19\n##   .. ..$ description     : chr \"Orks do not navigate the Warp. They need no Astronomican like pesky Umiez. Gork and Mork guide them to WAAAGH! \"| __truncated__\n##   .. ..$ price_gold      : int 10000\n##   .. ..$ ship_id_str     : chr \"PZSD718\"\n##   .. ..$ has_demo_profile: logi FALSE\n##   .. ..$ images          :List of 4\n##   .. .. [list output truncated]\n##   ..$ 3741234640:List of 19\n##   .. ..$ description     : chr \"One of the versions of a project of a small light cruiser (MLK-8-152). Her main battery artillery consisted of \"| __truncated__\n##   .. ..$ price_gold      : int 11000\n##   .. ..$ ship_id_str     : chr \"PRSC528\"\n##   .. ..$ has_demo_profile: logi FALSE\n##   .. ..$ images          :List of 4\n##   .. .. [list output truncated]\n##   ..$ 3655251408:List of 19\n##   .. ..$ description     : chr \"Developed after World War II, a project of a small light cruiser (MLK 16 x 130) equipped with sixteen dual-purp\"| __truncated__\n##   .. ..$ price_gold      : int 34650\n##   .. ..$ ship_id_str     : chr \"PRSC610\"\n##   .. ..$ has_demo_profile: logi FALSE\n##   .. ..$ images          :List of 4\n##   .. .. [list output truncated]\n##   .. [list output truncated]"},{"path":"how-to-obtain-data-from-api-and-process-raw-json-data.html","id":"transform-json-to-data-frame","chapter":"37 How to obtain data from API and process raw JSON data","heading":"37.4 Transform JSON to data frame","text":"","code":""},{"path":"how-to-obtain-data-from-api-and-process-raw-json-data.html","id":"json-structure-analysis-list","chapter":"37 How to obtain data from API and process raw JSON data","heading":"37.4.1 JSON structure analysis (list)","text":"learned class deal CSV data, fixed approach convert data data frame ’s structure ’re used working data (’s goal). However, data need stored table columns rows. tabular format inefficient, many Internet companies never store raw data format. example, let’s say store following buys. distinctive visits may stamped entire sum cash went , length visit, taken point--point thing data (item name, brand, quantity, product id, etc.). ’s lot detailed information, putting away data single table isn’t exceptionally productive since got rehash total information (add get time) point--point thing. different tables one solution, consumers dataset typically prefer single data source information. leads existence hierarchical data types like lists. use read_json() read JSON file return data type list. , handle JSON files equivalent handle lists.can see resulting data structure applying content function api_response list. Lists foremost adaptable data type can contain sorts information. list can contain distinctive diverse sorts information component, indeed another list. element list length another element. makes lists flexible also relatively difficult use. Notice case, found raw_json three layers. first layer contains information status, type character. second layer list containing elements meta data (summarized data) count, page, total… applying summary() function second layer (meta), get detailed information inside second layer (meta) raw_json. example, know seven pages. third layer crucial part, huge list containing warship data 100 elements (meaning 100 warships). element list well. apply names() function third layer (data), get id’s . explored , likely deduced different “names” correspond different warships. can imagine wanted study warship, want create data frame rows correspond different warships columns correspond different features warships inside third layer name, nation, type, tier.","code":"\ntypeof(raw_json)## [1] \"list\"\nsummary(raw_json)##        Length Class  Mode     \n## status   1    -none- character\n## meta     5    -none- list     \n## data   100    -none- list\nsummary(raw_json$meta)##            Length Class  Mode   \n## count      1      -none- numeric\n## page_total 1      -none- numeric\n## total      1      -none- numeric\n## limit      1      -none- numeric\n## page       1      -none- numeric\nprint(raw_json$meta)## $count\n## [1] 100\n## \n## $page_total\n## [1] 7\n## \n## $total\n## [1] 608\n## \n## $limit\n## [1] 100\n## \n## $page\n## [1] 1\nnames(raw_json$data)##   [1] \"3542005744\" \"3553572848\" \"3541972176\" \"3741234640\" \"3655251408\"\n##   [6] \"3338548944\" \"3560912592\" \"3553572560\" \"3340744656\" \"3551475152\"\n##  [11] \"3667834576\" \"3554555600\" \"3532568272\" \"3667802064\" \"3667900400\"\n##  [16] \"3332323024\" \"3340711888\" \"3315513040\" \"3545183952\" \"3730749392\"\n##  [21] \"3740153648\" \"3730749424\" \"3666818864\" \"3333404368\" \"3741202128\"\n##  [26] \"3551442640\" \"3445536752\" \"3731830736\" \"3554588464\" \"3340678608\"\n##  [31] \"3659445712\" \"3552491216\" \"3669964624\" \"3668916016\" \"3541972688\"\n##  [36] \"3553539792\" \"3363780304\" \"3542005456\" \"3667900208\" \"3522082512\"\n##  [41] \"3740219088\" \"3655251664\" \"3374266064\" \"3542005552\" \"3658397424\"\n##  [46] \"3741267408\" \"3668883440\" \"3555636944\" \"3550394352\" \"3337500656\"\n##  [51] \"3340678960\" \"3655284528\" \"3655218960\" \"3730748880\" \"3730782032\"\n##  [56] \"3550360880\" \"3730782192\" \"3655251952\" \"3666786288\" \"3740186416\"\n##  [61] \"3720263664\" \"3530504176\" \"3555670000\" \"3741267760\" \"3355359056\"\n##  [66] \"3655218480\" \"3741267792\" \"3667867440\" \"3676223184\" \"3553540080\"\n##  [71] \"3555669712\" \"3335501808\" \"3353294672\" \"3543054032\" \"3554621136\"\n##  [76] \"3340711728\" \"3550393808\" \"3679369200\" \"3667900112\" \"3655251920\"\n##  [81] \"3655317296\" \"3551410160\" \"3667801808\" \"3741235184\" \"3333371888\"\n##  [86] \"3730814960\" \"3667801296\" \"3655284688\" \"3720263120\" \"3552524016\"\n##  [91] \"3666818896\" \"3552524272\" \"3668850672\" \"3741235152\" \"3552523984\"\n##  [96] \"3667867632\" \"3551409616\" \"3340645840\" \"3655219184\" \"3552458448\""},{"path":"how-to-obtain-data-from-api-and-process-raw-json-data.html","id":"process-to-extract-target-features-from-a-list","chapter":"37 How to obtain data from API and process raw JSON data","heading":"37.4.2 Process to extract target features from a list","text":"","code":""},{"path":"how-to-obtain-data-from-api-and-process-raw-json-data.html","id":"steps-to-create-extraction-function","chapter":"37 How to obtain data from API and process raw JSON data","heading":"37.4.2.1 Steps to create extraction function","text":"’s code grab features (name, nation, type, tier) warship.\ndemo step step. figuring structure huge list (raw_json), let’s extract information need project. clarify, let’s pull elements (name, nation, type, tier) need first warship. example, want get name first warship, following.Note codes give result name first warship. remember, need extract 100 names warships. second approach best choice.Applying names() function first (data[[1]]) third layers (data) raw_json data, found target features (name, nation, type tier) layer.Running code , extract target features first warship. ’re going take remaining 99 warships, row warship, column target feature, turn manageable data frame type.However, amount data want extract increases, creating function extracts data given individual “data” might better. , let’s rewrite code define function easily:get first row aimed data frame. output expected. Right now, formally define function:","code":"\nraw_json$data$`3542005744`$name## [1] \"AL Montpelier\"\nraw_json$data[[1]]$name## [1] \"AL Montpelier\"\nnames(raw_json$data[[1]])##  [1] \"description\"      \"price_gold\"       \"ship_id_str\"      \"has_demo_profile\"\n##  [5] \"images\"           \"modules\"          \"modules_tree\"     \"nation\"          \n##  [9] \"is_premium\"       \"ship_id\"          \"price_credit\"     \"default_profile\" \n## [13] \"upgrades\"         \"tier\"             \"next_ships\"       \"mod_slots\"       \n## [17] \"type\"             \"is_special\"       \"name\"\nname = raw_json$data[[1]]$name\nnation = raw_json$data[[1]]$nation\ntype = raw_json$data[[1]]$type\ntier = raw_json$data[[1]]$tier\nname ## [1] \"AL Montpelier\"\nnation ## [1] \"usa\"\ntype ## [1] \"Cruiser\"\ntier## [1] 8\ndata = raw_json$data[[1]]\nresult = data.frame(\n  name = data$name,\n  nation = data$nation,\n  type = data$type,\n  tier = data$tier)\nresult##            name nation    type tier\n## 1 AL Montpelier    usa Cruiser    8\nextract_data = function(data){\n  result = data.frame(\n    name = data$name,\n    nation = data$nation,\n    type = data$type,\n    tier = data$tier)\n  return(result)\n}\n\nextract_data(raw_json$data[[1]])##            name nation    type tier\n## 1 AL Montpelier    usa Cruiser    8"},{"path":"how-to-obtain-data-from-api-and-process-raw-json-data.html","id":"lapply-function","chapter":"37 How to obtain data from API and process raw JSON data","heading":"37.4.2.2 lapply() function","text":"function works perfectly, returning results previous section. now can extract warship information using lapply() function:first argument lapply() list. second input function. lapply() apply function element list provided return output list format. applies given function element list, several function calls. example, case, function called extract_data, lapply() function applies extract_data function list called data_features, also returns list. Notice lapply() function:input data list, output data listThe length input equal length outputThe function “distributed” across element within list","code":"\ndata_features = raw_json$data\nwarship_list = lapply(data_features, extract_data)\ntypeof(warship_list)## [1] \"list\"\nhead(warship_list,2)## $`3542005744`\n##            name nation    type tier\n## 1 AL Montpelier    usa Cruiser    8\n## \n## $`3553572848`\n##         name nation       type tier\n## 1 California    usa Battleship    7"},{"path":"how-to-obtain-data-from-api-and-process-raw-json-data.html","id":"do.call-function","chapter":"37 How to obtain data from API and process raw JSON data","heading":"37.4.2.3 do.call() function","text":"final step combining outputs .call(). code makes list component contains information outline single row. Right now, need make single data frame. .call() applies given function list whole, one function call. case, use rbind() function combine rows.","code":"\nwarship = do.call(rbind,warship_list)\nhead(warship)##                     name   nation       type tier\n## 3542005744 AL Montpelier      usa    Cruiser    8\n## 3553572848    California      usa Battleship    7\n## 3541972176   Ship Smasha pan_asia  Destroyer    8\n## 3741234640       Ochakov     ussr    Cruiser    8\n## 3655251408      Smolensk     ussr    Cruiser   10\n## 3338548944   [Shimakaze]    japan  Destroyer   10"},{"path":"how-to-obtain-data-from-api-and-process-raw-json-data.html","id":"why-introducing-lappy-and-do.call","chapter":"37 How to obtain data from API and process raw JSON data","heading":"37.4.3 Why introducing lappy() and do.call()","text":"programming perspective, using lapply() .call() approaches can make code easier read. also straightforward modify want add/remove parts data frame want create. also directly apply different dataset warship data. contrast, loop-based approach, ’d need change every result occurrence loop defining original outcome variable .","code":""},{"path":"how-to-obtain-data-from-api-and-process-raw-json-data.html","id":"filter-desired-data-by-directly-passing-parameters-to-api","chapter":"37 How to obtain data from API and process raw JSON data","heading":"37.5 Filter desired data by directly passing parameters to API","text":"showed parse extract useful data huge JSON file. However, one advantage API can request data need server. API like Wargaming provides detailed information reference page tells API works parameters receives. assume every API thoroughly documents usage. context poorly described API, excavating useful information mess needed. figuring information need, can pass corresponding parameters convert data frame. Continuing example, read reference page, know setting fields parameters filter entry want.saw name, nation, type, tier entries obtained 100 warships. data relatively neat, don’t need call lapply() .call() convert data frame. Instead, use enframe() unlist() jointly remove nested structure. return us long list.looks like tidy data set combine two variables one. Calling separate() name generate tidy data set. call pivot_wider() tidy data obtain desired data frame analysis.","code":"\napi_response_filtered <- GET(url = \"https://api.worldofwarships.eu/wows/encyclopedia/ships/\",\n                    query = list(\n                      application_id = app_id,\n                      fields = \"name, nation, tier, type\"\n                                 ))\nraw_json_filtered <- content(api_response_filtered)\nglimpse(raw_json_filtered, list.len = 5)## List of 3\n##  $ status: chr \"ok\"\n##  $ meta  :List of 5\n##   ..$ count     : int 100\n##   ..$ page_total: int 7\n##   ..$ total     : int 608\n##   ..$ limit     : int 100\n##   ..$ page      : int 1\n##  $ data  :List of 100\n##   ..$ 3542005744:List of 4\n##   .. ..$ tier  : int 8\n##   .. ..$ type  : chr \"Cruiser\"\n##   .. ..$ name  : chr \"AL Montpelier\"\n##   .. ..$ nation: chr \"usa\"\n##   ..$ 3553572848:List of 4\n##   .. ..$ tier  : int 7\n##   .. ..$ type  : chr \"Battleship\"\n##   .. ..$ name  : chr \"California\"\n##   .. ..$ nation: chr \"usa\"\n##   ..$ 3541972176:List of 4\n##   .. ..$ tier  : int 8\n##   .. ..$ type  : chr \"Destroyer\"\n##   .. ..$ name  : chr \"Ship Smasha\"\n##   .. ..$ nation: chr \"pan_asia\"\n##   ..$ 3741234640:List of 4\n##   .. ..$ tier  : int 8\n##   .. ..$ type  : chr \"Cruiser\"\n##   .. ..$ name  : chr \"Ochakov\"\n##   .. ..$ nation: chr \"ussr\"\n##   ..$ 3655251408:List of 4\n##   .. ..$ tier  : int 10\n##   .. ..$ type  : chr \"Cruiser\"\n##   .. ..$ name  : chr \"Smolensk\"\n##   .. ..$ nation: chr \"ussr\"\n##   .. [list output truncated]\ndenested_list <- enframe(unlist(raw_json_filtered$data))\nhead(denested_list, 8)## # A tibble: 8 × 2\n##   name              value        \n##   <chr>             <chr>        \n## 1 3542005744.tier   8            \n## 2 3542005744.type   Cruiser      \n## 3 3542005744.name   AL Montpelier\n## 4 3542005744.nation usa          \n## 5 3553572848.tier   7            \n## 6 3553572848.type   Battleship   \n## 7 3553572848.name   California   \n## 8 3553572848.nation usa\nwarship_dataframe <- denested_list %>%\n  separate(name, into = c(\"id\", \"entry\")) %>% \n  pivot_wider(names_from = entry, values_from = value)\nhead(warship_dataframe)## # A tibble: 6 × 5\n##   id         tier  type       name          nation  \n##   <chr>      <chr> <chr>      <chr>         <chr>   \n## 1 3542005744 8     Cruiser    AL Montpelier usa     \n## 2 3553572848 7     Battleship California    usa     \n## 3 3541972176 8     Destroyer  Ship Smasha   pan_asia\n## 4 3741234640 8     Cruiser    Ochakov       ussr    \n## 5 3655251408 10    Cruiser    Smolensk      ussr    \n## 6 3338548944 10    Destroyer  [Shimakaze]   japan"},{"path":"how-to-obtain-data-from-api-and-process-raw-json-data.html","id":"evaluation-2","chapter":"37 How to obtain data from API and process raw JSON data","heading":"37.6 Evaluation","text":"writing tutorial topic, gained clearer understanding legally use API get data source website understand structure JSON files lists r language. tutorial can also said final project students service, help students get data clean data. Getting data cleaning time-consuming unique steps project (every data resource different). purpose tutorial cover two steps general way possible. course, wanted improve tutorial . case, might insert graphic images explain list structure detail since believe architectural images always intuitive. also talk little bit lapply() functions (apply() family), maybe mention logic inherent functions like mapping, also important understanding function.","code":""},{"path":"how-to-obtain-data-from-api-and-process-raw-json-data.html","id":"reference-3","chapter":"37 How to obtain data from API and process raw JSON data","heading":"37.7 Reference","text":"https://cran.r-project.org/web/packages/httr/vignettes/quickstart.htmlhttps://cran.r-project.org/web/packages/jsonlite/vignettes/json-aaquickstart.htmlhttps://www.rdocumentation.org/packages/base/versions/3.6.2/topics/lapplyhttps://www.rdocumentation.org/packages/base/versions/3.6.2/topics/.callhttps://developers.wargaming.net/documentation/guide/getting-started/https://developer.mozilla.org/en-US/docs/Web/HTTP/Statushttps://www.json.org/json-en.htmlhttps://nordicapis.com/ultimate-guide---9-standard-http-methods/https://www.mulesoft.com/resources/api/---apihttps://leewtai.github.io/courses/stat_computing/lectures/learning_r_data_wrangle.html","code":""},{"path":"sample-statistic-tutorial.html","id":"sample-statistic-tutorial","chapter":"38 Sample Statistic Tutorial","heading":"38 Sample Statistic Tutorial","text":"Yuqing Hong William Gu","code":"\nlibrary(tidyverse)\nlibrary(boot)\nlibrary(ggplot2)"},{"path":"sample-statistic-tutorial.html","id":"motivation-7","chapter":"38 Sample Statistic Tutorial","heading":"38.1 Motivation","text":"guide introduce basic statistical inferences data scientist, including Data Cleaning, Bootstrapping, Hypothesis Testing. starting raw data, use methods preprocess clean messy data time better understand improves data quantity. Whether sample statistics can used good approximation population? provide methods test sample parameters. tutorial, provide step--step instructions implement statistical inferences R. Hopefully help next time working analyzing data!","code":""},{"path":"sample-statistic-tutorial.html","id":"data-cleaning-1","chapter":"38 Sample Statistic Tutorial","heading":"38.2 Data Cleaning","text":"reality, raw data collected usually messy. Raw data may contain wrong data types, duplicate data unique, . Thus motivated preprocess raw data consistent data improves data quality can better analyzed,","code":""},{"path":"sample-statistic-tutorial.html","id":"duplicated-data","chapter":"38 Sample Statistic Tutorial","heading":"38.2.1 Duplicated Data","text":"variables unique data set, drop duplicate rows. example, data set containing students’ grades, student ID unique student, thus avoid duplicates IDs., introduce three functions can used remove duplicates.function unique() (R base function):\nextracting unique elements vector:\n\nv <- c(1, 2, 2, 6, 4, 6, 4)\nunique(v) \n## [1] 1 2 6 4\nextracting unique rows data frame\nRemark: entries considered duplicated row.\n\n\nunique(stud_grades)\n## # tibble: 6 × 3\n##   student_id name      grade\n##        <dbl> <chr>     <dbl>\n## 1          1 Marry        97\n## 2          2 John         76\n## 3          3 Mike         84\n## 4          4 Emma         85\n## 5          5 Elizabeth    74\n## 6          6 Elizabeth    73function unique() (R base function):extracting unique elements vector:extracting unique rows data frame\nRemark: entries considered duplicated row.\nRemark: entries considered duplicated row.function distinct() (dplyr package): first row preserved duplicate rows.\n\ndistinct(stud_grades)\n## # tibble: 6 × 3\n##   student_id name      grade\n##        <dbl> <chr>     <dbl>\n## 1          1 Marry        97\n## 2          2 John         76\n## 3          3 Mike         84\n## 4          4 Emma         85\n## 5          5 Elizabeth    74\n## 6          6 Elizabeth    73\nwant drop duplicates based particular variables:\n\n#remove duplicated rows based variable \"name\"\n#'.keep_all = TRUE' means output variable 'name' output            #variables \ndistinct(stud_grades, name, .keep_all = TRUE)\n## # tibble: 5 × 3\n##   student_id name      grade\n##        <dbl> <chr>     <dbl>\n## 1          1 Marry        97\n## 2          2 John         76\n## 3          3 Mike         84\n## 4          4 Emma         85\n## 5          5 Elizabeth    74function distinct() (dplyr package): first row preserved duplicate rows.want drop duplicates based particular variables:function duplicated() (R base function): identifying input element duplicated returning Boolean results.\nextracting unique elements vector:\nRemark: duplicated() return TRUE duplicated elements, thus !duplicated() used obtain unique elements.\n\n\nv <- c(1, 2, 2, 6, 4, 6, 4)\nv[!duplicated(v)] #used filter \n## [1] 1 2 6 4\nextracting unique rows data frame\n\nstud_grades[!duplicated(stud_grades$student_id),]\n## # tibble: 6 × 3\n##   student_id name      grade\n##        <dbl> <chr>     <dbl>\n## 1          1 Marry        97\n## 2          2 John         76\n## 3          3 Mike         84\n## 4          4 Emma         85\n## 5          5 Elizabeth    74\n## 6          6 Elizabeth    73function duplicated() (R base function): identifying input element duplicated returning Boolean results.extracting unique elements vector:\nRemark: duplicated() return TRUE duplicated elements, thus !duplicated() used obtain unique elements.\nRemark: duplicated() return TRUE duplicated elements, thus !duplicated() used obtain unique elements.extracting unique rows data frame","code":"\nlibrary(tidyverse)\n\nstud_grades <- tibble(\n  student_id = c(1, 2, 3, 4, 5, 4, 6),\n  name = c(\"Marry\", \"John\", \"Mike\", \"Emma\", \"Elizabeth\", \"Emma\", \"Elizabeth\"),  \n  grade = c(97, 76, 84, 85, 74, 85, 73)\n  )\n\nstud_grades## # A tibble: 7 × 3\n##   student_id name      grade\n##        <dbl> <chr>     <dbl>\n## 1          1 Marry        97\n## 2          2 John         76\n## 3          3 Mike         84\n## 4          4 Emma         85\n## 5          5 Elizabeth    74\n## 6          4 Emma         85\n## 7          6 Elizabeth    73\nv <- c(1, 2, 2, 6, 4, 6, 4)\nunique(v) ## [1] 1 2 6 4\nunique(stud_grades)## # A tibble: 6 × 3\n##   student_id name      grade\n##        <dbl> <chr>     <dbl>\n## 1          1 Marry        97\n## 2          2 John         76\n## 3          3 Mike         84\n## 4          4 Emma         85\n## 5          5 Elizabeth    74\n## 6          6 Elizabeth    73\ndistinct(stud_grades)## # A tibble: 6 × 3\n##   student_id name      grade\n##        <dbl> <chr>     <dbl>\n## 1          1 Marry        97\n## 2          2 John         76\n## 3          3 Mike         84\n## 4          4 Emma         85\n## 5          5 Elizabeth    74\n## 6          6 Elizabeth    73\n#remove duplicated rows based on variable \"name\"\n#'.keep_all = TRUE' means that not only output the variable 'name' but output all            #variables \ndistinct(stud_grades, name, .keep_all = TRUE)## # A tibble: 5 × 3\n##   student_id name      grade\n##        <dbl> <chr>     <dbl>\n## 1          1 Marry        97\n## 2          2 John         76\n## 3          3 Mike         84\n## 4          4 Emma         85\n## 5          5 Elizabeth    74\nv <- c(1, 2, 2, 6, 4, 6, 4)\nv[!duplicated(v)] #used as a filter ## [1] 1 2 6 4\nstud_grades[!duplicated(stud_grades$student_id),]## # A tibble: 6 × 3\n##   student_id name      grade\n##        <dbl> <chr>     <dbl>\n## 1          1 Marry        97\n## 2          2 John         76\n## 3          3 Mike         84\n## 4          4 Emma         85\n## 5          5 Elizabeth    74\n## 6          6 Elizabeth    73"},{"path":"sample-statistic-tutorial.html","id":"missing-data","chapter":"38 Sample Statistic Tutorial","heading":"38.2.2 Missing Data","text":"Reasons missing data\nSensor error data collection\nData entry error\nSurvey-subject refuse answer questions\netc.\n\n#create simple data frame example visualization\nfruits <- tibble(\n  name = c(\"apple\", \"banana\", \"orange\", \"strawberry\", \"blueberry\"),  \n  price    = .numeric(c(\"2\", \"3\", \"3\", \"four\" ,\"5\")),\n  weight = c(14.5, 4.4, NA, 5.1, 5.7)\n  )\nfruits\n## # tibble: 5 × 3\n##   name       price weight\n##   <chr>      <dbl>  <dbl>\n## 1 apple          2   14.5\n## 2 banana         3    4.4\n## 3 orange         3   NA  \n## 4 strawberry    NA    5.1\n## 5 blueberry      5    5.7\nexample, value missing processing, value missing wrong data type correctly processed.Sensor error data collectionData entry errorSurvey-subject refuse answer questionsetc.example, value missing processing, value missing wrong data type correctly processed.Dealing missing data\nremove fill missing data many models can’t handle missing data. introduce two basic approaches deal NA entries.\n\n#na.rm = TRUE shoule set tell function ignore NAs.\nmean(fruits$price, na.rm = TRUE)\n## [1] 3.25\n\n#return Boolean input element missing value .\n.na(fruits$price)\n## [1] FALSE FALSE FALSE  TRUE FALSE\nMethod 1: Drop entire row\nfunction na.omit() removes rows containing missing data.\nfunction complete.cases() can used removes rows containing missing data particular columns. return logical vector specifying rows missing values across columns specified.\n\n\nna.omit(fruits)\n## # tibble: 3 × 3\n##   name      price weight\n##   <chr>     <dbl>  <dbl>\n## 1 apple         2   14.5\n## 2 banana        3    4.4\n## 3 blueberry     5    5.7\n\n#used filter instead outputting directly\n#input fruits[ , 3:3] tell let function consider NAs column #3 (\"weight\")\nfruits[complete.cases(fruits[ , 3:3]),] \n## # tibble: 4 × 3\n##   name       price weight\n##   <chr>      <dbl>  <dbl>\n## 1 apple          2   14.5\n## 2 banana         3    4.4\n## 3 strawberry    NA    5.1\n## 4 blueberry      5    5.7\n - Disadvantages: may lose large number data removing rows.\nMethod 2: Impute data column\nInstead removing data, can fill missing values value inferred existing values column. Common filler values 1)mean, 2)median 3)mode.\n\n\nfruits_new <- fruits\n\n#1. fill NAs 'weight' median weight existing values\nfruits_new$weight = ifelse(.na(fruits_new$weight), \n                           median(fruits_new$weight,na.rm = TRUE),\n                           fruits_new$weight)\n\n#2. fill NAs 'price' mean price existing values  \nfruits_new$price = ifelse(.na(fruits_new$price), \n                          mean(fruits_new$price,na.rm = TRUE),\n                          fruits_new$price)\nfruits_new\n## # tibble: 5 × 3\n##   name       price weight\n##   <chr>      <dbl>  <dbl>\n## 1 apple       2      14.5\n## 2 banana      3       4.4\n## 3 orange      3       5.4\n## 4 strawberry  3.25    5.1\n## 5 blueberry   5       5.7\n - Disadvantages: may lose feature interactions filling.remove fill missing data many models can’t handle missing data. introduce two basic approaches deal NA entries.Method 1: Drop entire row\nfunction na.omit() removes rows containing missing data.\nfunction complete.cases() can used removes rows containing missing data particular columns. return logical vector specifying rows missing values across columns specified.\nfunction na.omit() removes rows containing missing data.function na.omit() removes rows containing missing data.function complete.cases() can used removes rows containing missing data particular columns. return logical vector specifying rows missing values across columns specified.function complete.cases() can used removes rows containing missing data particular columns. return logical vector specifying rows missing values across columns specified.Method 2: Impute data column\nInstead removing data, can fill missing values value inferred existing values column. Common filler values 1)mean, 2)median 3)mode.\nInstead removing data, can fill missing values value inferred existing values column. Common filler values 1)mean, 2)median 3)mode.","code":"\n#create a simple data frame as an example to visualization\nfruits <- tibble(\n  name = c(\"apple\", \"banana\", \"orange\", \"strawberry\", \"blueberry\"),  \n  price    = as.numeric(c(\"2\", \"3\", \"3\", \"four\" ,\"5\")),\n  weight = c(14.5, 4.4, NA, 5.1, 5.7)\n  )\nfruits## # A tibble: 5 × 3\n##   name       price weight\n##   <chr>      <dbl>  <dbl>\n## 1 apple          2   14.5\n## 2 banana         3    4.4\n## 3 orange         3   NA  \n## 4 strawberry    NA    5.1\n## 5 blueberry      5    5.7\n#na.rm = TRUE shoule be set to tell the function ignore NAs.\nmean(fruits$price, na.rm = TRUE)## [1] 3.25\n#return Boolean if the input element is a missing value or not.\nis.na(fruits$price)## [1] FALSE FALSE FALSE  TRUE FALSE\nna.omit(fruits)## # A tibble: 3 × 3\n##   name      price weight\n##   <chr>     <dbl>  <dbl>\n## 1 apple         2   14.5\n## 2 banana        3    4.4\n## 3 blueberry     5    5.7\n#used as a filter instead of outputting directly\n#The input fruits[ , 3:3] tell let function only consider NAs on the column #3 (\"weight\")\nfruits[complete.cases(fruits[ , 3:3]),] ## # A tibble: 4 × 3\n##   name       price weight\n##   <chr>      <dbl>  <dbl>\n## 1 apple          2   14.5\n## 2 banana         3    4.4\n## 3 strawberry    NA    5.1\n## 4 blueberry      5    5.7 - Disadvantages: we may lose a large number of data after removing those rows.\nfruits_new <- fruits\n\n#1. fill the NAs in 'weight' by the median weight of existing values\nfruits_new$weight = ifelse(is.na(fruits_new$weight), \n                           median(fruits_new$weight,na.rm = TRUE),\n                           fruits_new$weight)\n\n#2. fill the NAs in 'price' by the mean price of existing values  \nfruits_new$price = ifelse(is.na(fruits_new$price), \n                          mean(fruits_new$price,na.rm = TRUE),\n                          fruits_new$price)\nfruits_new## # A tibble: 5 × 3\n##   name       price weight\n##   <chr>      <dbl>  <dbl>\n## 1 apple       2      14.5\n## 2 banana      3       4.4\n## 3 orange      3       5.4\n## 4 strawberry  3.25    5.1\n## 5 blueberry   5       5.7 - Disadvantages: it may lose feature interactions after filling."},{"path":"sample-statistic-tutorial.html","id":"hypothesis-testing","chapter":"38 Sample Statistic Tutorial","heading":"38.3 Hypothesis Testing","text":"transform raw sample data consistent data ready statistical inference, can now obtain desired sample statistic estimate population. can use hypothesis testing check whether sample statistic good approximation population.","code":""},{"path":"sample-statistic-tutorial.html","id":"bootstrapping","chapter":"38 Sample Statistic Tutorial","heading":"38.3.1 Bootstrapping","text":"trying find good approximation population parameters using sample statistics. samples give better approximation. can’t generate additional samples?Bootstrapping statistical method draws samples replacement within single dataset create many simulated samples.Bootstrapping procedure:\nDraw random sample size n replacement original sample data.\nRecord sample statistic. eg. mean, median, etc.\nRepeat steps 1 2 many times.\nPlot calculated stats forms bootstrap distribution.\nUsing bootstrap distribution sample statistic, can calculate x% bootstrap confidence intervals perform hypothesis testing.\nDraw random sample size n replacement original sample data.Record sample statistic. eg. mean, median, etc.Repeat steps 1 2 many times.Plot calculated stats forms bootstrap distribution.Using bootstrap distribution sample statistic, can calculate x% bootstrap confidence intervals perform hypothesis testing.Implementation R\npackage boot allows us Bootstrapping.\n\nlibrary(boot)\nlibrary(ggplot2)  #plotting\nCreate function computes statistic want use, mean, median, correlation, etc.\nNote: calling boot, need define function return desired statistic(s). first argument passed function dataset. second argument can index vector observations dataset frequency weight vector informs sampling probabilities. example, use default index vector , statistic interest mean example.\n\n\nx1 <- sample(1:199,100,replace=FALSE) #used example original sample data\nfc <- (function(d,) mean(d[]))\nUsing boot function find R bootstrap statistic.\n\nb1<-boot(x1,fc,R=1000)\nPlot generated bootstrap distribution. example, distribution bootstrap samples’ mean shown.\n\n# Plot bootstrap sampling\n# distribution using ggplot\nplot(b1)\nUsing boot.ci() function get confidence intervals using bootstrap samples.\nreturn x% confidence interval setting confidence level conf = x. , use 95 percent confidence interval.\nset type intervals \n“norm”: using Standard confidence limits methods compute CI standard deviation.\n“basic”: using Hall’s (second percentile) method calculate upper lower limit test statistic percentile.\n“stud”: resamples bootstrap sample find second-stage bootstrap statistic use calculate CI.\n“perc”: using Quantile-based intervals calculate CI.\n“bca”: Bias Corrected Accelerated use percentile limits bias correction estimate acceleration coefficient corrects limit find CI.\nsimply type = “” obtain five types intervals\n\n\n\nmean(x1) #output sample mean\n## [1] 98.86\n\nboot.ci(b1, conf = 0.95, type=c(\"norm\",\"basic\",\"perc\"))\n## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\n## Based 1000 bootstrap replicates\n## \n## CALL : \n## boot.ci(boot.= b1, conf = 0.95, type = c(\"norm\", \"basic\", \n##     \"perc\"))\n## \n## Intervals : \n## Level      Normal              Basic              Percentile     \n## 95%   ( 87.43, 110.15 )   ( 87.43, 110.47 )   ( 87.25, 110.29 )  \n## Calculations Intervals Original Scale\nexample, randomly sample 100 numbers without replacement (1,199) simulate random sample data. want use sample mean mean(x1) approximate represent population true mean, can use Bootstrapping generate samples test original sample statistic. , boot.ci() gives 95 percent confident population mean interval. sample mean mean(x1) interval, can’t say bad approximation.\npackage boot allows us Bootstrapping.Create function computes statistic want use, mean, median, correlation, etc.\nNote: calling boot, need define function return desired statistic(s). first argument passed function dataset. second argument can index vector observations dataset frequency weight vector informs sampling probabilities. example, use default index vector , statistic interest mean example.\nNote: calling boot, need define function return desired statistic(s). first argument passed function dataset. second argument can index vector observations dataset frequency weight vector informs sampling probabilities. example, use default index vector , statistic interest mean example.Using boot function find R bootstrap statistic.Plot generated bootstrap distribution. example, distribution bootstrap samples’ mean shown.Using boot.ci() function get confidence intervals using bootstrap samples.\nreturn x% confidence interval setting confidence level conf = x. , use 95 percent confidence interval.\nset type intervals \n“norm”: using Standard confidence limits methods compute CI standard deviation.\n“basic”: using Hall’s (second percentile) method calculate upper lower limit test statistic percentile.\n“stud”: resamples bootstrap sample find second-stage bootstrap statistic use calculate CI.\n“perc”: using Quantile-based intervals calculate CI.\n“bca”: Bias Corrected Accelerated use percentile limits bias correction estimate acceleration coefficient corrects limit find CI.\nsimply type = “” obtain five types intervals\n\nreturn x% confidence interval setting confidence level conf = x. , use 95 percent confidence interval.set type intervals \n“norm”: using Standard confidence limits methods compute CI standard deviation.\n“basic”: using Hall’s (second percentile) method calculate upper lower limit test statistic percentile.\n“stud”: resamples bootstrap sample find second-stage bootstrap statistic use calculate CI.\n“perc”: using Quantile-based intervals calculate CI.\n“bca”: Bias Corrected Accelerated use percentile limits bias correction estimate acceleration coefficient corrects limit find CI.\nsimply type = “” obtain five types intervals\n“norm”: using Standard confidence limits methods compute CI standard deviation.“basic”: using Hall’s (second percentile) method calculate upper lower limit test statistic percentile.“stud”: resamples bootstrap sample find second-stage bootstrap statistic use calculate CI.“perc”: using Quantile-based intervals calculate CI.“bca”: Bias Corrected Accelerated use percentile limits bias correction estimate acceleration coefficient corrects limit find CI.simply type = “” obtain five types intervalsAs example, randomly sample 100 numbers without replacement (1,199) simulate random sample data. want use sample mean mean(x1) approximate represent population true mean, can use Bootstrapping generate samples test original sample statistic. , boot.ci() gives 95 percent confident population mean interval. sample mean mean(x1) interval, can’t say bad approximation.","code":"\nlibrary(boot)\nlibrary(ggplot2)  #for plotting\nx1 <- sample(1:199,100,replace=FALSE) #used as an example of original sample data\nfc <- (function(d,i) mean(d[i]))\nb1<-boot(x1,fc,R=1000)\n# Plot the bootstrap sampling\n# distribution using ggplot\nplot(b1)\nmean(x1) #output the sample mean## [1] 98.86\nboot.ci(b1, conf = 0.95, type=c(\"norm\",\"basic\",\"perc\"))## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\n## Based on 1000 bootstrap replicates\n## \n## CALL : \n## boot.ci(boot.out = b1, conf = 0.95, type = c(\"norm\", \"basic\", \n##     \"perc\"))\n## \n## Intervals : \n## Level      Normal              Basic              Percentile     \n## 95%   ( 87.43, 110.15 )   ( 87.43, 110.47 )   ( 87.25, 110.29 )  \n## Calculations and Intervals on Original Scale"},{"path":"sample-statistic-tutorial.html","id":"hypothesis-testing-1","chapter":"38 Sample Statistic Tutorial","heading":"38.3.2 Hypothesis Testing","text":"Hypothesis testing statistical method used decide whether data sufficiently support particular hypothesis.evaluated two mutually exclusive statements population, method used access plausibility hypothesis using sample data.Keywords understand:\nNull Hypothesis(H0): statistics, null hypothesis general given statement statistic observed due random chance difference two measured cases among groups (e.g. true difference means equal 0). usually statement want prove wrong.\nAlternative Hypothesis(H1): alternative hypothesis hypothesis contrary null hypothesis, want prove right. (e.g. true difference means equal 0).\nLevel significance: refers degree significance accept reject null-hypothesis. value probability false rejection hypothesis test. usually select level significance 5%, means output 95% confident give similar predictions.\nP-value: P value, also known calculated probability, probability finding observed extreme results null hypothesis(H0) study given problem true. P value less chosen significance level, really small chance obtain observed value given null hypothesis true, reject null hypothesis.\nType error & Type II error: Type error (= p value): reject null hypothesis, although hypothesis true. Type II error: accept null hypothesis false.\nKeywords understand:Null Hypothesis(H0): statistics, null hypothesis general given statement statistic observed due random chance difference two measured cases among groups (e.g. true difference means equal 0). usually statement want prove wrong.Null Hypothesis(H0): statistics, null hypothesis general given statement statistic observed due random chance difference two measured cases among groups (e.g. true difference means equal 0). usually statement want prove wrong.Alternative Hypothesis(H1): alternative hypothesis hypothesis contrary null hypothesis, want prove right. (e.g. true difference means equal 0).Alternative Hypothesis(H1): alternative hypothesis hypothesis contrary null hypothesis, want prove right. (e.g. true difference means equal 0).Level significance: refers degree significance accept reject null-hypothesis. value probability false rejection hypothesis test. usually select level significance 5%, means output 95% confident give similar predictions.Level significance: refers degree significance accept reject null-hypothesis. value probability false rejection hypothesis test. usually select level significance 5%, means output 95% confident give similar predictions.P-value: P value, also known calculated probability, probability finding observed extreme results null hypothesis(H0) study given problem true. P value less chosen significance level, really small chance obtain observed value given null hypothesis true, reject null hypothesis.P-value: P value, also known calculated probability, probability finding observed extreme results null hypothesis(H0) study given problem true. P value less chosen significance level, really small chance obtain observed value given null hypothesis true, reject null hypothesis.Type error & Type II error: Type error (= p value): reject null hypothesis, although hypothesis true. Type II error: accept null hypothesis false.Type error & Type II error: Type error (= p value): reject null hypothesis, although hypothesis true. Type II error: accept null hypothesis false.Procedure Hypothesis testing\nState null alternative hypothesis.\nFormulate analysis plan, set significance level.\nAnalyze sample data test statistic. Use value test statistic make decision based significance level.\nProcedure Hypothesis testingState null alternative hypothesis.Formulate analysis plan, set significance level.Analyze sample data test statistic. Use value test statistic make decision based significance level.Implementation R:Implementation R:T-test usually used relatively small-amount normally distributed data without known standard deviation. use t.test() T-tests R.\nOne sample T-test used test mean sample known mean value:\nexample, randomly sample 50 numbers normal distributed population true mean 10 simulate sample data. Running t test given null hypothesis “true mean 10” result large p-value, means reject null hypothesis expected. contrast, running t test given null hypothesis “true mean 6” result really small p-value, means really small chance obtain observed sample mean given null hypothesis true, reject null hypothesis.\n\nx <- rnorm(50, 10) # true mean 10\nmean(x)\n## [1] 10.20618\n\nt.test(x, mu=10) # setting H0: true mean equal 10\n## \n##  One Sample t-test\n## \n## data:  x\n## t = 1.8931, df = 49, p-value = 0.06426\n## alternative hypothesis: true mean equal 10\n## 95 percent confidence interval:\n##   9.987311 10.425043\n## sample estimates:\n## mean x \n##  10.20618\n\nt.test(x, mu=6) # setting H0: true mean equal 6\n## \n##  One Sample t-test\n## \n## data:  x\n## t = 38.62, df = 49, p-value < 2.2e-16\n## alternative hypothesis: true mean equal 6\n## 95 percent confidence interval:\n##   9.987311 10.425043\n## sample estimates:\n## mean x \n##  10.20618\nTwo sample T-test used compare two sample data indicates :\nexample, randomly sample 20 50 numbers normal distributed population true mean 5 two sets sample data. Running t test two samples sets null hypothesis “difference means 0” give large p-value, fails reject H0 expected. test used random samples different means, expect extremely small p value indicates difference statistically significant, case reject null hypothesis. samples size, can also set parameter pairs True perform paired test\n\nx <- rnorm(50, 5)\nx1 <- rnorm(20, 5)\ny <- rnorm(20, 5)\nt.test(x, y)\n## \n##  Welch Two Sample t-test\n## \n## data:  x y\n## t = 0.31774, df = 37.268, p-value = 0.7525\n## alternative hypothesis: true difference means equal 0\n## 95 percent confidence interval:\n##  -0.4656674  0.6389280\n## sample estimates:\n## mean x mean y \n##  5.023414  4.936783\n\nt.test(x1, y, paired=TRUE)\n## \n##  Paired t-test\n## \n## data:  x1 y\n## t = 0.60717, df = 19, p-value = 0.5509\n## alternative hypothesis: true mean difference equal 0\n## 95 percent confidence interval:\n##  -0.5656056  1.0278536\n## sample estimates:\n## mean difference \n##        0.231124One sample T-test used test mean sample known mean value:\nexample, randomly sample 50 numbers normal distributed population true mean 10 simulate sample data. Running t test given null hypothesis “true mean 10” result large p-value, means reject null hypothesis expected. contrast, running t test given null hypothesis “true mean 6” result really small p-value, means really small chance obtain observed sample mean given null hypothesis true, reject null hypothesis.Two sample T-test used compare two sample data indicates :\nexample, randomly sample 20 50 numbers normal distributed population true mean 5 two sets sample data. Running t test two samples sets null hypothesis “difference means 0” give large p-value, fails reject H0 expected. test used random samples different means, expect extremely small p value indicates difference statistically significant, case reject null hypothesis. samples size, can also set parameter pairs True perform paired testWhat normality assumption fails? ’ll use U-test, usually used relatively large sample data, comparison computed one sample data non-parametric. use wilcox.test() U-test R.\nOne-sample U-test takes “exact” parameter, logical indicating whether exact p-value computed.\nexample, randomly sample 100 numbers normal distributed population true mean 0. Hence, null hypothesis “true location mean 0”. test ’re expected get large p-value fails reject H0 expected. can also set mu=3, ’ll get extremely small p-value take alternative hypothesis: “true mean location 3”.\n\nx <- rnorm(100)\nwilcox.test(x, exact = FALSE)\n## \n##  Wilcoxon signed rank test continuity correction\n## \n## data:  x\n## V = 2434, p-value = 0.7557\n## alternative hypothesis: true location equal 0\n\nwilcox.test(x, mu = 3)\n## \n##  Wilcoxon signed rank test continuity correction\n## \n## data:  x\n## V = 0, p-value < 2.2e-16\n## alternative hypothesis: true location equal 3\nTwo-sample U-test used compare two sets sample data:\nexample, randomly sample 100 numbers normal distributed population different true mean 10 20 create two sample dataset. null hypothesis test “difference ture means 0”. test ’ll expect p-value < 2.2e-16 since mean different, reject H0. Alternatively, get large p-value data, reject null hypothesis.\n\nx <- rnorm(100, 10)\ny <- rnorm(100, 20)\nwilcox.test(x, y)\n## \n##  Wilcoxon rank sum test continuity correction\n## \n## data:  x y\n## W = 0, p-value < 2.2e-16\n## alternative hypothesis: true location shift equal 0One-sample U-test takes “exact” parameter, logical indicating whether exact p-value computed.\nexample, randomly sample 100 numbers normal distributed population true mean 0. Hence, null hypothesis “true location mean 0”. test ’re expected get large p-value fails reject H0 expected. can also set mu=3, ’ll get extremely small p-value take alternative hypothesis: “true mean location 3”.Two-sample U-test used compare two sets sample data:\nexample, randomly sample 100 numbers normal distributed population different true mean 10 20 create two sample dataset. null hypothesis test “difference ture means 0”. test ’ll expect p-value < 2.2e-16 since mean different, reject H0. Alternatively, get large p-value data, reject null hypothesis.","code":"\nx <- rnorm(50, 10) # true mean is 10\nmean(x)## [1] 10.20618\nt.test(x, mu=10) # setting H0: true mean is equal to 10## \n##  One Sample t-test\n## \n## data:  x\n## t = 1.8931, df = 49, p-value = 0.06426\n## alternative hypothesis: true mean is not equal to 10\n## 95 percent confidence interval:\n##   9.987311 10.425043\n## sample estimates:\n## mean of x \n##  10.20618\nt.test(x, mu=6) # setting H0: true mean is equal to 6## \n##  One Sample t-test\n## \n## data:  x\n## t = 38.62, df = 49, p-value < 2.2e-16\n## alternative hypothesis: true mean is not equal to 6\n## 95 percent confidence interval:\n##   9.987311 10.425043\n## sample estimates:\n## mean of x \n##  10.20618\nx <- rnorm(50, 5)\nx1 <- rnorm(20, 5)\ny <- rnorm(20, 5)\nt.test(x, y)## \n##  Welch Two Sample t-test\n## \n## data:  x and y\n## t = 0.31774, df = 37.268, p-value = 0.7525\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -0.4656674  0.6389280\n## sample estimates:\n## mean of x mean of y \n##  5.023414  4.936783\nt.test(x1, y, paired=TRUE)## \n##  Paired t-test\n## \n## data:  x1 and y\n## t = 0.60717, df = 19, p-value = 0.5509\n## alternative hypothesis: true mean difference is not equal to 0\n## 95 percent confidence interval:\n##  -0.5656056  1.0278536\n## sample estimates:\n## mean difference \n##        0.231124\nx <- rnorm(100)\nwilcox.test(x, exact = FALSE)## \n##  Wilcoxon signed rank test with continuity correction\n## \n## data:  x\n## V = 2434, p-value = 0.7557\n## alternative hypothesis: true location is not equal to 0\nwilcox.test(x, mu = 3)## \n##  Wilcoxon signed rank test with continuity correction\n## \n## data:  x\n## V = 0, p-value < 2.2e-16\n## alternative hypothesis: true location is not equal to 3\nx <- rnorm(100, 10)\ny <- rnorm(100, 20)\nwilcox.test(x, y)## \n##  Wilcoxon rank sum test with continuity correction\n## \n## data:  x and y\n## W = 0, p-value < 2.2e-16\n## alternative hypothesis: true location shift is not equal to 0"},{"path":"sample-statistic-tutorial.html","id":"reference-4","chapter":"38 Sample Statistic Tutorial","heading":"38.4 Reference","text":"https://www.geeksforgeeks.org/bootstrap-confidence-interval--r-programming/https://www.geeksforgeeks.org/bootstrap-confidence-interval--r-programming/https://www.datanovia.com/en/lessons/identify--remove-duplicate-data--r/https://www.datanovia.com/en/lessons/identify--remove-duplicate-data--r/https://www.geeksforgeeks.org/data-cleaning--r/https://www.geeksforgeeks.org/data-cleaning--r/https://www.geeksforgeeks.org/hypothesis-testing--r-programming/https://www.geeksforgeeks.org/hypothesis-testing--r-programming/https://www.geeksforgeeks.org/understanding-hypothesis-testing/https://www.geeksforgeeks.org/understanding-hypothesis-testing/","code":""},{"path":"data-cleaning-tutorial.html","id":"data-cleaning-tutorial","chapter":"39 Data cleaning tutorial","heading":"39 Data cleaning tutorial","text":"Saili Myana Sai Rithvik Kanakamedala","code":""},{"path":"data-cleaning-tutorial.html","id":"motivation-8","chapter":"39 Data cleaning tutorial","heading":"39.1 Motivation","text":"Data cleaning important preliminary processing step perform data visualize model . Data errors missing values lead erroneous observations. Making business decisions inaccurate observations may lead downfall company.\nexample, Hillier said. “’s like creating foundation building: right can build something strong long-lasting. wrong, building soon collapse.”\ncan’t perform visualization machine learning analysis without first cleaning dataset.tutorial, going perform step--step data cleaning real world dataset Zillow Research. data can downloaded going list sale prices section selecting Median List Price(Raw, Homes, Monthly) contains data monthly listing prices houses across United States.","code":""},{"path":"data-cleaning-tutorial.html","id":"observe-the-dataset","chapter":"39 Data cleaning tutorial","heading":"39.2 Observe the Dataset","text":"important first view dataset understand column means. example, analyzing median listing prices houses different cities different dates.","code":"\nlisting_price <- read.csv(url(\"https://files.zillowstatic.com/research/public_csvs/mlp/Metro_mlp_uc_sfrcondo_month.csv?t=1668571408\"))\nprint(dim(listing_price))## [1] 898  63\nhead(listing_price)[colnames(listing_price)[1:6]] # Showing only 6 columns, remaining are similar to the date column##   RegionID SizeRank      RegionName RegionType StateName X2018.01.31\n## 1   102001        0   United States    country                255000\n## 2   394913        1    New York, NY        msa        NY      489000\n## 3   753899        2 Los Angeles, CA        msa        CA      709000\n## 4   394463        3     Chicago, IL        msa        IL      269900\n## 5   394514        4      Dallas, TX        msa        TX      318000\n## 6   394692        5     Houston, TX        msa        TX      289500"},{"path":"data-cleaning-tutorial.html","id":"check-the-data-type-of-columns","chapter":"39 Data cleaning tutorial","heading":"39.3 Check the data type of columns","text":"data may contain Numeric, Character, Logical, Factor data types. need check columns raw data suitable data type. examples numerical data may stored character important convert numerical proceeding analyse data.column dataset check data type using str(dataset). show data type column along example values.","code":"\nstr(listing_price)## 'data.frame':    898 obs. of  63 variables:\n##  $ RegionID   : int  102001 394913 753899 394463 394514 394692 395209 394856 394974 394347 ...\n##  $ SizeRank   : int  0 1 2 3 4 5 6 7 8 9 ...\n##  $ RegionName : chr  \"United States\" \"New York, NY\" \"Los Angeles, CA\" \"Chicago, IL\" ...\n##  $ RegionType : chr  \"country\" \"msa\" \"msa\" \"msa\" ...\n##  $ StateName  : chr  \"\" \"NY\" \"CA\" \"IL\" ...\n##  $ X2018.01.31: num  255000 489000 709000 269900 318000 ...\n##  $ X2018.02.28: num  264900 505000 725000 289000 325000 ...\n##  $ X2018.03.31: num  269900 515000 730000 294900 325990 ...\n##  $ X2018.04.30: num  279000 519000 750000 299900 334500 ...\n##  $ X2018.05.31: num  280000 529900 750000 307000 334900 ...\n##  $ X2018.06.30: num  280000 530000 750000 299900 329000 ...\n##  $ X2018.07.31: num  279900 525000 749000 299000 324605 ...\n##  $ X2018.08.31: num  279000 524800 729900 294900 319000 ...\n##  $ X2018.09.30: num  279900 529000 729900 294450 319900 ...\n##  $ X2018.10.31: num  279500 529000 725000 289000 317998 ...\n##  $ X2018.11.30: num  279000 529000 719000 282000 315000 ...\n##  $ X2018.12.31: num  275000 525000 715000 279000 315000 ...\n##  $ X2019.01.31: num  275000 520000 699900 279000 310000 ...\n##  $ X2019.02.28: num  280000 529000 719900 294900 316000 ...\n##  $ X2019.03.31: num  289900 535000 729000 299900 324900 ...\n##  $ X2019.04.30: num  298000 549000 749000 309000 330000 ...\n##  $ X2019.05.31: num  299900 549000 750000 310000 330500 ...\n##  $ X2019.06.30: num  299900 549000 759800 309400 329900 ...\n##  $ X2019.07.31: num  299000 545000 759900 299995 325000 ...\n##  $ X2019.08.31: num  295000 539000 758000 299000 319999 ...\n##  $ X2019.09.30: num  295000 549000 759000 298000 319900 ...\n##  $ X2019.10.31: num  290000 549000 759000 289900 319900 ...\n##  $ X2019.11.30: num  289900 549000 769000 285000 319000 ...\n##  $ X2019.12.31: num  285000 549900 769000 279900 315000 ...\n##  $ X2020.01.31: num  284900 549000 765000 280000 310000 ...\n##  $ X2020.02.29: num  290000 549000 779894 295000 315900 ...\n##  $ X2020.03.31: num  299000 559000 795000 299900 319900 ...\n##  $ X2020.04.30: num  299900 559900 789000 299000 320000 ...\n##  $ X2020.05.31: num  305000 559000 799000 299900 325000 ...\n##  $ X2020.06.30: num  315000 565000 819000 305000 331900 ...\n##  $ X2020.07.31: num  319900 575000 830000 309000 334900 ...\n##  $ X2020.08.31: num  319000 584900 829000 309000 332900 ...\n##  $ X2020.09.30: num  319000 595000 829900 309900 335000 ...\n##  $ X2020.10.31: num  318900 599000 835000 300000 330000 ...\n##  $ X2020.11.30: num  310000 599900 849982 299900 329800 ...\n##  $ X2020.12.31: num  304990 599900 849000 294900 326000 ...\n##  $ X2021.01.31: num  300500 595000 849000 294900 329900 ...\n##  $ X2021.02.28: num  319900 599000 849900 309900 335000 ...\n##  $ X2021.03.31: num  325000 599000 850000 310000 343490 ...\n##  $ X2021.04.30: num  335000 599000 874900 310000 350000 ...\n##  $ X2021.05.31: num  342000 599000 878000 315000 360000 ...\n##  $ X2021.06.30: num  345000 599000 885000 315000 369000 ...\n##  $ X2021.07.31: num  342000 590000 869999 309900 374878 ...\n##  $ X2021.08.31: num  339900 579000 850000 299900 370000 ...\n##  $ X2021.09.30: num  342000 588000 850000 299900 375000 ...\n##  $ X2021.10.31: num  340000 589000 859000 299000 375000 ...\n##  $ X2021.11.30: num  335000 586000 850000 289999 378259 ...\n##  $ X2021.12.31: num  328000 595000 859000 279900 379900 ...\n##  $ X2022.01.31: num  332000 599000 889000 285000 385000 325000 450000 450000 275000 345000 ...\n##  $ X2022.02.28: num  350000 619000 899869 299000 397995 ...\n##  $ X2022.03.31: num  365000 629000 928944 309000 400000 ...\n##  $ X2022.04.30: num  379900 647639 949900 319900 420000 ...\n##  $ X2022.05.31: num  394900 649000 950000 325000 434900 ...\n##  $ X2022.06.30: num  399000 649000 950000 325000 440000 ...\n##  $ X2022.07.31: num  399000 629000 949000 321948 439900 ...\n##  $ X2022.08.31: num  390000 625000 925000 315000 425000 ...\n##  $ X2022.09.30: num  389900 629000 924900 315000 420000 ...\n##  $ X2022.10.31: num  385000 644950 900000 309900 410000 ..."},{"path":"data-cleaning-tutorial.html","id":"optional-step","chapter":"39 Data cleaning tutorial","heading":"39.4 Optional Step","text":"can also check columns named neatly. Using clean_names function, can rename columns consistent format.","code":"\nlisting_price <- clean_names(listing_price)\ncolnames(listing_price)##  [1] \"region_id\"   \"size_rank\"   \"region_name\" \"region_type\" \"state_name\" \n##  [6] \"x2018_01_31\" \"x2018_02_28\" \"x2018_03_31\" \"x2018_04_30\" \"x2018_05_31\"\n## [11] \"x2018_06_30\" \"x2018_07_31\" \"x2018_08_31\" \"x2018_09_30\" \"x2018_10_31\"\n## [16] \"x2018_11_30\" \"x2018_12_31\" \"x2019_01_31\" \"x2019_02_28\" \"x2019_03_31\"\n## [21] \"x2019_04_30\" \"x2019_05_31\" \"x2019_06_30\" \"x2019_07_31\" \"x2019_08_31\"\n## [26] \"x2019_09_30\" \"x2019_10_31\" \"x2019_11_30\" \"x2019_12_31\" \"x2020_01_31\"\n## [31] \"x2020_02_29\" \"x2020_03_31\" \"x2020_04_30\" \"x2020_05_31\" \"x2020_06_30\"\n## [36] \"x2020_07_31\" \"x2020_08_31\" \"x2020_09_30\" \"x2020_10_31\" \"x2020_11_30\"\n## [41] \"x2020_12_31\" \"x2021_01_31\" \"x2021_02_28\" \"x2021_03_31\" \"x2021_04_30\"\n## [46] \"x2021_05_31\" \"x2021_06_30\" \"x2021_07_31\" \"x2021_08_31\" \"x2021_09_30\"\n## [51] \"x2021_10_31\" \"x2021_11_30\" \"x2021_12_31\" \"x2022_01_31\" \"x2022_02_28\"\n## [56] \"x2022_03_31\" \"x2022_04_30\" \"x2022_05_31\" \"x2022_06_30\" \"x2022_07_31\"\n## [61] \"x2022_08_31\" \"x2022_09_30\" \"x2022_10_31\""},{"path":"data-cleaning-tutorial.html","id":"check-for-duplicate-data","chapter":"39 Data cleaning tutorial","heading":"39.5 Check for duplicate data","text":"Next, check duplicate rows dataset remove repeated rows using distinct function dplyr package.","code":"\nlisting_price <- distinct(listing_price)"},{"path":"data-cleaning-tutorial.html","id":"remove-empty-rows-and-columns","chapter":"39 Data cleaning tutorial","heading":"39.6 Remove empty rows and columns","text":"need remove empty rows columns dont contain useful information. can done using remove_empty function janitor package.","code":"\nlisting_price <- remove_empty(listing_price, which = c(\"rows\", \"cols\"), quiet = FALSE)"},{"path":"data-cleaning-tutorial.html","id":"converting-messy-data-into-tidy-data","chapter":"39 Data cleaning tutorial","heading":"39.7 Converting messy data into tidy data","text":"data currently messy. good practice, can convert data tidy format every column variable every every row observation.","code":"\ntidy_df <- listing_price %>% pivot_longer(cols=colnames(listing_price)[6:60], names_to=\"date\", values_to = \"listing_price\") \nhead(tidy_df)## # A tibble: 6 × 10\n##   region…¹ size_…² regio…³ regio…⁴ state…⁵ x2022…⁶ x2022…⁷ x2022…⁸ date  listi…⁹\n##      <int>   <int> <chr>   <chr>   <chr>     <dbl>   <dbl>   <dbl> <chr>   <dbl>\n## 1   102001       0 United… country \"\"       390000  389900  385000 x201…  255000\n## 2   102001       0 United… country \"\"       390000  389900  385000 x201…  264900\n## 3   102001       0 United… country \"\"       390000  389900  385000 x201…  269900\n## 4   102001       0 United… country \"\"       390000  389900  385000 x201…  279000\n## 5   102001       0 United… country \"\"       390000  389900  385000 x201…  280000\n## 6   102001       0 United… country \"\"       390000  389900  385000 x201…  280000\n## # … with abbreviated variable names ¹​region_id, ²​size_rank, ³​region_name,\n## #   ⁴​region_type, ⁵​state_name, ⁶​x2022_08_31, ⁷​x2022_09_30, ⁸​x2022_10_31,\n## #   ⁹​listing_price"},{"path":"data-cleaning-tutorial.html","id":"check-for-missing-values","chapter":"39 Data cleaning tutorial","heading":"39.8 Check for missing values","text":"check data contains missing values using .na() function. can find number missing values dataset calculating sum(.na(data)).","code":"\nsum(is.na(tidy_df))## [1] 1074"},{"path":"data-cleaning-tutorial.html","id":"handling-missing-values","chapter":"39 Data cleaning tutorial","heading":"39.9 Handling missing values","text":"multiple ways handle missing values.can omit rows contain missing values.can replace missing value previous value column.\n(may best way replace missing values case group rows given value column written )can replace missing value average column.can replace average values prior post missing value depending context.advanced methods predict missing values like Regression techniques, can used needed. Depending dataset, can select method appropriate.dataset, two appropriate ways dealing missing values.can remove cities missing values. remove 95 898 cities. can done omitting rows containing NA tidying data.can remove cities missing values. remove 95 898 cities. can done omitting rows containing NA tidying data.can remove dates particular city price listed. can done converting data tidy format omitting rows missing values. choosing way, omitting dates price unavailable good enough analysis.can remove dates particular city price listed. can done converting data tidy format omitting rows missing values. choosing way, omitting dates price unavailable good enough analysis.","code":"\ndata <- na.omit(data) # we can also use na.exclude(data)\nfor (i in colnames(data)){\ndata %>% fill(all_of(i), .direction = 'up')\n}\ndata <- na.aggregate(data) # Replace NA in all columns with column mean\ntidy_df <- na.omit(tidy_df)"},{"path":"data-cleaning-tutorial.html","id":"removing-outliers-optional","chapter":"39 Data cleaning tutorial","heading":"39.10 Removing Outliers (Optional)","text":"case outliers changing trends data, may want analyse removing outliers. can done finding Z scores shown threshold Z score filter outliers.dataset, required, aim visualize house prices .Now dataset ready analyzed. can also model data using Machine Learning models doesn’t contain missing values corrupted data.","code":"\n#For numeric columns of data only\ndf <- data[names(dplyr::select_if(data,is.numeric))]\nz_scores <- as.data.frame(sapply(df, function(df) (abs(df-mean(df))/sd(df))))\nhead(z_scores)\n\nno_outliers_data <- data[!rowSums(z_scores>3), ] #removing rows with z-scores greater than 3\ndim(no_outliers_data) #to check final dimensions of data"},{"path":"data-cleaning-tutorial.html","id":"conclusion-6","chapter":"39 Data cleaning tutorial","heading":"39.11 Conclusion","text":"tutorial, discovered different ways data can messy. thought wrong column names, wrong data types, deal data missing unwanted outliers. journey, learnt question anything unexpected data convert desired format. think data cleaning steps dataset encounter future, implement requisite steps.","code":""},{"path":"mlr3-package-tutorial.html","id":"mlr3-package-tutorial","chapter":"40 mlr3 Package Tutorial","heading":"40 mlr3 Package Tutorial","text":"Lingjun Zhang Michelle SunFor data scientists, topic machine learning indispensable matter school workplace. Usually, people use Python machine learning work. mlr3 package R help people machine learning tasks R. provide tutorial specific package.mlr3 efficient, object-oriented programming building blocks machine learning. Provides ‘R6’ objects tasks, learners, resamplings, measures. package geared towards scalability larger datasets supporting parallelization --memory data-backends like databases. ‘mlr3’ focuses core computational operations, add-packages provide additional functionality.rendered book readers read easily link:\nhttps://ellie-117.github.io/mlr3_tutorial/","code":""},{"path":"guide-to-nonparametric-tests.html","id":"guide-to-nonparametric-tests","chapter":"41 Guide to nonparametric tests","heading":"41 Guide to nonparametric tests","text":"Tseng-Han Yu","code":"\nlibrary(\"DescTools\")"},{"path":"guide-to-nonparametric-tests.html","id":"introduction-4","chapter":"41 Guide to nonparametric tests","heading":"41.1 Introduction","text":"guide common non-parametric tests focus problems sample size 2 one-way data. guide starts two-sample inference procedures designed detect observations (location) one population tend different another, including special case two-sample paired data well. appropriate tests compare variability (scale) observations also included. test, summary, required assumptions, possible hypotheses, implementation test conducted R.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"two-sample-inference-procedure-difference-in-location-permutation-test","chapter":"41 Guide to nonparametric tests","heading":"41.2 Two-Sample Inference Procedure (Difference in Location): Permutation Test","text":"","code":""},{"path":"guide-to-nonparametric-tests.html","id":"summary","chapter":"41 Guide to nonparametric tests","heading":"41.2.1 Summary","text":"Permutation Test permutes observations two samples assume difference two compute differences location parameter, mean. steps obtain p-value following:Pool m observations sample 1 n observations sample 2 together.Permute observations samples m units assigned sample 1 n units assigned sample 2. Compute difference means. Repeat step (m+n)!/m!n! times.p-value fraction permutations extreme extreme observed.Note number possible ways permute data : (m+n)!/m!n!. becomes time consuming generate possible permutations, can instead take random sample permutations (common choice 1000 permutations). Furthermore, permutation test difference means greater power light-tailed distributions.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"assumptions","chapter":"41 Guide to nonparametric tests","heading":"41.2.2 Assumptions","text":"data two randomly selected independent samplesBoth population distributions continuous (discrete/categorical)","code":""},{"path":"guide-to-nonparametric-tests.html","id":"hypotheses","chapter":"41 Guide to nonparametric tests","heading":"41.2.3 Hypotheses","text":"three possible sets hypotheses :\nHo: F_1(x) = F_2(x) vs. Ha: F_1(x) ≤ F_2(x)\nHo: F_1(x) = F_2(x) vs. Ha: F_1(x) ≥ F_2(x)\nHo: F_1(x) = F_2(x) vs. Ha: F_1(x) ≥ F_2(x) F_1(x) ≤ F_2(x) x strict inequality least one x\nnull hypothesis implies two distributions, population 1 population 2, identical. Note alternative hypothesis Ha: F_1(x) ≤ F_2(x), actually implies population 1 larger observed values population 2 F_1(x) F_2(x) distribution functions population 1 population 2 respectively.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"implementation","chapter":"41 Guide to nonparametric tests","heading":"41.2.4 Implementation","text":"","code":"\nsample1 <- c(38, 50, 56, 58)\nm <- length(sample1)\n\nsample2 <- c(24, 32, 39, 47)\nn <- length(sample2)\n\ndata <- c(sample1, sample2)\n# inputs the data from example\n\n# How many permutations of m assigned to sample 1 and n assigned to sample 2?\nchoose(m+n,m)## [1] 70\n# Find all permutations of data for sample 1: all unique groupings of m data points\nperm1 <- combn(data,m)\nperm1 <- t(perm1) #transpose the data so each permuted \"sample 1\" is a row, rather than a column\n\n# Find the \"paired\" n data observations that are not classified as sample 1, \n# and are thus grouped into sample 2, for each permutation\nperm2 <- NULL\nfor (i in 1:choose(m+n,m)){\n  perm2 <- rbind(perm2, setdiff(data, perm1[i,]))\n}\n\n# Calculate the difference in means between the groups\ndiffmeans <- rep(NA, choose(m+n,m))\nfor (i in 1:choose(m+n,m)){\n  diffmeans[i] <- mean(perm1[i,]) - mean(perm2[i,])\n}\n\n# Create a table with each permutation and its difference in means \n# (not necessary to run test, but interesting to look at)\ntable.perms <- cbind(perm1, perm2, diffmeans)\ntable.perms <- table.perms[order(-diffmeans),] #sorts from largest to smallest\nhead(table.perms)##                              diffmeans\n## [1,] 50 56 58 47 38 24 32 39      19.5\n## [2,] 50 56 58 39 38 24 32 47      15.5\n## [3,] 38 50 56 58 24 32 39 47      15.0\n## [4,] 56 58 39 47 38 50 24 32      14.0\n## [5,] 38 56 58 47 50 24 32 39      13.5\n## [6,] 50 56 58 32 38 24 39 47      12.0\n# Find the proportion of permutations that have a difference of means \n# greater than the observed difference of means\ndiffmeans.obs <- mean(sample1)-mean(sample2) # calculate observed difference in means\nsum(diffmeans >= diffmeans.obs) # this is the total number of differences greater than or equal to observed## [1] 3\nsum(diffmeans >= diffmeans.obs)/choose(m+n,m) # this is the proportion of differences greater than or equal to observed (p-value)## [1] 0.04285714"},{"path":"guide-to-nonparametric-tests.html","id":"two-sample-inference-procedure-difference-in-location-wilcoxon-rank-sum-test","chapter":"41 Guide to nonparametric tests","heading":"41.3 Two-Sample Inference Procedure (Difference in Location): Wilcoxon Rank-Sum Test","text":"","code":""},{"path":"guide-to-nonparametric-tests.html","id":"summary-1","chapter":"41 Guide to nonparametric tests","heading":"41.3.1 Summary","text":"Wilcoxon Rank-Sum Test version permutation test applied data ranks rather actual data values.\nsteps obtain p-value following:Pool m observations sample 1 n observations sample 2 together. Rank smallest (1) largest (m+n).Permute observations samples m units assigned sample 1 n units assigned sample 2. Compute sum ranks sample 1, W, test statistic test. Repeat step (m+n)!/m!n! times.p-value fraction rank sums extreme extreme observed.Note number possible ways permute data : (m+n)!/m!n!. becomes time consuming generate possible permutations, can instead take random sample permutations (common choice 1000 permutations). Furthermore, Wilcoxon Rank-Sum Test greater power heavy-tailed skewed distributions.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"assumptions-1","chapter":"41 Guide to nonparametric tests","heading":"41.3.2 Assumptions","text":"data two randomly selected independent samplesBoth population distributions continuous (discrete/categorical).two populations equal variance spread.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"hypotheses-1","chapter":"41 Guide to nonparametric tests","heading":"41.3.3 Hypotheses","text":"three possible sets hypotheses :\nHo: F_1(x) = F_2(x) vs. Ha: F_1(x) ≤ F_2(x)\nHo: F_1(x) = F_2(x) vs. Ha: F_1(x) ≥ F_2(x)\nHo: F_1(x) = F_2(x) vs. Ha: F_1(x) ≥ F_2(x) F_1(x) ≤ F_2(x) x strict inequality least one x\nnull hypothesis implies two distributions, population 1 population 2, identical. Note alternative hypothesis Ha: F_1(x) ≤ F_2(x), actually implies population 1 larger observed values population 2 F_1(x) F_2(x) distribution functions population 1 population 2 respectively.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"implementation-1","chapter":"41 Guide to nonparametric tests","heading":"41.3.4 Implementation","text":"","code":"\nsample1 <- c(38, 50, 56, 58)\nsample2 <- c(24, 32, 39, 47)\n\n### RANKING DATA ###\n\ndata<-c(sample1, sample2)\n\nranks<-rank(data, ties.method=\"average\")\n# The rank function returns the ranks in order of the original data.\n# You need to specify ties.method=\"average\" if there are ties.\n\nrbind(data, ranks)##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n## data    38   50   56   58   24   32   39   47\n## ranks    3    6    7    8    1    2    4    5\n# the output is just to visualize how the ranking works\n\n### WILCOXON RANK-SUM TEST ###\n\nwilcox.test(sample1, sample2, alternative=\"greater\")## \n##  Wilcoxon rank sum exact test\n## \n## data:  sample1 and sample2\n## W = 14, p-value = 0.05714\n## alternative hypothesis: true location shift is greater than 0\n# The options for alternative are \"less\", \"greater\", or \"two.sided\":\n# \"less\" refers to the lower tail test\n# \"greater\" refers to the upper tail test\n\n# The test statistic returned (W) is not the Wilcoxon Rank-Sum Test statistic, \n# but actually the Mann-Whitney Test statistic.\n\n# The p-value, however, is correct."},{"path":"guide-to-nonparametric-tests.html","id":"two-sample-inference-procedure-difference-in-location-mann-whiteney-test","chapter":"41 Guide to nonparametric tests","heading":"41.4 Two-Sample Inference Procedure (Difference in Location): Mann-Whiteney Test","text":"","code":""},{"path":"guide-to-nonparametric-tests.html","id":"summary-2","chapter":"41 Guide to nonparametric tests","heading":"41.4.1 Summary","text":"Mann-Whitney Test focuses pair observations: (x_i, y_j) x_i observations population 1 y_j observations population 2. Mann-Whitney test statistic number pairs x_i > y_j. steps calculate p_value following:Pool m observations sample 1 n observations sample 2 together.Permute observations samples m units assigned sample 1 n units assigned sample 2. Compute test statistic, U. Repeat step (m+n)!/m!n! times.p-value fraction permutations extreme extreme observed.Note number possible ways permute data : (m+n)!/m!n!. becomes time consuming generate possible permutations, can instead take random sample permutations (common choice 1000 permutations). Furthermore, proven Mann-Whitney test statistic linearly related, thus equivalent, Wilcoxon test statistic, meaning test statistics result p-value conclusion.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"assumptions-2","chapter":"41 Guide to nonparametric tests","heading":"41.4.2 Assumptions","text":"random sample selected population, two populations independent otherTwo populations assumed similar shapeBoth population distributions continuous (discrete/categorical)","code":""},{"path":"guide-to-nonparametric-tests.html","id":"hypotheses-2","chapter":"41 Guide to nonparametric tests","heading":"41.4.3 Hypotheses","text":"three possible sets hypotheses :\nHo: F_1(x) = F_2(x) vs. Ha: F_1(x) ≤ F_2(x)\nHo: F_1(x) = F_2(x) vs. Ha: F_1(x) ≥ F_2(x)\nHo: F_1(x) = F_2(x) vs. Ha: F_1(x) ≥ F_2(x) F_1(x) ≤ F_2(x) x strict inequality least one x\nnull hypothesis implies two distributions, population 1 population 2, identical. Note alternative hypothesis Ha: F_1(x) ≤ F_2(x), actually implies population 1 larger observed values population 2 F_1(x) F_2(x) distribution functions population 1 population 2 respectively.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"implementation-2","chapter":"41 Guide to nonparametric tests","heading":"41.4.4 Implementation","text":"equivalence Wilcoxon Rank-Sum Test, can use function R tests.","code":"\nsample1 <- c(38, 50, 56, 58)\nsample2 <- c(24, 32, 39, 47)\n\nwilcox.test(sample1, sample2, alternative=\"greater\")## \n##  Wilcoxon rank sum exact test\n## \n## data:  sample1 and sample2\n## W = 14, p-value = 0.05714\n## alternative hypothesis: true location shift is greater than 0\n# The options for alternative are \"less\", \"greater\", or \"two.sided\":\n# The test statistic returned by R is actually the Mann-Whitney Test’s U, not W for the Wilcoxon Rank-Sum Test."},{"path":"guide-to-nonparametric-tests.html","id":"two-sample-paired-data-inference-procedure-difference-in-location-wilcoxon-signed-rank-test","chapter":"41 Guide to nonparametric tests","heading":"41.5 Two-Sample Paired Data Inference Procedure (Difference in Location): Wilcoxon Signed-Rank Test","text":"","code":""},{"path":"guide-to-nonparametric-tests.html","id":"summary-3","chapter":"41 Guide to nonparametric tests","heading":"41.5.1 Summary","text":"Signed ranks method ranking matched pairs data accounting positive negative nature differences.\nMethod:Take absolute value difference rank smallest largest.Assign + – signs ranks based sign observed difference.Wilcoxon Signed-Rank Test modifies Wilcoxon Rank-Sum Test signed ranks. test statistic SR+, sum positive signed ranks. Wilcoxon Signed-Rank Test also permutation-based procedure. steps obtain p-value following:n pairs, obtain 2^n possible assignments plus minus signs ranks absolute differences.2^n permutations step 1, compute SR+, sum positive signed ranks.p-value fraction rank sums extreme extreme observedNote Wilcoxon Signed-Rank Test ideal test conduct differences skewed heavy-tailed.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"assumptions-3","chapter":"41 Guide to nonparametric tests","heading":"41.5.2 Assumptions","text":"Two samples need dependent observations cases, differences two samples independent distribution symmetric.population distributions continuous (discrete/categorical).dependent measurements least ordinal scale.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"hypotheses-3","chapter":"41 Guide to nonparametric tests","heading":"41.5.3 Hypotheses","text":"two possible sets hypotheses :\nHo: F(x) = 1 - F(-x) vs. Ha: F(x) ≤ 1 - F(-x)\nHo: F(x) = 1 - F(-x) vs. Ha: F(x) ≥ 1 - F(-x)\nnull hypothesis implies population distribution symmetric zero. Note first alternative hypothesis Ha: F(x) ≤ 1 - F(-x), indicates population distribution greater probability zero (differences predominantly positive). hand, second alternative hypothesis: Ha: F(x) ≥ 1 - F(-x) indicate population distribution greater probability zero (differences predominantly negative).","code":""},{"path":"guide-to-nonparametric-tests.html","id":"implementation-3","chapter":"41 Guide to nonparametric tests","heading":"41.5.4 Implementation","text":"","code":"\npre<-c(1180, 1210, 1300, 1080, 1120, 1240, 1360, 980)\npost<-c(1230, 1280, 1310, 1140, 1150, 1200, 1340, 1100)\ndiff<-post-pre\nwilcox.test(diff, alternative=\"greater\")## \n##  Wilcoxon signed rank exact test\n## \n## data:  diff\n## V = 30, p-value = 0.05469\n## alternative hypothesis: true location is greater than 0"},{"path":"guide-to-nonparametric-tests.html","id":"two-sample-paired-data-inference-procedure-difference-in-location-sign-test","chapter":"41 Guide to nonparametric tests","heading":"41.6 Two-Sample Paired Data Inference Procedure (Difference in Location): Sign Test","text":"","code":""},{"path":"guide-to-nonparametric-tests.html","id":"summary-4","chapter":"41 Guide to nonparametric tests","heading":"41.6.1 Summary","text":"Sign Test uses fact x = 0, null hypothesis Ho: F(x) = 1 - F(-x) F(0) = 1 - F(0) = 0.5 since total probability cdf function equal 1. Let SN+ denote number observations (differences) greater 0. case SN+ binomially distributed random variable :\n1. n differences sampled.\n2. use random sampling, n differences sampled independent.\n3. observation Di either larger zero (“success”) less zero (“failure”).\n4. Ho true (zero really median), probability success p equal 0.5.Thus, p-value can obtained :\n– Upper tail: P(SN+ ≥ SN+obs) -> 1 - pbinom(SN+obs – 1, n, 0.5)\n– Lower tail: P(SN+ ≤ SN+obs) -> pbinom(SN+obs, n, 0.5)Note Sign Test also ideal test conduct differences skewed heavy-tailed, generally low power small--moderate sample sizes.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"assumptions-4","chapter":"41 Guide to nonparametric tests","heading":"41.6.2 Assumptions","text":"Two samples need dependent observations cases, differences assumed independent.populations sampled independently.population distributions continuous (discrete/categorical).dependent measurements least ordinal scale.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"hypotheses-4","chapter":"41 Guide to nonparametric tests","heading":"41.6.3 Hypotheses","text":"two possible sets hypotheses :\nHo: F(x) = 1 - F(-x) vs. Ha: F(x) ≤ 1 - F(-x)\nHo: F(x) = 1 - F(-x) vs. Ha: F(x) ≥ 1 - F(-x)\nnull hypothesis implies population distribution symmetric zero. Note first alternative hypothesis Ha: F(x) ≤ 1 - F(-x), indicates population distribution greater probability zero (differences predominantly positive). hand, second alternative hypothesis: Ha: F(x) ≥ 1 - F(-x) indicate population distribution greater probability zero (differences predominantly negative).","code":""},{"path":"guide-to-nonparametric-tests.html","id":"implementation-4","chapter":"41 Guide to nonparametric tests","heading":"41.6.4 Implementation","text":"","code":"\npre<-c(1180, 1210, 1300, 1080, 1120, 1240, 1360, 980)\npost<-c(1230, 1280, 1310, 1140, 1150, 1200, 1340, 1100)\ndiff<-post-pre\nsigntest_stats = sum(diff>0)\n\n# Upper tail: P(SN+ ≥ SN+obs) -> pbinom(SN+obs – 1, n, 0.5)\n1 - pbinom(signtest_stats-1, 8, 0.5)## [1] 0.1445312"},{"path":"guide-to-nonparametric-tests.html","id":"two-sample-data-inference-procedure-difference-in-scale-siegel-tukey-test","chapter":"41 Guide to nonparametric tests","heading":"41.7 Two-Sample Data Inference Procedure (Difference in Scale): Siegel-Tukey Test","text":"","code":""},{"path":"guide-to-nonparametric-tests.html","id":"summary-5","chapter":"41 Guide to nonparametric tests","heading":"41.7.1 Summary","text":"Siegel-Tukey Test rank-based procedure detect differences scale. Siegel-Tukey test applies scoring procedure places lower scores extreme observations higher scores middle observations detect differences scale. steps Siegel-Tukey Test following:Sort observations smallest largest.Assign smallest observation rank 1, largest observation rank 2, second largest observation rank 3, second smallest observation rank 4, third smallest observation rank 5, .Apply Wilcoxon Rank-Sum Test “ranks.”Note easily perform test starting alternating pattern ranking observations largest instead smallest. may likely yield slightly different value Wilcoxon statistic, decision reached test generally , making test quite arbitrary. Furthermore, two populations different location parameters, use Siegel-Tukey Test trying detect differences scale two samples assumption violated. also powerful larger sample sizes distributions skewed, heavy-tailed distributions.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"assumptions-5","chapter":"41 Guide to nonparametric tests","heading":"41.7.2 Assumptions","text":"Two random (independent) samples generated.population distributions continuous.populations share location parameter.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"hypotheses-5","chapter":"41 Guide to nonparametric tests","heading":"41.7.3 Hypotheses","text":"three possible sets hypotheses :\nHo: 𝜎1 = 𝜎2 vs. Ha: 𝜎1 ≤ 𝜎2\nHo: 𝜎1 = 𝜎2 vs. Ha: 𝜎1 ≥ 𝜎2\nHo: 𝜎1 = 𝜎2 vs. Ha: 𝜎1 != 𝜎2Note case, 𝜎i general scale parameter, necessarily standard deviation.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"implementation-5","chapter":"41 Guide to nonparametric tests","heading":"41.7.4 Implementation","text":"","code":"\nx<-c(16.55, 15.36, 15.94, 16.43, 16.01)\ny<-c(16.05, 15.98, 16.10, 15.88, 15.91)\n\n# install.packages(\"DescTools\")\nSiegelTukeyTest(x, y, alternative=\"greater\")## \n##  Siegel-Tukey-test for equal variability\n## \n## data:  x and y\n## ST = 24, p-value = 0.2738\n## alternative hypothesis: true ratio of scales is greater than 1"},{"path":"guide-to-nonparametric-tests.html","id":"two-sample-data-inference-procedure-difference-in-scale-ansari-bradley-test","chapter":"41 Guide to nonparametric tests","heading":"41.8 Two-Sample Data Inference Procedure (Difference in Scale): Ansari-Bradley Test","text":"","code":""},{"path":"guide-to-nonparametric-tests.html","id":"summary-6","chapter":"41 Guide to nonparametric tests","heading":"41.8.1 Summary","text":"Ansari-Bradley Test overcomes ambiguity Siegel-Tukey Test averaging ranks obtained Siegel-Tukey ranking directions (starting largest starting smallest). However, now working average ranks, can longer use Wilcoxon Rank-Sum critical values find rejection region test. Instead, p-values must obtained permutations.Note two populations different location parameters, use Ansari-Bradley Test trying detect differences scale two samples assumption violated. also powerful larger sample sizes distributions skewed, heavy-tailed distributions.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"assumptions-6","chapter":"41 Guide to nonparametric tests","heading":"41.8.2 Assumptions","text":"Two random (independent) samples generated.population distributions continuous.populations share location parameter.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"hypotheses-6","chapter":"41 Guide to nonparametric tests","heading":"41.8.3 Hypotheses","text":"three possible sets hypotheses :\nHo: 𝜎1 = 𝜎2 vs. Ha: 𝜎1 < 𝜎2\nHo: 𝜎1 = 𝜎2 vs. Ha: 𝜎1 > 𝜎2\nHo: 𝜎1 = 𝜎2 vs. Ha: 𝜎1 != 𝜎2Note case, 𝜎i general scale parameter, necessarily standard deviation.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"implementation-6","chapter":"41 Guide to nonparametric tests","heading":"41.8.4 Implementation","text":"","code":"\nx<-c(16.55, 15.36, 15.94, 16.43, 16.01)\ny<-c(16.05, 15.98, 16.10, 15.88, 15.91)\nansari.test(x, y, alternative=\"greater\")## \n##  Ansari-Bradley test\n## \n## data:  x and y\n## AB = 13, p-value = 0.2698\n## alternative hypothesis: true ratio of scales is greater than 1\n# note that the test statistic is different, but the p-value is correct comparing to Siegel-Tukey Test "},{"path":"guide-to-nonparametric-tests.html","id":"two-sample-data-inference-procedure-difference-in-scale-test-on-deviance","chapter":"41 Guide to nonparametric tests","heading":"41.9 Two-Sample Data Inference Procedure (Difference in Scale): Test on Deviance","text":"","code":""},{"path":"guide-to-nonparametric-tests.html","id":"summary-7","chapter":"41 Guide to nonparametric tests","heading":"41.9.1 Summary","text":"deviance calculated subtracting location parameter observation, making observations samples come distributions located 0, difference terms scale. assess difference scale parameters two populations, test statistic ratio absolute mean differences (RMD), calculated using sum absolute value deviance divided number sample samples find ratio. Thus, large values RMD indicate population 1 (numerator) greater variability. Small values RMD indicate population 2 (denominator) greater variability. steps obtain p-value following:computing estimated deviance observationSolve estimated RMD statisticPool m observations sample 1 n observations sample 2 together. Permute observations samples m units assigned sample 1 n units assigned sample 2. Compute estimated RMD statistic. Repeat step (m+n)!/m!n! times.p-value fraction permutations extreme extreme observed.Note two populations different location parameters, use test deviance trying detect differences scale two samples assumption violated Siegel-Tukey Ansari-Bradley Tests. also powerful smaller sample sizes distributions symmetric, light-tailed distributions.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"assumptions-7","chapter":"41 Guide to nonparametric tests","heading":"41.9.2 Assumptions","text":"Two random (independent) samples generated.population distributions continuous.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"hypotheses-7","chapter":"41 Guide to nonparametric tests","heading":"41.9.3 Hypotheses","text":"three possible sets hypotheses :\nHo: 𝜎1 = 𝜎2 vs. Ha: 𝜎1 < 𝜎2\nHo: 𝜎1 = 𝜎2 vs. Ha: 𝜎1 > 𝜎2\nHo: 𝜎1 = 𝜎2 vs. Ha: 𝜎1 != 𝜎2Note case, 𝜎i general scale parameter, necessarily standard deviation.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"implementation-7","chapter":"41 Guide to nonparametric tests","heading":"41.9.4 Implementation","text":"","code":"\nx<-c(16.55, 15.36, 15.94, 16.43, 16.01)\ny<-c(16.05, 15.98, 16.10, 15.88, 15.91)\n# first, we need to combine the data for permuting\ndata<-c(x, y)\nm<-length(x)\nn<-length(y)\n\n# find all permutations of m observations into the \"sample 1\" group\nx.perm <- combn(data,m)\nx.perm <- t(x.perm) # want each permutation as a row in our x.perm matrix\n\n# find the corresponding \"sample 2\" observations for each permutation\ny.perm <- NULL\nfor (i in 1:choose(m+n, m)){\n  y.perm <- rbind(y.perm, setdiff(data, x.perm[i,]))\n}\n\n# calculate the RMD statistic for each permutation (each pair of rows in x.perm and y.perm)\nRMD <- rep(NA, choose(m+n, m))\nfor (i in 1:choose(m+n, m)){\n  RMD[i] <- mean(abs(x.perm[i,]-median(x.perm[i,])))/mean(abs(y.perm[i,]-median(y.perm[i,])))\n}\n\n# solve for the observed RMD statistic with the original data\nRMD.obs <- mean(abs(x-median(x)))/mean(abs(y-median(y)))\n\n# solve for the p-value\nsum(RMD >= RMD.obs) # use > because this is the upper tail test (would switch for different Ha)## [1] 20\nsum(RMD >= RMD.obs)/choose(m+n, m)## [1] 0.07936508"},{"path":"guide-to-nonparametric-tests.html","id":"motivation-9","chapter":"41 Guide to nonparametric tests","heading":"41.10 Motivation","text":"conducting data analysis, quite common limitations regarding data, small sample size unknown distribution. non-parametric tests come play. Non-parametric tests require fewer weaker assumptions comparing parametric tests. One common assumption associated parametric tests data normal distribution, quite hard meet dealing practical cases. Non-parametric tests can also applied wider range data types, ordinal nominal data, quite useful make inferences surveys. Nevertheless, realized online resources related conducting non-parametric tests R either limited really comprehensive even though method really beneficial times. existing online resources related conducting non-parametric tests R, found quite disorganized straightforward comes choosing test conduct. decided make guide hopefully make easier audience understand test conduct specific situations. project, lot different non-parametric tests applied problems dealing two samples one-way data, plan just focus . also trying help audience understand logistics behind test instead just blindly calling functions R. sometimes easier just call function directly, believe quite helpful actually understand tests obtain statistics. guide, include summary, required assumptions, possible hypotheses, implementation test.project, realized non-parametric tests actually quite similar . might existing function called conduct tests R, found statistics certain tests obtained just using basic functions R. regards implementations, decided use simpler data demonstration purpose hopefully make easier audience understand. Nevertheless, also understand benefits using existing data packages practical, might differently next time.","code":""},{"path":"guide-to-nonparametric-tests.html","id":"citations","chapter":"41 Guide to nonparametric tests","heading":"41.11 Citations","text":"[Nonparametric Statistics] (https://faculty.ksu.edu.sa/sites/default/files/nonparametric_statistics_a_step--step_approach.pdf)\n[Handbook Parametric Nonparametric Statistical Procedures] (https://fmipa.umri.ac.id/wp-content/uploads/2016/03/David_J._Sheskin_David_Sheskin_Handbook_of_ParaBookFi.org_.pdf)\n[wilcox.test: Wilcoxon Rank Sum Signed Rank Tests] (https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/wilcox.test)\n[Mann-Whitney U Test: Assumptions Example] (https://www.technologynetworks.com/informatics/articles/mann-whitney-u-test-assumptions--example-363425)\n[Package ‘DescTools’] (https://cran.r-project.org/web/packages/DescTools/DescTools.pdf)\n[ansari.test: Ansari-Bradley Test] (https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/ansari.test)\n[Nonparametric Tests] (https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_nonparametric/bs704_nonparametric_print.html)","code":""},{"path":"dplyr-pacakage-in-r.html","id":"dplyr-pacakage-in-r","chapter":"42 dplyr pacakage in R","heading":"42 dplyr pacakage in R","text":"Siyu ShenThe dplyr package mainly used cleaning sorting data. using dplyr package, can efficiently process data, important people data analysis. note, going introduce common used functions dplyr.32 observations 11 variables. meanings variables :mpg: Miles/(US) gallon,cyl: Number cylinders,disp: Displacement (cu..),hp: Gross horsepower,drat: Rear axle ratio,wt: Weight (1000 lbs),qsec: 1/4 mile time,vs: Engine (0 = V-shaped, 1 = straight),: Transmission (0 = automatic, 1 = manual),gear: Number forward gears,carb: Number carburetors.","code":"\n#load 'dplyr' package\nlibrary(dplyr)\n#load data 'mtcars' as example\ndata(mtcars)\n\n#show the structure of the data 'mtcars'\nstr(mtcars)## 'data.frame':    32 obs. of  11 variables:\n##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n##  $ disp: num  160 160 108 258 360 ...\n##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n##  $ qsec: num  16.5 17 18.6 19.4 17 ...\n##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ..."},{"path":"dplyr-pacakage-in-r.html","id":"pipe-operator","chapter":"42 dplyr pacakage in R","heading":"42.1 1. Pipe operator ‘%>%’","text":"important know pipe %>% operator start learn functions dplyr package. use %>% , multiple functions can wrapper together. can used function.","code":"\n# usage in filter() function\nfilter(data_frame, variable == value)\ndata_frame %>% filter(variable == value)\n\n# usage in mutate() function\nmutate(data_frame, expression(s))\ndata_frame %>% mutate(expression(s))"},{"path":"dplyr-pacakage-in-r.html","id":"filter-by-row-filter-function","chapter":"42 dplyr pacakage in R","heading":"42.2 2. Filter by row: filter() function","text":"can use filter() function filter subsets given logic, similar subset() function, example:","code":"\n#filter when mpg is greater than or equal to 22\nfilter(mtcars, mpg >= 22)##                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Datsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n## Merc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n## Merc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n## Fiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## Honda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## Toyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n## Fiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Porsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## Lotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n#filter when cyl=4 or gear=3\nfilter(mtcars, cyl == 4 | gear == 3)\n\n#filter when cyl=4 and gear=3\nfilter(mtcars, cyl == 4 & gear == 3)\n\n#note: when using AND operation, avoid using '&&' instead of '&'"},{"path":"dplyr-pacakage-in-r.html","id":"filter-by-column-select-function","chapter":"42 dplyr pacakage in R","heading":"42.3 3. Filter by column: select() function","text":"select() function selects subdatasets column names arguments. dplyr package provides special functions used conjunction select() function filter variables, including starts_with, ends_with, contains, matches, one_of, num_range, everything.","code":"\n#choose data 'iris' as example\ndata(iris)\n\niris = tbl_df(iris)\n\n#show the 'iris' data\nhead(iris)## # A tibble: 6 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n## 1          5.1         3.5          1.4         0.2 setosa \n## 2          4.9         3            1.4         0.2 setosa \n## 3          4.7         3.2          1.3         0.2 setosa \n## 4          4.6         3.1          1.5         0.2 setosa \n## 5          5           3.6          1.4         0.2 setosa \n## 6          5.4         3.9          1.7         0.4 setosa\n#select the columns that start with 'Petal'\nselect(iris, starts_with(\"Petal\"))## # A tibble: 150 × 2\n##    Petal.Length Petal.Width\n##           <dbl>       <dbl>\n##  1          1.4         0.2\n##  2          1.4         0.2\n##  3          1.3         0.2\n##  4          1.5         0.2\n##  5          1.4         0.2\n##  6          1.7         0.4\n##  7          1.4         0.3\n##  8          1.5         0.2\n##  9          1.4         0.2\n## 10          1.5         0.1\n## # … with 140 more rows\n#select the columns that not start with 'Petal'\nselect(iris, -starts_with(\"Petal\"))\nselect(iris, !starts_with(\"Petal\"))\n\n#select the columns that end with 'Petal'\nselect(iris, ends_with(\"Width\"))\n\n#select the columns that contain with 'etal'\nselect(iris, contains(\"etal\"))\n\n#select the columns that the variables name is with 't'\nselect(iris, matches(\".t.\"))\n\n#select the columns you want directly\nselect(iris, Petal.Length, Petal.Width)\n\n#select multi-columns by using colon\nselect(iris, Sepal.Length:Petal.Width)\n#when we cannot use the character vector filter, we need to use 'one_of()' function\nvars <- c(\"Petal.Length\", \"Petal.Width\")\nselect(iris, one_of(vars))\n#return all columns, generally used when adjusting the order of variables in a dataset\nselect(iris, everything())\n\n#return all columns, but reorder the columns and put 'Species' column in the front\nselect(iris, Species, everything())"},{"path":"dplyr-pacakage-in-r.html","id":"mutate-and-transmute","chapter":"42 dplyr pacakage in R","heading":"42.4 4. mutate() and transmute()","text":"mutate() adds new variables preserves existing ones; transmute() adds new variables drops existing ones.4.1. mutate()4.1.1. New columns4.1.2. Delete columns4.1.3. Application Window function4.2. transmute()return value contain original dataset variables, variables calculation transformation retained.","code":"\n#create two more columns named 'cyl2' and 'cyl4'\nhead(mtcars %>% mutate(cyl2 = cyl * 2, cyl4 = cyl2 * 2))##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb cyl2 cyl4\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4   12   24\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4   12   24\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1    8   16\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1   12   24\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2   16   32\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1   12   24\n#delete the column 'mpg' and update the column 'disp'\nmtcars %>% mutate(mpg = NULL, disp = disp * 0.0163871)\n\n#delete the column 'cyl'\nmtcars %>% mutate(cyl = NULL)\n#create a new column 'rank' using min_rank() function by group 'cyl'\nhead(mtcars %>% group_by(cyl) %>% mutate(rank = min_rank(desc(mpg))))## # A tibble: 6 × 12\n## # Groups:   cyl [3]\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb  rank\n##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <int>\n## 1  21       6   160   110  3.9   2.62  16.5     0     1     4     4     2\n## 2  21       6   160   110  3.9   2.88  17.0     0     1     4     4     2\n## 3  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1     8\n## 4  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1     1\n## 5  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2     2\n## 6  18.1     6   225   105  2.76  3.46  20.2     1     0     3     1     6\n#create a new column 'mpg_max' using max() function by group 'cyl'\nhead(mtcars %>% group_by(cyl) %>% mutate(mpg_max = max(mpg)))## # A tibble: 6 × 12\n## # Groups:   cyl [3]\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb mpg_max\n##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>\n## 1  21       6   160   110  3.9   2.62  16.5     0     1     4     4    21.4\n## 2  21       6   160   110  3.9   2.88  17.0     0     1     4     4    21.4\n## 3  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1    33.9\n## 4  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1    21.4\n## 5  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2    19.2\n## 6  18.1     6   225   105  2.76  3.46  20.2     1     0     3     1    21.4\nhead(mtcars %>% mutate(wt_log=log(wt)))##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb    wt_log\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 0.9631743\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 1.0560527\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 0.8415672\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 1.1678274\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 1.2354715\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 1.2412686\nhead(mtcars %>% transmute(wt_log=log(wt)))##                      wt_log\n## Mazda RX4         0.9631743\n## Mazda RX4 Wag     1.0560527\n## Datsun 710        0.8415672\n## Hornet 4 Drive    1.1678274\n## Hornet Sportabout 1.2354715\n## Valiant           1.2412686\nhead(mtcars %>% mutate(displ_l = disp / 61.0237))##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb  displ_l\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 2.621932\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 2.621932\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 1.769804\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 4.227866\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 5.899347\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 3.687092\nhead(mtcars %>% transmute(displ_l = disp / 61.0237))##                    displ_l\n## Mazda RX4         2.621932\n## Mazda RX4 Wag     2.621932\n## Datsun 710        1.769804\n## Hornet 4 Drive    4.227866\n## Hornet Sportabout 5.899347\n## Valiant           3.687092"},{"path":"dplyr-pacakage-in-r.html","id":"ranking-function","chapter":"42 dplyr pacakage in R","heading":"42.5 5. Ranking function","text":"row_number: results parallel rankings different order, elements appearing first ranked first.min_rank: results tie ranks , next rank occupied.dense_rank: tied ranking occupy ranking, example: matter many tied second place, subsequent ranking still third placepercent_rank: rank percentagecume_dist: rank cumulative distribution intervalntile: roughly ranks vectors dividing n buckets. Larger buckets lower rank.","code":"\nx = c(5, 1, 3, 2, 2, NA)\n\nrow_number(x)## [1]  5  1  4  2  3 NA\nmin_rank(x)## [1]  5  1  4  2  2 NA\ndense_rank(x)## [1]  4  1  3  2  2 NA\npercent_rank(x)## [1] 1.00 0.00 0.75 0.25 0.25   NA\ncume_dist(x)## [1] 1.0 0.2 0.8 0.6 0.6  NA\nntile(x, 2)## [1]  2  1  2  1  1 NA\nhead(mtcars%>%mutate(dense_rank=cume_dist(cyl)))##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb dense_rank\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4    0.56250\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4    0.56250\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1    0.34375\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1    0.56250\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2    1.00000\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1    0.56250"},{"path":"dplyr-pacakage-in-r.html","id":"sort-function-arrange","chapter":"42 dplyr pacakage in R","heading":"42.6 6. Sort function: arrange()","text":"Note difference sorting ranking.arrange() sorts rows sequentially given column names.","code":"\n#sorted by column 'mpg'\nhead(arrange(mtcars, mpg))##                      mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Cadillac Fleetwood  10.4   8  472 205 2.93 5.250 17.98  0  0    3    4\n## Lincoln Continental 10.4   8  460 215 3.00 5.424 17.82  0  0    3    4\n## Camaro Z28          13.3   8  350 245 3.73 3.840 15.41  0  0    3    4\n## Duster 360          14.3   8  360 245 3.21 3.570 15.84  0  0    3    4\n## Chrysler Imperial   14.7   8  440 230 3.23 5.345 17.42  0  0    3    4\n## Maserati Bora       15.0   8  301 335 3.54 3.570 14.60  0  1    5    8\n#sorted by column 'mpg' and 'disp'\nhead(arrange(mtcars, mpg, disp))##                      mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Lincoln Continental 10.4   8  460 215 3.00 5.424 17.82  0  0    3    4\n## Cadillac Fleetwood  10.4   8  472 205 2.93 5.250 17.98  0  0    3    4\n## Camaro Z28          13.3   8  350 245 3.73 3.840 15.41  0  0    3    4\n## Duster 360          14.3   8  360 245 3.21 3.570 15.84  0  0    3    4\n## Chrysler Imperial   14.7   8  440 230 3.23 5.345 17.42  0  0    3    4\n## Maserati Bora       15.0   8  301 335 3.54 3.570 14.60  0  1    5    8\n#reverse order by using desc()\nhead(arrange(mtcars, desc(mpg)))##                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Toyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n## Fiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## Honda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## Lotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Fiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Porsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n#reverse order by using '-'\nhead(arrange(mtcars, -mpg))##                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Toyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n## Fiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## Honda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## Lotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Fiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Porsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2"},{"path":"dplyr-pacakage-in-r.html","id":"summary-function-summarise","chapter":"42 dplyr pacakage in R","heading":"42.7 7. Summary function: summarise()","text":"7.1. Summary whole7.2. Summary group","code":"\n#return the mean value in column 'disp'\nsummarise(mtcars, mean(disp))##   mean(disp)\n## 1   230.7219\n#return the standard deviation in column 'disp'\nsummarise(mtcars, sd(disp))##   sd(disp)\n## 1 123.9387\n#return the maximum and minimum value in column 'disp'\nsummarise(mtcars, max(disp), min(disp))##   max(disp) min(disp)\n## 1       472      71.1\n#return the number of columns in dataset 'mtcars'\nsummarise(mtcars, n())##   n()\n## 1  32\n#return the number of unique gear\nsummarise(mtcars, n_distinct(gear))##   n_distinct(gear)\n## 1                3\n#return the first value in column 'disp'\nsummarise(mtcars, first(disp))##   first(disp)\n## 1         160\n#return the last value in column 'disp'\nsummarise(mtcars, last(disp))##   last(disp)\n## 1        121\n#return the mean value and standard deviation of column 'disp' by grouping column 'cyl'\nmtcars %>% group_by(cyl) %>%\nsummarise(mean = mean(disp), sd = sd(disp))## # A tibble: 3 × 3\n##     cyl  mean    sd\n##   <dbl> <dbl> <dbl>\n## 1     4  105.  26.9\n## 2     6  183.  41.6\n## 3     8  353.  67.8"},{"path":"how-not-to-visualize-american-elections.html","id":"how-not-to-visualize-american-elections","chapter":"43 How (not) to visualize American elections","heading":"43 How (not) to visualize American elections","text":"Daniel de CastroEvery two years since 1789, American political media faced problem visually convey numbers describe election. effective statewide election visualizations make use best practices inform audiences true number, distribution, effect votes cast, today’s common visualization technique employs size color confuse even intentionally mislead audience balance power. guide dos don’ts visualizing American electoral system.","code":""},{"path":"time-series-visualization-with-r.html","id":"time-series-visualization-with-r","chapter":"44 Time series visualization with R","heading":"44 Time series visualization with R","text":"Runnan Jiang","code":"\nlibrary(tidyverse)\n#library(ggplot2)\n#library(dplyr)\nlibrary(openintro)\nlibrary(plotrix)\nlibrary(zoo)\nlibrary(gcookbook)\nlibrary(xts)\nlibrary(dygraphs)"},{"path":"time-series-visualization-with-r.html","id":"introduction-5","chapter":"44 Time series visualization with R","heading":"44.0.1 1. Introduction","text":"time series sequence data points occur successive order period time. Time series important many industries. tutorial, collect organize useful methods create customize time series visualizations R. hope tutorial help us visualize time series data effectively efficiently future. main packages used tidyverse dygraphs. ggplot2 already offered great features comes visualize time series: date can recognized automatically result neat X axis labels; scale_x_data() makes easy customize labels. Besides, packages including dygraphs,plotly can create interactive plots time series.","code":""},{"path":"time-series-visualization-with-r.html","id":"line-plots-1","chapter":"44 Time series visualization with R","heading":"44.0.2 2 Line Plots","text":"section, use economics dataset ggplot2 package. dataset produced US economic time series data available Federal Reserve Bank St. Louis. contains information personal consumption expenditures, total population, personal savings rate unemployment respect year month.Line plots commonly used plot time series data.can use scale_x_date() control x-axis breaks, limits, labels; use scale_y_continuous() control y-axis breaks, limits, labels; use geom_vline() annotate() mark specific events time series.Sometimes multiple indicators change along time incomparable. time, data drawn coordinate system. Generally, multiple slices can drawn aligned according time axis. facet_wrap() can used faceting.can use rollmean() Zoo package compute rolling means.","code":"\neconomics %>% glimpse()## Rows: 574\n## Columns: 6\n## $ date     <date> 1967-07-01, 1967-08-01, 1967-09-01, 1967-10-01, 1967-11-01, …\n## $ pce      <dbl> 506.7, 509.8, 515.6, 512.2, 517.4, 525.1, 530.9, 533.6, 544.3…\n## $ pop      <dbl> 198712, 198911, 199113, 199311, 199498, 199657, 199808, 19992…\n## $ psavert  <dbl> 12.6, 12.6, 11.9, 12.9, 12.8, 11.8, 11.7, 12.3, 11.7, 12.3, 1…\n## $ uempmed  <dbl> 4.5, 4.7, 4.6, 4.9, 4.7, 4.8, 5.1, 4.5, 4.1, 4.6, 4.4, 4.4, 4…\n## $ unemploy <dbl> 2944, 2945, 2958, 3143, 3066, 3018, 2878, 3001, 2877, 2709, 2…\neconomics %>%\n  ggplot(aes(date,psavert)) +\n  geom_line(color=\"#0099CC\") +\n  theme_bw()\neconomics %>%\n  ggplot(aes(x=date,y=psavert)) +\n  geom_line(color=\"#0099CC\") +\n  scale_x_date(date_breaks = \"5 years\", date_labels = \"%Y-%m\") +\n  scale_y_continuous(breaks = seq(2,20,2)) +\n  geom_vline(xintercept=as.Date(\"2007-12-01\"), color=\"#FF6666\") +\n  annotate(\"text\", x=as.Date(\"2009-01-01\"), y=12, label=\"Global Recession\", angle=90, size=3, color=\"#FF6666\") +\n  ggtitle(\"Personal Savings Rate Time Series\") +\n  theme_bw()\neconomics_facet <- economics %>%\n  pivot_longer(c(pce, psavert),\n               names_to = \"index\", \n               values_to = \"value\")\neconomics_facet %>%\n  ggplot(aes(date,value)) + \n  geom_line(color=\"#0099CC\") + \n  facet_wrap(~index, ncol = 1, scales = \"free_y\") +\n  theme_bw()\neconomics_rolling <- economics %>%\n  mutate(roll_mean = rollmean(economics$psavert,k=12,align=\"right\",fill = NA))\n\neconomics_rolling <- gather(economics_rolling, key=Metric, value = psavert, \n                            c(\"psavert\",\"roll_mean\"))\n\nggplot(economics_rolling) +\n  geom_line(aes(x=date,y=psavert,group=Metric,color=Metric)) +\n  theme_bw()"},{"path":"time-series-visualization-with-r.html","id":"bar-plots","chapter":"44 Time series visualization with R","heading":"44.0.2.1 3 Bar Plots","text":"can also use barplot visualize time series data.","code":"\nggplot(economics) +\n  geom_bar(aes(x = date, y = psavert, fill = pop), stat = 'identity') +\n  labs(title = \"Personal Savings Rate and Total Population Time Series\") +\n  theme_bw()\neconomics.grouped <-\n  economics %>%\n  mutate(year=format(date,\"%Y\")) %>%\n  group_by(year) %>%\n  summarise(mean_pop_by_year=mean(pce))\n\neconomics.grouped <-\n  economics.grouped %>%\n  filter(year > '2000')\n\nggplot(economics.grouped) + \n  geom_bar(aes(x = year, y = mean_pop_by_year), stat = 'identity', fill=\"#0099CC\",color='black',alpha=0.6) +\n  labs(title = \"Total Population Yearly Average\") +\n  theme_bw()"},{"path":"time-series-visualization-with-r.html","id":"area-plots","chapter":"44 Time series visualization with R","heading":"44.0.3 4. Area Plots","text":"","code":""},{"path":"time-series-visualization-with-r.html","id":"areas-under-a-single-time-series","chapter":"44 Time series visualization with R","heading":"44.0.3.1 4.1 Areas Under a Single Time Series","text":"","code":"\nggplot(economics, aes(x = date, y = psavert)) +\n  geom_area(fill=\"#0099CC\",color='black',alpha=0.6) +\n  labs(title = \"Personal Savings Rate\",\n       x = \"Date\",\n       y = \"Personal Savings Rate\") +\n  scale_x_date(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0)) +\n  theme_bw()"},{"path":"time-series-visualization-with-r.html","id":"stacked-polygons","chapter":"44 Time series visualization with R","heading":"44.0.3.2 4.2 Stacked Polygons","text":"stacked area chart can used show differences groups time. section, use uspopage dataset gcookbook package. plot age distribution US population 1900 2002.create stacked polygons (area plots), use function stackpoly() plotrix package.define appropriate order stack .variable percentage sum year always equal hundred, use proportional stacked area graph visualize data.","code":"\ndata(uspopage, package = \"gcookbook\")\nuspopage %>% glimpse()## Rows: 824\n## Columns: 3\n## $ Year      <int> 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1901, 1901, …\n## $ AgeGroup  <fct> <5, 5-14, 15-24, 25-34, 35-44, 45-54, 55-64, >64, <5, 5-14, …\n## $ Thousands <int> 9181, 16966, 14951, 12161, 9273, 6437, 4026, 3099, 9336, 171…\nggplot(uspopage, aes(x = Year,\n                     y = Thousands, \n                     fill = AgeGroup)) +\n  geom_area(alpha=0.6 , size=.5, colour=\"white\") +\n  ggtitle(\"US Population by age\") +\n  theme_bw()\n# Give a specific order\nuspopage$AgeGroup <- factor(uspopage$AgeGroup, \n                   levels=c(\">64\",\"55-64\",\"45-54\",\"35-44\",\"25-34\",\"15-24\",\"5-14\",\"<5\"))\n\nggplot(uspopage, aes(x=Year,y=Thousands,fill=AgeGroup)) + \n    geom_area(alpha=0.6 , size=.5, colour=\"white\") + \n    ggtitle(\"US Population by age\") +\n    theme_bw()\nuspopage_percentage <- uspopage  %>%\n  group_by(Year, AgeGroup) %>%\n  summarise(n = sum(Thousands)) %>%\n  mutate(percentage = n / sum(n))\n\nggplot(uspopage_percentage, aes(x=Year, y=percentage, fill=AgeGroup)) + \n    geom_area(alpha=0.6 , size=1, colour=\"white\") +\n    ggtitle(\"US Population Proportion by age\") +\n    theme_bw()"},{"path":"time-series-visualization-with-r.html","id":"interactive-time-series","chapter":"44 Time series visualization with R","heading":"44.0.4 5. Interactive Time Series","text":"dygraphs package visualizing time series data. dygraphs, can easily implement zooming, hovering, minimaps much visualizations.","code":"\nstr(economics)## spc_tbl_ [574 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n##  $ date    : Date[1:574], format: \"1967-07-01\" \"1967-08-01\" ...\n##  $ pce     : num [1:574] 507 510 516 512 517 ...\n##  $ pop     : num [1:574] 198712 198911 199113 199311 199498 ...\n##  $ psavert : num [1:574] 12.6 12.6 11.9 12.9 12.8 11.8 11.7 12.3 11.7 12.3 ...\n##  $ uempmed : num [1:574] 4.5 4.7 4.6 4.9 4.7 4.8 5.1 4.5 4.1 4.6 ...\n##  $ unemploy: num [1:574] 2944 2945 2958 3143 3066 ...\ndon <- xts(x = economics$psavert, order.by = economics$date)\np <- dygraphs::dygraph(don) %>%\n  dyOptions(colors=\"#0099CC\")\np\np <- dygraph(don) %>%\n  dyOptions(labelsUTC = TRUE, fillGraph=TRUE, fillAlpha=0.1, drawGrid = FALSE,colors = \"#0099CC\") %>%\n  dyRangeSelector() %>%\n  dyCrosshair(direction = \"vertical\") %>%\n  dyHighlight(highlightCircleSize = 5, highlightSeriesBackgroundAlpha = 0.2, hideOnMouseOut = FALSE)  %>%\n  dyRoller(rollPeriod = 1)\np"},{"path":"time-series-visualization-with-r.html","id":"conclusion-7","chapter":"44 Time series visualization with R","heading":"44.0.5 6. Conclusion","text":"tutorial, collect organize useful methods create customize time series visualizations R. hope tutorial help us visualize time series data effectively efficiently future. main packages used tidyverse dygraphs. ggplot2 already offered great features comes visualize time series: date can recognized automatically result neat X axis labels; scale_x_data() makes easy customize labels. Besides, packages including dygraphs,plotly can create interactive plots time series.","code":""},{"path":"time-series-visualization-with-r.html","id":"reference-5","chapter":"44 Time series visualization with R","heading":"44.0.6 7. Reference","text":"R documentation: R documentationdygraphs package: dygraphs Rsearching useful R packages: R Graph Gallery","code":""},{"path":"causal-inference-tutorial-a-quick-glance-at-interventional-treatment-and-simpsons-paradox.html","id":"causal-inference-tutorial-a-quick-glance-at-interventional-treatment-and-simpsons-paradox","chapter":"45 Causal Inference Tutorial: A Quick Glance at Interventional Treatment and Simpson’s Paradox","heading":"45 Causal Inference Tutorial: A Quick Glance at Interventional Treatment and Simpson’s Paradox","text":"Yizhan Niu","code":"\n# remotes::install_github(\"allisonhorst/palmerpenguins\")\nlibrary(palmerpenguins)\n# remotes::install_github(\"hrbrmstr/hrbrthemes\")\nlibrary(hrbrthemes)\n\nlibrary(gridExtra)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)"},{"path":"causal-inference-tutorial-a-quick-glance-at-interventional-treatment-and-simpsons-paradox.html","id":"motivation-10","chapter":"45 Causal Inference Tutorial: A Quick Glance at Interventional Treatment and Simpson’s Paradox","heading":"45.1 Motivation","text":"Causal inference recognized data science world. One primary reason data scientists like analyze estimate causal effect event (Y) another event (X), mere statistics can hardly capable satisfying need find causal effects. short tutorial aims giving readers quick glance applications causal inference interpreting analyzing data. chose one tool causal inference called intervention treatment famous problem, Simpson’s Paradox, show causal inference can applied practice solve problems using R.","code":""},{"path":"causal-inference-tutorial-a-quick-glance-at-interventional-treatment-and-simpsons-paradox.html","id":"intro-to-causal-graphs","chapter":"45 Causal Inference Tutorial: A Quick Glance at Interventional Treatment and Simpson’s Paradox","heading":"45.2 Intro to Causal Graphs","text":"represent causal effects pairs different observed variables, need mathematical language describe causal relationships. case, graph theory becomes best candidate. Typically, directed acyclic graph (DAG) choice representing causality among multiple variables. famous example shown following:example famous Bayesian Network DAG representing causal relationships. describes relationships among season (X1), whether rain fall (X2), whether sprinkler turned (X3), whether floor wet (X4), whether floor slippery (X5).context causal inference, causal graph like induce probability distribution, .e., joint distribution observed variables, call observations. One goals use observations determine causal effects one variable . causal effects can accomplished intervening variable (X) like test, elaborated .","code":""},{"path":"causal-inference-tutorial-a-quick-glance-at-interventional-treatment-and-simpsons-paradox.html","id":"intro-to-interventional","chapter":"45 Causal Inference Tutorial: A Quick Glance at Interventional Treatment and Simpson’s Paradox","heading":"45.3 Intro to Interventional","text":"may hear time time sentence “correlation causation.” increase ice cream consumption (X), shark attacks (Y) also rise. seems correlation, hard imagine causal effects eating ice cream shark attacks.One explanation hot weather (Z) causes people eat ice cream also causes people go beaches, increasing possibility attacked sharks. , hot weather common cause ice cream consumption shark attacks, intuitively causal relationships two variables curious .eliminate effects common cause, one thing can intervene people’s ice cream consumption. can choose group people consume ice cream observe whether shark attack rate rises period, leaving variable . one example intervention, deliberately intervening one variable see variable behave. Even though intervention realistic time, techniques back-door adjustment -calculus using non-experimental data, elaborate .show examples connects causal diagrams intervention operation.","code":""},{"path":"causal-inference-tutorial-a-quick-glance-at-interventional-treatment-and-simpsons-paradox.html","id":"first-causal-graph-example","chapter":"45 Causal Inference Tutorial: A Quick Glance at Interventional Treatment and Simpson’s Paradox","heading":"45.3.1 First Causal Graph Example","text":"Graph1This causal graph represents ice cream shark attack case, Z hot weather, X ice cream consumption, Y shark attack rate.can easily find variables highly correlated . like test causal effects intervening people’s ice cream consumption.","code":"\nset.seed(1)\n\nn <- 1000\nz <- rnorm(n, 0, 1)\nx <- z + rnorm(n, 0, 0.1)\ny <- z  + rnorm(n, 0, 0.1)\n\ndata <- cbind(x,y,z)\n\ncorr_xy <- cor(x, y, method = \"pearson\")\ncorr_xz <- cor(x, z, method = \"pearson\")\ncorr_yz <- cor(y, z, method = \"pearson\")\n\npar(mfrow = c(1, 3))\nscatter.smooth(x,y)\nscatter.smooth(z,y)\nscatter.smooth(z,x)"},{"path":"causal-inference-tutorial-a-quick-glance-at-interventional-treatment-and-simpsons-paradox.html","id":"first-causal-graph-after-intervention","chapter":"45 Causal Inference Tutorial: A Quick Glance at Interventional Treatment and Simpson’s Paradox","heading":"45.3.2 First Causal Graph After Intervention","text":"Graph1 InterventionThe right hand side shows graph intervention. (Z effects X, already people eat fixed number ice creams.)can see two graphs, people eating 2022 ice creams period affect distribution likelihood attacked sharks. Thus, done real world, can conclude ice cream consumption correlations shark attacks causation event.","code":"\n# Simulate data from the SCM where do(X = x)\nintervene_x <- function(x, n = 1000) {\n  z <- rnorm(n, 0, 1)\n  y <- z  + rnorm(n, 0, 0.1)\n  cbind(x, y, z)\n}\n\nset.seed(1)\nx_do1 <- intervene_x(x=2022)\n\npar(mfrow = c(1, 3))\nhist(\n  y, xlab = 'Y', breaks = 30,\n  xlim = c(-6, 6), col = 'gray76', main = 'P(Y)'\n)\n\nhist(\n  x_do1[, 2], xlab = 'Y', breaks = 20,\n  xlim = c(-6, 6), col = 'gray76', main = 'P(Y | do(X = 2022))'\n)"},{"path":"causal-inference-tutorial-a-quick-glance-at-interventional-treatment-and-simpsons-paradox.html","id":"second-causal-graph-example","chapter":"45 Causal Inference Tutorial: A Quick Glance at Interventional Treatment and Simpson’s Paradox","heading":"45.3.3 Second Causal Graph Example","text":"still example ice cream consumption shark attacks, imagine world ice cream can make people smell delicious, thus causes sharks attack people.Graph2We can find arrow X Y now.Still, like test causal effects intervening people’s ice cream consumption, causes following graph.","code":"\nset.seed(1)\n\nn <- 1000\nz <- rnorm(n, 0, 1)\nx <- z + rnorm(n, 0, 0.1)\ny <- x + z  + rnorm(n, 0, 0.1)\n\ndata <- cbind(x,y,z)\n\ncorr_xy <- cor(x, y, method = \"pearson\")\ncorr_xz <- cor(x, z, method = \"pearson\")\ncorr_yz <- cor(y, z, method = \"pearson\")\n\npar(mfrow = c(1, 3))\nscatter.smooth(x,y)\nscatter.smooth(z,y)\nscatter.smooth(z,x)"},{"path":"causal-inference-tutorial-a-quick-glance-at-interventional-treatment-and-simpsons-paradox.html","id":"second-causal-graph-after-intervention","chapter":"45 Causal Inference Tutorial: A Quick Glance at Interventional Treatment and Simpson’s Paradox","heading":"45.3.4 Second Causal Graph After Intervention","text":"Graph2 InterventionNow, even though let people eat 2 ice creams, (X=2) , slightly previous consumption zero ice creams, probability distribution shifts right, sharks likely attack people consumer ice creams. case, eliminate causality ice cream consumption shark attack rate.","code":"\n# Simulate data from the SCM where do(X = x)\nintervene_x <- function(x, n = 1000) {\n  z <- rnorm(n, 0, 1)\n  y <- x + z  + rnorm(n, 0, 0.1)\n  cbind(x, y, z)\n}\n\nset.seed(1)\nx_do1 <- intervene_x(x=2)\n\npar(mfrow = c(1, 3))\nhist(\n  y, xlab = 'Y', breaks = 30,\n  xlim = c(-6, 6), col = 'gray76', main = 'P(Y)'\n)\n\nhist(\n  x_do1[, 2], xlab = 'Y', breaks = 20,\n  xlim = c(-6, 6), col = 'gray76', main = 'P(Y | do(X = 2))'\n)"},{"path":"causal-inference-tutorial-a-quick-glance-at-interventional-treatment-and-simpsons-paradox.html","id":"intro-to-simpsons-paradox","chapter":"45 Causal Inference Tutorial: A Quick Glance at Interventional Treatment and Simpson’s Paradox","heading":"45.4 Intro to Simpson’s Paradox","text":"Now, like shift mind another famous topic statistics: Simpson’s Paradox. Basically, Simpson’s Paradox describes situation data shows positive (negative) correlations, correlation flips negative (positive) separate data based another variable. use Palmer Penguins dataset show occurrence Simpson’s Paradox use causal inference interpret phenomenon.","code":""},{"path":"causal-inference-tutorial-a-quick-glance-at-interventional-treatment-and-simpsons-paradox.html","id":"example-of-palmer-penguins-dataset","chapter":"45 Causal Inference Tutorial: A Quick Glance at Interventional Treatment and Simpson’s Paradox","heading":"45.4.1 Example of Palmer Penguins Dataset","text":"Cute PenguinsThe dataset collected published Dr. Kristen Gorman Palmer Station, Antarctica LTER, originally species research purposes. dataset different features penguin row: species, island, bill_length_mm, bill_depth_mm, flipper_length_, body_mass_g, sex, . case, look bill lengths (X), bill depths (Y), species (Z), Simpson’s Paradox found among three variables.brief overview dataset looks like.","code":"\nset.seed(9450)\n\npenguin_df<- \n  palmerpenguins::penguins %>%\n  na.omit()\n\n\nDT::datatable(head(penguin_df))"},{"path":"causal-inference-tutorial-a-quick-glance-at-interventional-treatment-and-simpsons-paradox.html","id":"relationship-between-bill-length-x-and-bill-depth-y","chapter":"45 Causal Inference Tutorial: A Quick Glance at Interventional Treatment and Simpson’s Paradox","heading":"45.4.2 Relationship Between Bill Length (X) And Bill Depth (Y)","text":"First, taking species consideration, like see correlation bill lengths bill depths penguins whole dataset.found clear negative correlation shown red line.","code":"\n#typeof(as.numeric(df[\"bill_length_mm\"][[1]]))\n\nlin_reg <- lm(bill_depth_mm ~ bill_length_mm, data=penguin_df)\n\npenguin_df %>%\n  ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +\n  geom_point() +\n  geom_abline(slope = lin_reg$coefficients[[2]],\n              intercept = lin_reg$coefficients[[1]], \n              color=\"red\") +\n  labs(x=\"Length\", y=\"Depth\",\n       title=\"Regression of Depth as a function of Length\") +\n  theme_classic()"},{"path":"causal-inference-tutorial-a-quick-glance-at-interventional-treatment-and-simpsons-paradox.html","id":"relationship-between-bill-length-x-and-bill-depth-y-separated-by-species","chapter":"45 Causal Inference Tutorial: A Quick Glance at Interventional Treatment and Simpson’s Paradox","heading":"45.4.3 Relationship Between Bill Length (X) And Bill Depth (Y) Separated By Species","text":"Now, separated data three species, Adelie, Chinstrap, Gentoo, find something different.Surprisingly, negative correlation aggregated data flipped separated data species. shows case Simpson’s Paradox .following graphs plotted separately based different species.","code":"\nchin<-\n  penguin_df %>%\n  filter(species == \"Chinstrap\")\nadelie<-\n  penguin_df %>%\n  filter(species == \"Adelie\")\ngentoo<-\n  penguin_df %>%\n  filter(species == \"Gentoo\")\n\nlm_chin<- lm(data=chin, bill_depth_mm ~ bill_length_mm)\nlm_adelie<- lm(data=adelie, bill_depth_mm ~ bill_length_mm)\nlm_gentoo<- lm(data=gentoo, bill_depth_mm ~ bill_length_mm)\n\npenguin_df %>%\n  ggplot(aes(x=bill_length_mm, y=bill_depth_mm, \n             color=species)) +\n  geom_point() +\n  geom_abline(slope = lm_chin$coefficients[[2]],\n              intercept = lm_chin$coefficients[[1]], \n              color=\"black\") +\n  geom_abline(slope = lm_adelie$coefficients[[2]],\n              intercept = lm_adelie$coefficients[[1]], \n              color=\"black\") +\n  geom_abline(slope = lm_gentoo$coefficients[[2]],\n              intercept = lm_gentoo$coefficients[[1]], \n              color=\"black\") +\n  labs(x=\"Length\", y=\"Depth\",\n       title=\"Regression of Depth as a function of Length\") +\n  theme_classic()\n# Given the same Z, there exists some positive correlation to X and Y.\n\nlin_reg_adelie <- lm(bill_depth_mm ~ bill_length_mm, data=adelie)\nstd_adelie <- sd(adelie$bill_depth_mm)\n\nadelie_plot <- adelie %>%\n  ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +\n  geom_point(color=\"red\") +\n  geom_abline(slope = lin_reg_adelie$coefficients[[2]],\n              intercept = lin_reg_adelie$coefficients[[1]], \n              color=\"red\") +\n  labs(x=\"Length\", y=\"Depth\",\n       title=\"Regression of Depth as a function of Length (Adelie)\") +\n  theme_classic()\n\nlin_reg_chin <- lm(bill_depth_mm ~ bill_length_mm, data=chin)\nstd_chin <- sd(chin$bill_depth_mm)\n\nchin_plot <- chin %>%\n  ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +\n  geom_point(color=\"green\") +\n  geom_abline(slope = lin_reg_chin$coefficients[[2]],\n              intercept = lin_reg_chin$coefficients[[1]], \n              color=\"red\") +\n  labs(x=\"Length\", y=\"Depth\",\n       title=\"Regression of Depth as a function of Length (Chin)\") +\n  theme_classic()\n\nlin_reg_gentoo <- lm(bill_depth_mm ~ bill_length_mm, data=gentoo)\nstd_gentoo <- sd(gentoo$bill_depth_mm)\n\ngentoo_plot <- gentoo %>%\n  ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +\n  geom_point(color=\"blue\") +\n  geom_abline(slope = lin_reg_gentoo$coefficients[[2]],\n              intercept = lin_reg_gentoo$coefficients[[1]], \n              color=\"red\") +\n  labs(x=\"Length\", y=\"Depth\",\n       title=\"Regression of Depth as a function of Length (Gentoo)\") +\n  theme_classic()\n\ngrid.arrange(adelie_plot, chin_plot, gentoo_plot, nrow=3, ncol = 1,widths = 1,heights=unit(c(1.5,1.5,1.5),c(\"in\",\"in\",\"in\")))"},{"path":"causal-inference-tutorial-a-quick-glance-at-interventional-treatment-and-simpsons-paradox.html","id":"take-a-closer-look-at-the-distribution-given-x-41-and-x-42.","chapter":"45 Causal Inference Tutorial: A Quick Glance at Interventional Treatment and Simpson’s Paradox","heading":"45.4.4 Take a closer look at the distribution given X = 41 and X = 42.","text":"Now, better understand patterns data, now take closer look distribution bill depths, given certain bill length. choose bill lengths 41 42 millimeters.X0: bill_length_mm 40 41.9\nX1: bill_length_mm 42 43.9With two histograms, can somehow guess Simpson’s Paradox occurs based data distribution.Given certain species Gentoo, data balanced two graphs. imbalance data within one specie may directly lead occurrence Simpson’s paradox. Even though lower bill length 41 millimeters leads higher average bill depth penguins, penguins 41-mm group Adelie, deeper bills penguins, penguins 42-mm group Gentoo, shallower bills.Thus, imbalanced data among different species makes data aggregation less reliable taking species type account.","code":"\na = 40\nb = 41.9\n\nc = 42\nd = 43.9\n\nadelie_x0  <- adelie %>%\n  filter(between(adelie$bill_length_mm,a,b))\n\nchin_x0  <- chin %>%\n  filter(between(chin$bill_length_mm,a,b))\n\ngentoo_x0  <- gentoo %>%\n  filter(between(gentoo$bill_length_mm,a,b))\n\nadelie_x1  <- adelie %>%\n  filter(between(adelie$bill_length_mm,c,d))\n\nchin_x1  <- chin %>%\n  filter(between(chin$bill_length_mm,c,d))\n\ngentoo_x1  <- gentoo %>%\n  filter(between(gentoo$bill_length_mm,c,d))\n\ndata_x0 <- data.frame(\n  type = c( rep(\"Adelie_x0\", count(adelie_x0)), rep(\"Chin_x0\", count(chin_x0)),rep(\"Gentoo_x0\", count(gentoo_x0))),\n  value = c( adelie_x0$bill_depth_mm, chin_x0$bill_depth_mm , gentoo_x0$bill_depth_mm)\n)\n\ndata_x1 <- data.frame(\n  type = c( rep(\"Adelie_x1\", count(adelie_x1)), rep(\"Chin_x1\", count(chin_x1)),rep(\"Gentoo_x1\", count(gentoo_x1))),\n  value = c( adelie_x1$bill_depth_mm, chin_x1$bill_depth_mm , gentoo_x1$bill_depth_mm)\n)\n\n\n\np1 <- data_x0 %>%\n  ggplot( aes(x=value, fill=type)) +\n    geom_histogram( color=\"#e9ecef\", alpha=0.6, position = 'identity') +\n    scale_fill_manual(values=c(\"#69b3a2\", \"#404080\",\"#add61d\")) +\n    theme_ipsum() +\n    scale_x_continuous(limits = c(13, 22))+\n    geom_vline(aes(xintercept = mean(value)),col='red',size=2)+\n    labs(fill=\"\")\n\np2 <- data_x1 %>%\n  ggplot( aes(x=value, fill=type)) +\n    geom_histogram( color=\"#e9ecef\", alpha=0.6, position = 'identity') +\n    scale_fill_manual(values=c(\"#69b3a2\", \"#404080\",\"#add61d\")) +\n    theme_ipsum() +\n    scale_x_continuous(limits = c(13, 22))+\n    geom_vline(aes(xintercept = mean(value)),col='red',size=2)+\n    labs(fill=\"\")\n\n\n\ngrid.arrange(p1, p2, ncol = 1)"},{"path":"causal-inference-tutorial-a-quick-glance-at-interventional-treatment-and-simpsons-paradox.html","id":"connecting-palmer-penguins-example-with-causal-intervention","chapter":"45 Causal Inference Tutorial: A Quick Glance at Interventional Treatment and Simpson’s Paradox","heading":"45.4.5 Connecting Palmer Penguins Example With Causal Intervention","text":"Now necessary think intervention operation just learned .assuming causal graph follows X <- Z -> Y, causal effects X Y.can use back-door adjustment calculate (X=X0) avoid necessity directly intervene penguin’s bills, elaborated tutorial.now need find way represent data generating function formulate results intervention operation. Based linear regression, conducted back-door adjustment equation , can write data generating function. Also, assume exists random noise penguin’s bill lengths depths, error follows normal distribution.Causal Graph AssumptionThere three deductions can make based graphs:(1). plot histogram bill depths penguins based original distribution, distribution intervening bill length 44, distribution intervening bill length 60, find result varies intervening bill length, .e., (X=x). implies causal relationship exists bill length bill depth penguins.(2). Looking blue line graphs, know Y positive correlation X rather initially thought. discovery also proved assumption variable species determines penguins’ bill lengths bill depths. (Species (Z) confounder!)(3). Finally, can return original causal graph assumption. found correlation bill length bill depth even intervention, can say causal relationship bill length bill depth, represented direct path. corrected graph shown :Causal Graph Inferred","code":"\nn <- dim(penguin_df)[1]\n\nprior_adelie <- sum(penguin_df$species==\"Adelie\")/n\n\nprior_chin <- sum(penguin_df$species==\"Chinstrap\")/n\n\nprior_gentoo <- sum(penguin_df$species==\"Gentoo\")/n\n\n\n\nintervene_x <- function(x, n = 300) {\n    y <- (lin_reg_adelie$coefficients[2]*x+lin_reg_adelie$coefficients[1] + rnorm(n, 0, std_adelie))*prior_adelie+(lin_reg_chin$coefficients[2]*x+lin_reg_chin$coefficients[1] + rnorm(n, 0, std_chin))*prior_chin+(lin_reg_gentoo$coefficients[2]*x+lin_reg_gentoo$coefficients[1] + rnorm(n, 0, std_gentoo))*prior_gentoo\n    cbind(x, y)\n}\n\nset.seed(1)\nx_do1 <- intervene_x(x=mean(penguin_df$bill_length_mm))\nx_do2 <- intervene_x(x=60)\n\npar(mfrow = c(1, 3))\nhist(\n  penguin_df$bill_depth_mm, xlab = 'Y', breaks = 20,\n  xlim = c(10, 25), col = 'gray76', main = 'P(Y)',\n  prob = TRUE\n)\nlines(density(penguin_df$bill_depth_mm), col=\"red\", lwd=2) # add a density estimate with defaults\nabline(v=mean(penguin_df$bill_depth_mm),col=\"blue\",lwd=2)\n\n\nhist(\n  x_do1[, 2], xlab = 'Y', breaks = 20,\n  xlim = c(10, 25), col = 'gray76', main = 'P(Y | do(X = 44))',\n  prob = TRUE\n)\nlines(density(x_do1[,2]), col=\"red\", lwd=2)\nabline(v=mean(x_do1[,2]),col=\"blue\",lwd=2)\n\n\nhist(\n  x_do2[, 2], xlab = 'Y', breaks = 20,\n  xlim = c(10, 25), col = 'gray76', main = 'P(Y | do(X = 60))',\n  prob = TRUE\n)\nlines(density(x_do2[,2]), col=\"red\", lwd=2)\nabline(v=mean(x_do2[,2]),col=\"blue\",lwd=2)"},{"path":"causal-inference-tutorial-a-quick-glance-at-interventional-treatment-and-simpsons-paradox.html","id":"final-sentences","chapter":"45 Causal Inference Tutorial: A Quick Glance at Interventional Treatment and Simpson’s Paradox","heading":"45.5 Final Sentences:","text":"causal inference increasingly recognized industry academia data science. valuable meaningful learn causal inference techniques broaden skill sets.Even though still taking course, like share knowledge just learned. might also typos tutorial, concepts causality intervention causal diagrams interesting. like learn content, please read works Pearl. Books “Causality” “book ” really good materials learn causal inference.","code":""},{"path":"causal-inference-tutorial-a-quick-glance-at-interventional-treatment-and-simpsons-paradox.html","id":"reference-6","chapter":"45 Causal Inference Tutorial: A Quick Glance at Interventional Treatment and Simpson’s Paradox","heading":"45.6 Reference:","text":"introduction Causal inference: https://www.r-bloggers.com/2019/11/-introduction--causal-inference/introduction Causal inference: https://www.r-bloggers.com/2019/11/-introduction--causal-inference/Simpson’s Paradox RStudio: https://rpubs.com/shampjeff/blog_post_2Simpson’s Paradox RStudio: https://rpubs.com/shampjeff/blog_post_2Pearl, J., Glymour, M., & Jewell, N. P. (2019). Causal inference statistics: Primer. John Wiley & Sons.Pearl, J., Glymour, M., & Jewell, N. P. (2019). Causal inference statistics: Primer. John Wiley & Sons.Pearl, J. (2009). Causality. Cambridge University Press.Pearl, J. (2009). Causality. Cambridge University Press.Bareinboim, E., Correa, J. D., Ibeling, D., & Icard, T. (2022). Pearl’s hierarchy foundations causal inference. Probabilistic Causal Inference, 507–556. https://doi.org/10.1145/3501714.3501743Bareinboim, E., Correa, J. D., Ibeling, D., & Icard, T. (2022). Pearl’s hierarchy foundations causal inference. Probabilistic Causal Inference, 507–556. https://doi.org/10.1145/3501714.3501743Gorman KB, Williams TD, Fraser WR (2014). Ecological sexual dimorphism environmental variability within community Antarctic penguins (genus Pygoscelis). PLoS ONE 9(3):e90081. https://doi.org/10.1371/journal.pone.0090081Gorman KB, Williams TD, Fraser WR (2014). Ecological sexual dimorphism environmental variability within community Antarctic penguins (genus Pygoscelis). PLoS ONE 9(3):e90081. https://doi.org/10.1371/journal.pone.0090081palmerpenguins dataset: https://cran.r-project.org/web/packages/palmerpenguins/readme/README.htmlpalmerpenguins dataset: https://cran.r-project.org/web/packages/palmerpenguins/readme/README.html","code":""},{"path":"network-analysis-and-social-network-analysis.html","id":"network-analysis-and-social-network-analysis","chapter":"46 Network analysis and social network analysis","heading":"46 Network analysis and social network analysis","text":"Sam Fields","code":""},{"path":"network-analysis-and-social-network-analysis.html","id":"what-is-a-network","chapter":"46 Network analysis and social network analysis","heading":"46.1 What is a Network?","text":"Network analysis built upon concept network (also referred graph), constructed upon definition nodes (.e. circles) edges connect circles. undirected network, direction edges irrelevant, edge simply shows connection exists. However, directed network, edge source target, source node edge came target node edge connects .(source: http://omtlab.com/directed--undirected-graph/)","code":""},{"path":"network-analysis-and-social-network-analysis.html","id":"network-analysis-and-social-network-anlysis","chapter":"46 Network analysis and social network analysis","heading":"46.2 Network Analysis and Social Network Anlysis","text":"pretense analysis, common subset Network Analysis concept Social Network Analysis. framework, one might think nodes defined entities (people, organizations, social units interest, etc) edges nodes defined social connection entities. example, one might wish define social graph Twitter users. case, likely define directed graph nodes individual users edges directed edges depicting users follow.(source: https://blog.revolutionanalytics.com/2010/05/--map--twitter-social-network.html)point, might wondering: “Great! amazing, quantify things?”. Great Question! Well, easiest way introduce weighting graph. node edge allowed weight, numerical value . Twitter example, node weight might wish use number followers user . Furthermore, thinking might wish visualize node edge weighting, common approach scale size node thickness edge proportion respective weighting.(source: https://mathinsight.org/network_introduction)cool, right! now, might thinking: “categorize things?”. Well, glad asked. Like visualization approaches, can introduce color. node edge can assigned color either depict categorize gradient.(source: https://www.weblyzard.com/social-network-analysis/)","code":""},{"path":"network-analysis-and-social-network-analysis.html","id":"analyzing-a-network","chapter":"46 Network analysis and social network analysis","heading":"46.3 Analyzing a Network","text":"Ok, now properties network way, start analyzing ? Well, start , might help go quantifiable metrics network.interested positionality nodes network, centered fringe, might interested centrality network. Centrality, generally, calculates ranking nodes based central metric. example, degree centrality takes account number weighting edges attached node nodes edges higher weighting central network. Another approach question might calculating node’s density, density defined number edges node divided number possible edges whole network. , might think density normalized metric node connectivity since nodes density 0 1.interested uncovering community network, might interested method community detection. genre analysis, “community” defined subset nodes network aspect subset separates nodes network, subset nodes shares higher interrelated connectivity. One common approaches Louvain Algorithm. super high level, algorithm greedy algorithm seeks optimize modularity, measure relative density edges inside community comparison outside community. pretty complicated, wish understand detail, suggest go : https://en.wikipedia.org/wiki/Louvain_method. example network nodes colored result community detection workflow provided .(source: https://www.researchgate.net/figure/Social-network---Twitter-conversations---parliamentary-debate--banning_fig2_329210578","code":""},{"path":"network-analysis-in-r.html","id":"network-analysis-in-r","chapter":"47 Network Analysis in R","heading":"47 Network Analysis in R","text":"Luckily, hoping start Network Analysis, many libraries tutorials available assist . start, suggest using tidygraph R library. Tidygraph tidy framework built igraph framework. Meaning, rather working convoluted lists keep track nodes edges relationship using igraph library, tidygraph supports tabular framework, standard network programming environments, NetworkX python Gephi GUI interface. really succinct tutorial working tidygraph library can found : https://www.data-imaginist.com/2017/introducing-tidygraph/. tutorial empower ability define visualize network using tidygraph library.advantage using tidygraph library , built top igraph framework, also supports tidygraph igraph network analysis functions, although igraph functions supported tidygraph wrapper. Meaning analysis went earlier - centrality, density, community detection - many analytical methods natively supported can conducted “tidy” workflow! previous tutorial also sections perform network analysis tidygraph library, another strong tutorial task written Dr. David Garcia can found : https://dgarcia-eu.github.io/SocialDataScience/5_SocialNetworkPhenomena/057_Tidygraph2/tidygraph2.html. Following tutorial give strong understanding work tidygraph api derive metrics graph, also incorporate calculated metrics visualization graph well!Now ’re well equipped knowledge tools network analysis, hope enjoy networking!","code":""},{"path":"introduction-to-common-sampling-techniques-in-r.html","id":"introduction-to-common-sampling-techniques-in-r","chapter":"48 Introduction to common sampling techniques in r","heading":"48 Introduction to common sampling techniques in r","text":"Yuanyi Hu","code":"\n#load packages:\nlibrary(survey)\nlibrary(srvyr)\nlibrary(tidyverse)\nlibrary(sampling)"},{"path":"introduction-to-common-sampling-techniques-in-r.html","id":"motivation-11","chapter":"48 Introduction to common sampling techniques in r","heading":"48.0.1 Motivation","text":"semester, learned lot valuable techniques analyzing potential information behind large-scale data. Sometimes, entire population data accessible due cost data collection. ’s also quite efficient analyze key information finding proper sample estimate population. tutorial introduces conduct common sampling methods R different scenarios use .","code":""},{"path":"introduction-to-common-sampling-techniques-in-r.html","id":"simple-random-sampling","chapter":"48 Introduction to common sampling techniques in r","heading":"48.0.2 Simple random sampling","text":"Simple random sampling one popular frequently used methods research social survey. simple random sampling, every element population equal probability selected. package survey helpful realize method, package srvyr calculate summary statistics survey data. illustrate process, use api dataset survey. api dataset records information student performance California schools based academic performance index (’s ’s named api) computed standardized testing relevant information schools.example, use sample \\(600\\) schools, estimate total api99 (academic performance index 1999) entire population. words, process show precision using simple random sampling select approximately \\(10%\\) population data estimate target information. shown , apipop \\(6194\\) rows \\(37\\) columns. Since focusing api99, first check whether missing values (NA) column.Based check, NA api99. can first sum api99 population data, denoted total_api99. total_api99 equal \\(3914069\\). , can use function sample select sample \\(600\\) \\(6194\\). Since want use element , set replace=F, course can choose make SRS replacement situations.help histograms, can see sample reflects similar distribution entire population. ’s reasonable see slight difference since sample contains around \\(10\\%\\) data.\n, can summarize sample mean, variance, standard deviation, estimated total, standard error estimated total. Recall sample mean unbiased estimator population mean. Hence, calculating total api99 population using product population size sample mean still unbiased property expectation, \\(E[aX]=aE[X]\\). , standard error estimate can calculated \\(N\\sqrt{1-\\frac{n}{N}}\\frac{s}{\\sqrt{n} }\\), \\(N=6194\\) \\(n=600\\) case.estimated total api99 \\(3918335\\), relatively precise compared real value \\(3914069\\). way find estimated total using functions svydesign svytotal survey package, can see results . Also, can use confint find confidence interval estimate. default, calculate \\(95\\%\\) confidence interval, example, real total api99, \\(3914069\\), interval.","code":"\n#load data, we use the population data in it, apipop\ndata(api)\n#check number of rows and columns:\ndim(apipop)## [1] 6194   37\n#take a look at its situation of missing values in api99:\nsum(is.na(apipop$api99))## [1] 0\n#real sum of all api99 in population data:\napipop %>% summarise(total_api99 = sum(api99))##   total_api99\n## 1     3914069\n#if removed random seed, then we will get different results each time\n#here is just for reproducibility of results\nset.seed(100)\n#make a SRS with size of 600 without replacement\napi_sample = apipop %>% \n  slice(sort(sample(1:6194,size=600, replace=F)))\ndim(api_sample)## [1] 600  37\nggplot(api_sample,aes(x=api99)) + \n  geom_histogram(fill=\"darkseagreen3\",col=\"black\",bins = 20) \nggplot(apipop,aes(x=api99)) + \n  geom_histogram(fill=\"darkseagreen3\",col=\"black\", bins = 20)\napi_sample %>%\nsummarise(sample_mean=mean(api99),\nsample_variance = var(api99),\nsample_std = sd(api99),\nestimated_total_api99 = mean(api99)*6194,\nse_estimated_total = 6194*sqrt((1 - 600/6194))*sd(api99)/sqrt(600))##   sample_mean sample_variance sample_std estimated_total_api99\n## 1    632.6017         18159.5   134.7572               3918335\n##   se_estimated_total\n## 1           32383.45\napi_sample_d = svydesign(id=~1, data=api_sample, fpc=rep(6194,600))\n\nsvytotal(x=~api99,design=api_sample_d)##         total    SE\n## api99 3918335 32383\nconfint(svytotal(~api99,api_sample_d))##         2.5 %  97.5 %\n## api99 3854864 3981805"},{"path":"introduction-to-common-sampling-techniques-in-r.html","id":"stratified-sampling","chapter":"48 Introduction to common sampling techniques in r","heading":"48.0.3 Stratified sampling","text":"common sampling method stratified sampling. can use population can partitioned subgroups, subgroup distinct pattern. previous example, can see schools can classified three levels:labels represent high school, middle school, elementary school. Hence, can draw proportional number schools sample size level according size stratum entire population combine whole sample. sample similar data structure population data terms stype api99. , first calculate proportion level, find matched size stratum sample, get entire sample \\(600\\) schools.apply method, ’s important reorder data grouping level together., can apply stratified sampling without replacement using strata.can see variable named ID_unit. identifier selected units. can slice selected rows entire population.sample, can find estimated total api99 \\(95\\%\\) confidence interval using svydesign, svytotal, confint. can see estimated total api99 \\(3912987\\). ’s precise compared real value \\(3914069\\), makes closer estimate real value compared previous result using simple random sampling. Also, standard error estimate smaller previous. Hence, can see stratified sampling may helpful can divide population subgroups.","code":"\nunique(apipop$stype)## [1] H M E\n## Levels: E H M\n#we can see that there is no NA in the stype\nsum(is.na(apipop$stype))## [1] 0\n#calculate the proportion for each level\n(table = (apipop %>% count(stype) %>% \n            mutate(proportion=n/sum(n), \n                  stratum_sample_size = round(proportion*600, 0))))##   stype    n proportion stratum_sample_size\n## 1     E 4421  0.7137552                 428\n## 2     H  755  0.1218922                  73\n## 3     M 1018  0.1643526                  99\n#set the random seed for reproducibility\nset.seed(180)\n#reorder the population according to the labels:\napipop_reorder<-inner_join(apipop,table,by=\"stype\") %>% arrange(stype)\n#use the strata method from package sampling\napipop_sample <-strata(apipop_reorder,\n                       c(\"stype\"),size=c(428,73,99), method=\"srswor\")\ndim(apipop_sample)## [1] 600   4\nnames(apipop_sample)## [1] \"stype\"   \"ID_unit\" \"Prob\"    \"Stratum\"\napipop_sample <- apipop_reorder %>% slice(apipop_sample$ID_unit)\napi_sample_d = svydesign(id=~1,strata=~stype,data=apipop_sample,fpc=~n)\n\nsvytotal(~api99,api_sample_d)##         total    SE\n## api99 3912987 31953\nconfint(svytotal(~api99,api_sample_d))##         2.5 %  97.5 %\n## api99 3850360 3975614"},{"path":"introduction-to-common-sampling-techniques-in-r.html","id":"source","chapter":"48 Introduction to common sampling techniques in r","heading":"48.0.4 Source:","text":"https://www.rdocumentation.org/packages/survey/versions/4.1-1https://r-survey.r-forge.r-project.org/survey/html/api.htmlhttps://cran.r-project.org/web/packages/srvyr/srvyr.pdfhttps://online.stat.psu.edu/stat506/book/export/html/630https://en.wikipedia.org/wiki/Stratified_samplinghttps://www.rdocumentation.org/packages/sampling/versions/2.9/topics/stratahttps://zacharylhertz.github.io/posts/2021/06/survey-package","code":""},{"path":"ten-interview-questions-and-answers.html","id":"ten-interview-questions-and-answers","chapter":"50 Ten interview questions and answers","heading":"50 Ten interview questions and answers","text":"Mengsu Alan Yang Zhe Wang","code":""},{"path":"ten-interview-questions-and-answers.html","id":"forward","chapter":"50 Ten interview questions and answers","heading":"50.1 Forward","text":"following cheatsheet created help fellow classmates internship search current strong-headwind job market environment.\nDon’t give ! can ! Happy hunting.","code":""},{"path":"ten-interview-questions-and-answers.html","id":"questions-and-answers","chapter":"50 Ten interview questions and answers","heading":"50.2 Questions and Answers","text":"","code":""},{"path":"ten-interview-questions-and-answers.html","id":"q1.-what-is-the-difference-between-r-and-python","chapter":"50 Ten interview questions and answers","heading":"50.2.1 Q1. What is the difference between R and Python?","text":"Answer: model building Data Science libraries similar comparable.\nModel Interpretability: R better model interpretability compared python (easier humans understand, important reporting management).\nProduction: Python programming good production, R falls short.\nCommunity Support: R better community support Python.\nData Visualization: R better data visualization libraries python.\nLearning curve: learning curve python steep R, R higher technical barrier entry.","code":""},{"path":"ten-interview-questions-and-answers.html","id":"q2.-what-are-r-packages","chapter":"50 Ten interview questions and answers","heading":"50.2.2 Q2. What are R packages?","text":"Answer: Packages collections data, functions, documentation extends capabilities base R [1]. 10,000 packages stored CRAN repository number still increasing [3]! Packages ggplot2, tibble, tidyr, dplyr part opinionated collection known “tidyverse.” One can install package calling install.packages() name package quotes argument function.","code":""},{"path":"ten-interview-questions-and-answers.html","id":"q3.-what-are-the-advantages-of-using-r","chapter":"50 Ten interview questions and answers","heading":"50.2.3 Q3. What are the advantages of using R?","text":"Answer: One strength R lies fact open source, therefore freely available distribute. R support data wrangling needs able create high quality highly manipulatible graphs plots ggplot2. R also platform independent highly compatible [2] runs operating systems. language great statistics lot Statistician buy-therefore great community support. needed, R also capable supporting Machine Learning operations.","code":""},{"path":"ten-interview-questions-and-answers.html","id":"q4.-what-are-the-disadvantages-of-using-r","chapter":"50 Ten interview questions and answers","heading":"50.2.4 Q4. What are the disadvantages of using R?","text":"Answer: beginners, R complication language steep learning curve. lacks standard GUI RStudio must used, inferior Python Jupyter Notebooks organization work flow. R good big data; consumes high memory slower run time Python MATLAB [3]. R falls short security language basic security measures. Many functionalities spread across many different packages, inconsistent quality, functionality, documentation.","code":""},{"path":"ten-interview-questions-and-answers.html","id":"q5.-what-is-the-difference-between-matrix-and-data-frames","chapter":"50 Ten interview questions and answers","heading":"50.2.5 Q5. What is the difference between matrix and data frames?","text":"Answer: data structure special way organizing data computer can used effectively. idea reduce spatial temporal complexity different tasks. two important data structures R Matrix Dataframe, look differ nature.matrix R –\nhomogeneous collection data sets arranged two-dimensional rectangular organization. *n array similar data types. created using vector input. fixed number rows columns. can perform many arithmetic operations R matrices, addition, subtraction, multiplication, division.Dataframes R –\nused store data tables. can contain multiple data types multiple columns called fields. ’s list equal length vectors. ’s generalized form matrix. ’s like table Excel worksheet. column row names. Row names unique empty columns. data stored must number, character, factor type. DataFrame heterogeneous.","code":""},{"path":"ten-interview-questions-and-answers.html","id":"q6.-what-is-the-difference-between-library-and-require","chapter":"50 Ten interview questions and answers","heading":"50.2.6 Q6. What is the difference between library() and require()?","text":"Answer: library() require() can used attach load installed additional packages. Installed packages identified help “DESCRPTION” file contains Build:field. name package needs loaded using library() require() must match name package’s “DESCRPTION” file.require() designed used inside functions gives warning message returns logical value, FALSE requested package found TRUE \npackage loaded.library() default returns error requested package exist.","code":""},{"path":"ten-interview-questions-and-answers.html","id":"q7.-what-types-of-data-files-can-be-read-and-exported-in-r","chapter":"50 Ten interview questions and answers","heading":"50.2.7 Q7. What types of data files can be read and exported in R?","text":"Answer: Data import output R programming means can read data external files, write data external files, can access files outside R environment. File formats like CSV, XML, xlsx, JSON, web data like HTML tables can imported R environment read manipulated data analysis [4]; data present R environment can stored external files file formats.","code":""},{"path":"ten-interview-questions-and-answers.html","id":"q8.-how-many-data-structures-does-r-have","chapter":"50 Ten interview questions and answers","heading":"50.2.8 Q8. How many data structures does R have?","text":"Answer: R six types basic data structures. can organize data structures according dimensions(1d, 2d, nd). can also classify homogeneous heterogeneous (can contents different types ).VectorListMatrixData frameArrayFactor","code":""},{"path":"ten-interview-questions-and-answers.html","id":"q9.-what-is-the-difference-between-pivot_wider-and-pivot_longer","chapter":"50 Ten interview questions and answers","heading":"50.2.9 Q9. What is the difference between pivot_wider() and pivot_longer()?","text":"Answer: pivot_longer() makes datasets longer vertically increasing number rows decreasing number columns; every row becomes observation. Length relative term, can say (e.g.) dataset longer dataset B. pivot_longer() commonly needed tidy wild-caught datasets often optimize ease data entry ease comparison rather ease analysis.pivot_wider() opposite pivot_longer(): makes dataset wider increasing number columns decreasing number rows. ’s relatively rare need pivot_wider() make tidy data, ’s often useful creating summary tables presentation, data format needed tools.","code":""},{"path":"ten-interview-questions-and-answers.html","id":"q10.-why-do-people-use-ggplot2","chapter":"50 Ten interview questions and answers","heading":"50.2.10 Q10. Why do people use ggplot2?","text":"Answer: ggplot2 plotting package provides helpful commands create complex plots data data frame. provides programmatic interface specifying variables plot, displayed, general visual properties following Grammar Graphics. Therefore, need minimal changes underlying data change decide change bar plot scatterplot. helps creating publication quality plots minimal amounts adjustments tweaking.","code":""},{"path":"ten-interview-questions-and-answers.html","id":"references-1","chapter":"50 Ten interview questions and answers","heading":"50.3 References","text":"[1] Wickham, Hadley, Garrett Grolemund. R Data Science: Import, Tidy, Transform, Visualize Model Data, O’Reilly, Pozostate, 2017.[2] Intellipaaat. “R Programming Interview Questions.” YouTube, 3 Jan. 2021, https://www.youtube.com/watch?v=lyFDNkbsQuE&ab_channel=Intellipaat.[3] Simplilearn. “R: Overview, Applications R Used .” Simplilearn.com, Simplilearn, 3 Oct. 2022, https://www.simplilearn.com/--r-article.[4] “R Programming Language - Introduction.” GeeksforGeeks, 15 Aug. 2021, https://www.geeksforgeeks.org/r-programming-language-introduction/.","code":""},{"path":"data-science-internship-search-survival-guide.html","id":"data-science-internship-search-survival-guide","chapter":"51 Data science internship search survival guide","heading":"51 Data science internship search survival guide","text":"Austin Chen Kelly DuAs current graduate student (even undergraduate), always knew importance gaining real world experience, translate classroom teachings something tangible understand field likely investing better part career towards. However, process obtain valuable experience procure summer internship always easy, fact, actually quite contrary. personal experience, despite knowing plethora positions available, still struggle find actual application forms links. even applying numerous positions (thankfully getting past resume screenings), found idea expect different kinds interviews facing. Although certainly “interview preparation” resources available online, often find generalized, frankly unhelpful. Therefore, Community Contribution project pool together actual experiences two Data Science students, hopefully save fellow peers pain anguish felt first started “internship grind”.Throughout composition “survival guide”, genuinely surprised amount useful information able put words . Despite using majority included daily basis (Summer 2023 cycle still ongoing), actually translating knowledge pen paper made realize complicated interview process can . Thus, evaluate project, can confidently say work accomplished help others least getting familiarized data science internship search. certainly guarantee interview, even explicitly increase rate success, definitely equip new candidates toolbelt utilization. point something might differently next time, include section actual interview processes (company name, many rounds, interview type …etc). Although list going extraordinarily long (respectable), act reference need . conclude, internship search, especially current economic climate, undoubtedly frustrating time consuming, hope project ease stress future data science interns.link survival guide!\nhttps://github.com/austinchen11/Fall2022_EDAV_Community_Contribution","code":""},{"path":"data-visualization-interview-questions-sample.html","id":"data-visualization-interview-questions-sample","chapter":"52 Data visualization interview questions sample","heading":"52 Data visualization interview questions sample","text":"Yifan Zhu Yuqi Shao","code":""},{"path":"data-visualization-interview-questions-sample.html","id":"abstract","chapter":"52 Data visualization interview questions sample","heading":"52.1 Abstract","text":"Data visualization important data scientists efficiently understand big-data, define features, interpret findings, especially communicating non-technical audiences. Visualization tools dashboards containing multiple metrics convey monitor critical information, help executive team make business decisions workplace. current Master Science students Data Science, equipped data interpretation visualization techniques. motivation following content prepare data visualization questions may encounter Data Scientist Data Analyst interviews, develop basic framework start end.content designed two parts. 1. Data manipulation. importing tools, data prepared analytical format data order analyzed. Data preprocessing transformation always prioritized projects. issues carefully considered. example, missing data, sparse data, outliers normalization; 2. Tools Libraries. data processing, clarify questions interested investigate data using multiple methods. Therefore, questions regarding tools libraries prepared.analyzing reflecting interview process, learned better structure answer evaluator’s perspective. question bank initially writing questions ’ve asked interview process starting put categories, lengthy process made less efficient sometimes missed questions work. process , begin categorizing knowledge areas question types, create comprehensive framework organize ideas better. Also, initially try make interview sample answers close reality possible, later realize wording actual interviews may confuse readers conversational. reworded answer include relevant points.","code":""},{"path":"data-visualization-interview-questions-sample.html","id":"data-manipulation","chapter":"52 Data visualization interview questions sample","heading":"52.2 Data Manipulation","text":"","code":""},{"path":"data-visualization-interview-questions-sample.html","id":"question-what-is-tidy-data","chapter":"52 Data visualization interview questions sample","heading":"52.2.0.1 Question: What is tidy data?","text":"Answer: tidy data standard way mapping meaning dataset structure. variable forms column; observation forms row; type observational unit forms table.","code":""},{"path":"data-visualization-interview-questions-sample.html","id":"question-what-is-data-modelling","chapter":"52 Data visualization interview questions sample","heading":"52.2.0.2 Question: What is data modelling?","text":"Answer: Data modeling analysis data objects used business context identification relationships among data objects. Data modeling first step performing object-oriented programming","code":""},{"path":"data-visualization-interview-questions-sample.html","id":"question-what-is-sparse-data-and-what-is-missing-data","chapter":"52 Data visualization interview questions sample","heading":"52.2.0.3 Question: What is sparse data and what is missing data?","text":"Answer: Sparse data contains 0’s missing data contains null; sparse data cause overfitting increase time space complexity, model underestimate importance sparse features. Instead dropping missing data, use interpolation method fill NAs replace mean, mod, medium values.","code":""},{"path":"data-visualization-interview-questions-sample.html","id":"questionwhat-is-an-outlier-how-to-deal-with-outliers","chapter":"52 Data visualization interview questions sample","heading":"52.2.0.4 Question:What is an outlier? How to deal with outliers?","text":"Answer: outlier observation lies abnormal distance values random sample population. need define outliers based certain metrics (e.g., standard deviation, boxplot). Instead dropping outliers, also need consider transforming data.","code":""},{"path":"data-visualization-interview-questions-sample.html","id":"question-what-would-you-do-with-suspected-or-missing-data","chapter":"52 Data visualization interview questions sample","heading":"52.2.0.5 Question: What would you do with suspected or missing data?","text":"Answer: can prepare data validation report explain data failed. can use strategies like deletion, single imputation, mean imputation, median imputation, mode imputation, etc.","code":""},{"path":"data-visualization-interview-questions-sample.html","id":"question-what-process-of-transforming-raw-data-into-a-visualization","chapter":"52 Data visualization interview questions sample","heading":"52.2.0.6 Question: What process of transforming raw data into a visualization?","text":"Answer: First , need gather stakeholder’s input unstructured goal visualization. deal raw data procedure data collection, data cleaning (De-dupping, missing values, standardization), chart type selection, data preparation (formatting, converting, grouping/aggregating).","code":""},{"path":"data-visualization-interview-questions-sample.html","id":"tools-and-library","chapter":"52 Data visualization interview questions sample","heading":"52.3 Tools and Library","text":"","code":""},{"path":"data-visualization-interview-questions-sample.html","id":"question-list-several-r-libraries-that-can-be-used-for-data-visualisation","chapter":"52 Data visualization interview questions sample","heading":"52.3.0.1 Question: List several R libraries that can be used for data visualisation","text":"Answer: ggplot2, Lattice, highcharter, Leaflet, RColorBrewer, Plotly, sunburstR, RGL","code":""},{"path":"data-visualization-interview-questions-sample.html","id":"question-how-do-you-choose-a-suitable-visualization-method-in-terms-of-color-shape-measure-and-orientation","chapter":"52 Data visualization interview questions sample","heading":"52.3.0.2 Question: How do you choose a suitable visualization method in terms of color, shape, measure, and orientation?","text":"Answer: choosing appropriate visualization method, need consider various variables affect way data presented. Color, shape, measure, orientation critical variables consider. example, different colors can help differentiate different datasets; different shapes can help recognize different types data. Measure orientation also critical can affect way read understand data.","code":""},{"path":"data-visualization-interview-questions-sample.html","id":"question-what-is-the-difference-between-analytics-and-data-visualization","chapter":"52 Data visualization interview questions sample","heading":"52.3.0.3 Question: What is the difference between “analytics” and “data visualization”?","text":"Answer: “Analytics” broad can referred analytic methods data interpretation. “Data visualization” specific focuses visual representations showing patterns relationships data points help people better understand data.","code":""},{"path":"data-visualization-interview-questions-sample.html","id":"question-what-does-kiss-stand-for-how-does-this-relate-to-data-visualization","chapter":"52 Data visualization interview questions sample","heading":"52.3.0.4 Question: What does “KISS” stand for? How does this relate to data visualization?","text":"Answer: “KISS” short “Keep Simple, Stupid,” design goal create simple, easily interpreted visualizations avoid complexity.","code":""},{"path":"data-visualization-interview-questions-sample.html","id":"question-what-are-stages-of-visualization","chapter":"52 Data visualization interview questions sample","heading":"52.3.0.5 Question: What are stages of visualization?","text":"Answer: mainly four stages data visualization: exploratory, descriptive, predictive, explanatory, give business insights can visualize data.","code":""},{"path":"data-visualization-interview-questions-sample.html","id":"question-how-can-you-visualize-more-than-three-dimensions-in-a-single-chart","chapter":"52 Data visualization interview questions sample","heading":"52.3.0.6 Question: How can you visualize more than three dimensions in a single chart?","text":"Answer: standardized way can many different ways. example, using charts multiple axes like pie charts heat maps; using animation sequences show changes time.","code":""},{"path":"data-visualization-interview-questions-sample.html","id":"question-what-is-depth-cueing-in-visualization","chapter":"52 Data visualization interview questions sample","heading":"52.3.0.7 Question: What is depth cueing in visualization?","text":"Answer: concept using color, texture, size create sense depth 2 dimensional image, implemented blending objects background increasing distance viewers. Depth cueing can used different applications. example, one can show one object closer another making bigger darker; one can give two colors represent two different data points.","code":""},{"path":"data-visualization-interview-questions-sample.html","id":"question-what-is-row-level-security","chapter":"52 Data visualization interview questions sample","heading":"52.3.0.8 Question: What is Row-Level Security?","text":"Answer: Row-Level security row columns table permission setting protects database.","code":""},{"path":"data-visualization-interview-questions-sample.html","id":"question-is-excel-a-data-visualization-tool","chapter":"52 Data visualization interview questions sample","heading":"52.3.0.9 Question: Is excel a data visualization tool?","text":"Answer: , Excel spreadsheet software used create charts, tables, graphs, pivot tables. provides many features data analysis visualization, data visualization tool.","code":""},{"path":"how-i-got-into-amazon-as-a-full-time-data-analyst.html","id":"how-i-got-into-amazon-as-a-full-time-data-analyst","chapter":"53 How I got into Amazon as a full time data analyst","heading":"53 How I got into Amazon as a full time data analyst","text":"Robin ZhaoI wrote blog post got Amazon, full-time data analyst. Link: https://medium.com/@robinzhao39/--become--data-analyst--amazon--undergrad-c8371081df13?source=friends_link&sk=96295ba3cd71b28d6e7d0b24527b01e2Most master students class want find data analyst job study graduation. motivates write experiences help better understand process. blog includes learned essential data science skills, college activities, interview process. blog addresses job searching/learning anxiety, provides potential directions lost students, gives clarity big-tech interviews prepare .","code":""},{"path":"benford-case-study.html","id":"benford-case-study","chapter":"55 Benford Case Study","heading":"55 Benford Case Study","text":"Abhiram Gaddam, Devan Samant","code":"\nlibrary(benford.analysis)  #install.packages(\"benford.analysis\")\nlibrary(dplyr)\nlibrary(plotly)\nlibrary(tidyverse)"},{"path":"benford-case-study.html","id":"contents","chapter":"55 Benford Case Study","heading":"55.1 Contents","text":"tutorial, introduce numeric property called Benford’s Law illustrate applications fraud detection. using benford_analysis package R along example case study.Sections:IntroductionBenford.Analysis PackageRandomized Data Case StudyConclusionSources","code":""},{"path":"benford-case-study.html","id":"introduction-6","chapter":"55 Benford Case Study","heading":"55.2 Introduction","text":"niche area consulting industry called forensic analytics analysts try identify risks quantify wrongdoing using array statistical data techniques. example, imagine whistleblower notifies company’s general counsel collusion sales finance representatives artificially create invoices. company may hire forensic analysts extract determine happening. many quantitative qualitative methods perform concluding anything need specific context project. One heuristic Benford’s Law.","code":""},{"path":"benford-case-study.html","id":"what-is-benfords-law","chapter":"55 Benford Case Study","heading":"55.2.1 What is Benford’s Law?","text":"Benford’s law (also known first digit law) states leading digits many data sets probably going small. example, numbers set (30%) leading digit 1, one might expect probability 11.1% (one nine digits). one, second common leading digit 2 17.5%. forth 3 onward. put simply, Benford’s law probability distribution likelihood first digit set numbers (Frunza, 2015). pattern intuitive phenomenon holds true many naturally occurring datasets (ex. height mountains around world) well man-made ones company’s general ledger.formula Benford’s Law :\n\\(P(d) = \\frac{ln(1 + \\frac{1}{d})}{ln10}\\)\n\\(d\\) leading digit (number 1 9)distribution shows expected occurrence leading digits according Benford’s Law.Benfords Law Distribution.However, caveats regarding application Benford’s Law:Benford’s Law works better larger sets data. law shown hold true data sets containing 50 100 numbers, experts believe data sets 500 numbers better suited type analysis.\n(https://tinyurl.com/2p97fsvn)Benford’s Law works better larger sets data. law shown hold true data sets containing 50 100 numbers, experts believe data sets 500 numbers better suited type analysis.\n(https://tinyurl.com/2p97fsvn)conform law, data set use must contain data number 1 9 equal chance occurring leading digit. Otherwise, Benford’s Law doesn’t apply. example, consider listing heights current NBA players. Since NBA players range height 5 feet 10 inches 7 feet 3 inches, player heights begin 1, 2, 3, 4, 8, 9; Clearly digits chance first digit listing, making Benford’s Law inapplicable.conform law, data set use must contain data number 1 9 equal chance occurring leading digit. Otherwise, Benford’s Law doesn’t apply. example, consider listing heights current NBA players. Since NBA players range height 5 feet 10 inches 7 feet 3 inches, player heights begin 1, 2, 3, 4, 8, 9; Clearly digits chance first digit listing, making Benford’s Law inapplicable.Benford’s Law applicable datasets, generally applicable large sets naturally occurring numbers connection like:Companies’ stock market values.Data found texts.Demographic data, including state city populations.Income tax data.Mathematical tables, like logarithms.River drainage rates.Scientific data.\nBenford, F (1938)","code":""},{"path":"benford-case-study.html","id":"applications-in-fraud-detection","chapter":"55 Benford Case Study","heading":"55.2.2 Applications in Fraud Detection","text":"One primary practical use Benford’s Law fraud error detection. expected large set numbers follow law, accountants, auditors, economists tax professionals benchmark normal levels particular number set . famously documented examples Benford’s Law applied towards fraud detection (Frunza, 2015):1990s, accountant named Mark Nigrini found Benford’s law can effective red-flag test fabricated tax returns. Authentic tax data usually follows Benford’s law, whereas made-returns .law used 2001 study economic data Greece, implication country may manipulated numbers join European Union.Ponzi schemes can detected using law. Unrealistic returns, purported Maddoff scam, fall far expected Benford probability distribution .","code":""},{"path":"benford-case-study.html","id":"benford-package-in-r","chapter":"55 Benford Case Study","heading":"55.3 Benford Package in R","text":"Benford Analysis (benford.analysis) package provides tools make easier validate data using Benford’s Law. main purpose package identify suspicious data may need verification.Documentation package can found :\nhttps://cran.r-project.org/web/packages/benford.analysis/benford.analysis.pdfYou can install package CRAN running following (uncommented):package comes 6 real datasets Mark Nigrini’s book Benford’s Law: Applications Forensic Accounting, Auditing, Fraud Detection.","code":"\n#install.packages(\"benford.analysis\")"},{"path":"benford-case-study.html","id":"example-usage-of-benford.analysis","chapter":"55 Benford Case Study","heading":"55.3.1 Example Usage of benford.analysis","text":"section give example using 189,470 records corporate payments dataset provided package.Load package dataTo validate data Benford’s law simply use function “benford” appropriate column:creates object class “Benford” results analysis using first two significant digits default.Lets plot bfd observe trends. Note running analysis using default parameters, .e., ..digits = 2. parameter can modified want analyze first digit .original data blue expected frequency according Benford’s law red.\nexample, first plot shows data tendency follow Benford’s law.\nalso clear outlier 50.package also provides helper functions investigate data. example, can easily extract observations largest discrepancies using “getSuspects” function.","code":"\ndata(corporate.payment) \n\ndf <- corporate.payment\nhead(df)##   VendorNum       Date  InvNum Amount\n## 1      2001 2010-01-02 0496J10  36.08\n## 2      2001 2010-01-02 1726J10  77.80\n## 3      2001 2010-01-02 2104J10  34.97\n## 4      2001 2010-01-02 2445J10  59.00\n## 5      2001 2010-01-02 3281J10  59.56\n## 6      2001 2010-01-02 3822J10  50.38\nbfd <- benford(df$Amount)\nbfd## \n## Benford object:\n##  \n## Data: df$Amount \n## Number of observations used = 185083 \n## Number of obs. for second order = 65504 \n## First digits analysed = 2\n## \n## Mantissa: \n## \n##    Statistic  Value\n##         Mean  0.496\n##          Var  0.092\n##  Ex.Kurtosis -1.257\n##     Skewness -0.002\n## \n## \n## The 5 largest deviations: \n## \n##   digits absolute.diff\n## 1     50       5938.25\n## 2     11       3331.98\n## 3     10       2811.92\n## 4     14       1043.68\n## 5     98        889.95\n## \n## Stats:\n## \n##  Pearson's Chi-squared test\n## \n## data:  df$Amount\n## X-squared = 32094, df = 89, p-value < 2.2e-16\n## \n## \n##  Mantissa Arc Test\n## \n## data:  df$Amount\n## L2 = 0.0039958, df = 2, p-value < 2.2e-16\n## \n## Mean Absolute Deviation (MAD): 0.002336614\n## MAD Conformity - Nigrini (2012): Nonconformity\n## Distortion Factor: -1.065467\n## \n## Remember: Real data will never conform perfectly to Benford's Law. You should not focus on p-values!\nplot(bfd)\nsuspects <- getSuspects(bfd, df)\nsuspects##        VendorNum       Date       InvNum  Amount\n##     1:      2001 2010-01-02      3822J10   50.38\n##     2:      2001 2010-01-07     100107-2 1166.29\n##     3:      2001 2010-01-08  11210084007 1171.45\n##     4:      2001 2010-01-08      1585J10   50.42\n##     5:      2001 2010-01-08      4733J10  113.34\n##    ---                                          \n## 17852:     52867 2010-07-01 270358343233   11.58\n## 17853:     52870 2010-02-01 270682253025   11.20\n## 17854:     52904 2010-06-01 271866383919   50.15\n## 17855:     52911 2010-02-01 270957401515   11.20\n## 17856:     52934 2010-02-01 271745237617   11.88"},{"path":"benford-case-study.html","id":"randomized-case-study","chapter":"55 Benford Case Study","heading":"55.4 Randomized Case Study","text":"prior section saw data manipulated static values, clearly first digit rule broken. data randomly manipulated?Let us try experiment see can use Benford’s Law detect potential data manipulation bad actor randomly generated fake sales. experiment, baseline also using random data highlight point conclusion . Let us pretend company sales data comprised prices quantities items sold. code chunck sets data frame one row recorded sale number items sold set price.Now let us set see dataset exhibits expected pattern first digit.[value] sales exhibits commonality Benford’s Law exact see first digit quite 30%. However still decreasing probability leading digit. address limitation next section. now, let us try manipulating data see patterns change.","code":"\nprice <- sample(1:1e3, size = 1e5, replace=TRUE)\nquantity <- sample(1:1e4, size = 1e5, replace=TRUE)\ndf <- data.frame(price,quantity) %>% \n  mutate(value = price*quantity) %>% \n  mutate(digit = substr(as.character(value), 1, 1))\n\nhead(df)##   price quantity   value digit\n## 1   382     2174  830468     8\n## 2   754     7813 5891002     5\n## 3    26     5486  142636     1\n## 4   209     9865 2061785     2\n## 5   499     8341 4162159     4\n## 6   526     4060 2135560     2\ndf_group <- df %>% group_by(digit) %>% summarise(count = n()) %>%\n    mutate(count_percent = count/sum(count))\n\nbase_benford = data.frame(c(1,2,3,4,5,6,7,8,9), c(.31,.176,.125,.097,.079,.067,.058,.051,.046))\ncolnames(base_benford) <- c('digit','percent')\n\nggplot(data=df_group, aes(x=digit, y=count_percent, fill=\"blue\")) +\n  geom_bar(stat=\"identity\", fill='lightblue') + \n  geom_point(aes(x=base_benford$digit, y=base_benford$percent)) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"First Digit of Randomized Sales\",\n              subtitle = \"Original Data (bar) vs Expected Benford (point)\",\n              #caption = \"TBD\",\n              x = \"First Digit\", y = \"% Occurance\",\n              #tag = \"A\"\n              ) +  theme(legend.position=\"none\")"},{"path":"benford-case-study.html","id":"manipulated-data","chapter":"55 Benford Case Study","heading":"55.4.1 Manipulated Data","text":"next piece code, can pretend someone entered additional sales “randomly” adding lots relatively smaller sales 100k.can see adding sales randomly change distribution (enough added). can quantify change? Let us use benford package cacluate statistics. followingPrinting benford object creates verbose output copied main statistics two dataframes:bfd_1 (df - original)\nStatistic Value\nMean 0.526\nVar 0.073\nEx.Kurtosis -1.048\nSkewness -0.145bfd_1 (df2 - manipulated)\nStatistic Value\nMean 0.65\nVar 0.07\nEx.Kurtosis -0.59\nSkewness -0.66If data follows Benford’s Law, expected statistics close :\nStatistic Value\nMean 0.5\nVar 0.083\nEx.Kurtosis -1.2\nSkewness 0From glance, evident statistics corresponding original df much closer expected statistics df2. particular, Ex.Kurtosis Skewness differ significantly df2 expected values. indicators set values deviate expected distribution corresponding Benford’s Law.","code":"\nvalue2 <- sample(1:1e6, size = 4e5, replace=TRUE)\ndf_extra_sales <- data.frame(0,0,value2)\ncolnames(df_extra_sales) <- c('price','quantity','value')\ndf2 <- data.frame(price,quantity) %>% \n  mutate(value = price*(quantity)) %>% \n  rbind(df_extra_sales)  %>% \n  mutate(digit = substr(as.character(value), 1, 1))\n\ndf_group2 <- df2 %>% group_by(digit) %>% summarise(count = n()) %>%\n    mutate(count_percent = count/sum(count))\n\ndf_group_combined <- data.frame(df_group$digit,df_group$count_percent,df_group2$count_percent) \ncolnames(df_group_combined) <- c('digit','Original','Manipulated')\ndf_group_combined <- df_group_combined %>% pivot_longer(cols=c('Original','Manipulated'))\ncolnames(df_group_combined) <- c('digit','Dataset','count_percent')\n\nggplot(data=df_group_combined, aes(x=digit, y=count_percent, fill=fct_rev(as.factor(Dataset)) )) +\n  geom_bar(stat=\"identity\", position='dodge') +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"First Digit of Manipulated Sales\",\n              subtitle = \"Original Data vs Manipulated Data\",\n              x = \"First Digit\", y = \"% Occurance\",\n              fill=\"Dataset\"\n              ) +  theme(legend.position=\"bottom\")\nbfd_1 <- benford(df$value)\n#bfd_1\n\nbfd_2 <- benford(df2$value)\n#bfd_2"},{"path":"benford-case-study.html","id":"findings","chapter":"55 Benford Case Study","heading":"55.4.2 Findings","text":"show us? sets randomly generated, one conform Benford’s Law? One reason data product many random variables tends exhibit log-normal distribution fits well first digit rule. However, adding values static way, change data uniform distort decaying curve.Now important understand 1) case extreme example highlight point, 2) Benford’s Law statistics prove anything - provide guidance areas may look strange. reality, forensic testing done specific segments accounts, people interviewed, systems logs analyzed, etc. Similarly, even Benford’s Law holds true, mean fraud; bad actor understands fraud detection methods likely hide better.","code":""},{"path":"benford-case-study.html","id":"conclusion-8","chapter":"55 Benford Case Study","heading":"55.5 Conclusion","text":"important thing note Benford’s law strict mathematical proof, simply heuristic. broad guideline helps investigation large data sets see conform observed trends. feature dataset conforms Benford’s Law proof validity vice versa. reason, Benford analysis usually conducted primary investigatory exercise, following thorough investigation described previous section conducted.","code":""},{"path":"benford-case-study.html","id":"sources","chapter":"55 Benford Case Study","heading":"55.6 Sources","text":"Benford, F. “Law Anomalous Numbers,” Proceedings American Philosophical Society, 78, 551–572. 1938.Frunza, M. (2015). Solving Modern Crime Financial Markets: Analytics Case Studies. Academic Press.https://cran.r-project.org/web/packages/benford.analysis/benford.analysis.pdfhttps://www.statisticshowto.com/benfords-law/https://tinyurl.com/2p97fsvn","code":""},{"path":"equity-in-ai.html","id":"equity-in-ai","chapter":"57 Equity in AI","heading":"57 Equity in AI","text":"Dillon SparksPicture : long day work, get home exhausted even open fridge, let alone cook dinner clean afterwards. decide open favorite food delivery app instead. tons personally recommended restaurants fingertips, suggestions favorite foods. pick new place decide order one recommended dishes. Crisis averted: can sink couch watch Netflix food comes.technology advances, able “leverage computers machines mimic problem-solving decision-making capabilities human mind.” (IBM Cloud Education, Artificial Intelligence (AI)?) known artificial intelligence, AI, heart various technological tools use every single day. example, common real-world applications AI speech recognition, customer service, facial recognition/face ID, recommendation engines, automated stock trading, fraud detection. interact benefit technologies often might think. beginning scenario, food delivery app used AI recommend dish ordered, monitor transaction potential fraud checkout, chat unfortunate event issues order. small example ways companies like Microsoft, Google, Amazon, IBM, name , utilize AI enhance consumer experience, can simultaneously interact AI make life much convenient.However, must remember machine learning algorithms heart artificial intelligence developed humans, implicit biases inherent perspectives world. Oftentimes developers approaching real world issues solely efficiency mind, completely ignoring social justice implications algorithms. Additionally, aforementioned companies many others like seeking profit else, means powerful tools quickly gain ability cause great harm.numerous instances discrimination AI, providing evidence powerful technological tools inherit implicit biases developing . example, AI used consumer-lending applications advertising homes potential tenants/owners found discriminate lenders applicants based race. 2016, Facebook received backlash microtargeting feature “lets advertisers send ads specific groups via drop-menu categories, including age, race, marital status, disability status.” (Sisson, P., Housing discrimination goes high tech.) Reporters ProPublica able demonstrate dangerous feature purchasing ads social media website, blatantly discriminated people basis race demographics, effectively violating Fair Housing Act. study U.C. Berkeley discrimination AI influenced consumer-lending found , “lenders charge otherwise-equivalent Latinx/African-American borrowers 7.9 (3.6) bps higher rates purchase (refinance) mortgages, costing $765M yearly.” (Bartlett, R., Morse, ., Stanton, R., & Wallace, N. (2019). (rep.). Consumer-Lending Discrimination FinTech Era.)problem racism discrimination AI widespread SNL even created skit earlier year. scene focuses ad new Amazon Go store, customers don’t pay directly items purchase Amazon account billed later . narrator commercial says Amazon using “computer vision, deep learning algorithms, sensor fusion” (“Amazon Go”, SNL. ) handle transactions. Black customers store hesitant even pick anything afraid ‘cutting edge’ technology still unfairly target , accuse stealing. point skit dramatize Black people’s distrust new developments technology, like grab go store, technology always capacity reflect biases people developing , despite forward may seem. endless examples racism discrimination applications AI, including limited hiring technologies, facial recognition, crime prediction software.call equitable AI grown stronger uses, misuses, become widespread. companies taken upon serve populations often victims biased algorithms. example, professionals healthcare industry started using AI keep patients races engaged medical plans, treat avoid opioid abuse, increase vaccine equity, eliminate bias diagnosis treatment (Natarajan, P., Increasing access equity healthcare ai). approaches center wellbeing populations served, effectively works eliminate potential bias data interpreting .can see, technology extremely powerful capacity significantly improve quality life everyone, used correctly. Columbia data scientists pledge “Engineers Good”, important enter workforce equity forefront efforts.Sources Cited:Akselrod, O. (2021, July 13). artificial intelligence can deepen racial economic inequities: News & commentary. American Civil Liberties Union. Retrieved November 14, 2022, https://www.aclu.org/news/privacy-technology/-artificial-intelligence-can-deepen-racial--economic-inequitiesAmazon Go. (2022). Amazon Go- SNL. Retrieved November 14, 2022, https://www.youtube.com/watch?v=zS9U3Gc832Y.Bartlett, R., Morse, ., Stanton, R., & Wallace, N. (2019). (rep.). Consumer-Lending Discrimination FinTech Era. U.C. Berkeley. Retrieved https://faculty.haas.berkeley.edu/morse/research/papers/discrim.pdf.Duggan, W. (2022, November 11). Artificial Intelligence Stocks: 10 best AI companies. US NEWS. Retrieved November 14, 2022, https://money.usnews.com/investing/stock-market-news/slideshows/artificial-intelligence-stocks--10-best-ai-companiesNatarajan, P. (2022, April 13). Increasing access equity healthcare ai. MedCity News. Retrieved November 14, 2022, https://medcitynews.com/2022/04/increasing-access--equity--healthcare--ai/#:~:text=Augmenting%20existing%20human%2Dbased%20practices,%20diagnosis%20or%20treatment%20planIBM Cloud Education. (2020, June 3). Artificial Intelligence (AI)? IBM. Retrieved November 14, 2022, https://www.ibm.com/cloud/learn/--artificial-intelligence#toc-artificial-7ZT8FnXdRieke, ., Janardan, U., Duarte, N., & Hsu, M. (2021, July 6). Essential work. Upturn. Retrieved November 14, 2022, https://www.upturn.org/work/essential-work/\nSisson, P. (2019, December 17). Housing discrimination goes high tech. Curbed. Retrieved November 14, 2022, https://archive.curbed.com/2019/12/17/21026311/mortgage-apartment-housing-algorithm-discrimination","code":""},{"path":"breaking-into-tech-black-girl-edition.html","id":"breaking-into-tech-black-girl-edition","chapter":"58 Breaking into tech: black girl edition","heading":"58 Breaking into tech: black girl edition","text":"Woomy Michel","code":""},{"path":"breaking-into-tech-black-girl-edition.html","id":"background-1","chapter":"58 Breaking into tech: black girl edition","heading":"58.1 Background","text":"Hi, name Woomy Michel 👋🏼 ’m share story Black girl tech. first-born child Haitian immigrants, alumna Illustrious Clark Atlanta University (goooo Panthers 🐾♥️) current data science grad student Columbia University. first family attend graduate college, well attend grad school. short book, ’ll sharing honest journey, tips, advice ways break tech.honest, ’ve read “breaking tech” article, story never resonates . Whenever read piece, ’s told perspective someone ’s classed, well-resourced industry connections. always comes impersonal given grew complete opposite.Oftentimes, (emphasis “”) feel like people tend share good parts tech, without ever mentioning bad ugly. Social media honestly creates false sense reality. Everyone thinks tech extremely easy break roles pay $100k false. Don’t get wrong, ’s definitely roles easy get pretty high salaries, ’s everyone’s truth, especially mine., transparency big thing knowing full story diving deep waters unknown important.","code":""},{"path":"breaking-into-tech-black-girl-edition.html","id":"finding-my-true-calling","chapter":"58 Breaking into tech: black girl edition","heading":"58.2 Finding My True Calling","text":"started undergrad dual-degree engineering major. program geared towards students wanted two bachelor’s degrees: one engineering discipline one natural science discipline. time, majoring math biomedical engineering. thought going biomedical engineer. mean, else pursue someone extremely passionate mathematics? Engineering field people told go math background.tried engineering thing freshman year honestly…HATED ! didn’t help engineering professor go way belittle class days. type support program, tutors much help professor either. Despite hating engineering department, wanted stay program knew belonged . mean, getting good grades understanding material, wouldn’t stayed? Little know, program become severely unbearable 😅. “severely unbearable”, mean classes become significantly harder bring severe unhappiness. girl depressed stressed.miserable engineering student wasn’t anymore. knew heart math girlie, unsure wanted . Second semester, took programming class really enjoyed. first time taught computer science class another Black woman since middle school. Seeing similarities coding math made think ways apply math comp sci together.Computer Science Mathematics complimented well. Programming natural methodical, analytical catered problem solving skills much like math. Coding huge jigsaw puzzle ; writing chunks code constant debugging (#lots comments#) figure lines code returned error. Tracing code constantly rearranging chunks . never felt tiring, something happily kept night.end freshman year, completely changed major Mathematics added minor Computer Science. felt like burden lifted shoulders left engineering program. finally something passionate fun . affirmed Black faculty capable belonged STEM community exactly needed.","code":""},{"path":"breaking-into-tech-black-girl-edition.html","id":"ohtechnical-interviews","chapter":"58 Breaking into tech: black girl edition","heading":"58.3 Oh…Technical Interviews️ 😅","text":"feel like trajectory career changed technical interview season. completing first programming class, thought software engineering perfect route . Alas, whiteboard interviews extremely humbling experience .Now, roles interview mainly ask behavioral questions see ’re ideal candidate. However, comes tech industry, ’s behavioral interview followed several rounds technical interviews. never able get hang , honest. Something HackerRank interview just nerve wracking . knew program, couldn’t solve two coding assessments hour matter well prepped. top , way interviewer frame questions weird. ’d never understand asking interview questions asked convoluted way possible. One thing ’d always stumped ask clarifying questions gauge solve problem, even …wouldn’t click brain end interview. know feeling ’re taking exam ’re stuck problem finally remember solve , ’s late? .bombing nearly technical interviews, decided take step back think tech routes possibly see . Low behold, data science came across thanks CAU Math department.Spring semester sophomore year, took Intro Data Science Data Engineering class really enjoyed. know programming professor always mentioned know one programming language, can learn another. right, just get adjusted syntax. RStudio similar Python, like cousins (part). RMarkdown hardest thing beginning didn’t know knit document.Since discovering new found interest, ’ve opportunity :\n- conduct data science research National Science Foundation (NSF) ERASE Child Trafficking\n- presented research National Association Mathematicians (NAM) MathFest XXX Academic Data Science Alliance (ADSA) Spring Meeting 2022\n- interned GoFundMe data analyst\n- much !’m grateful didn’t let technical interviews deter following true passion. Sure days hard, never let stop pursuing career data science. failures always served learning experience . Always celebrate losses importantly, ’s start, ’s finish.","code":""},{"path":"breaking-into-tech-black-girl-edition.html","id":"tips-for-being-in-data-science","chapter":"58 Breaking into tech: black girl edition","heading":"58.4 Tips for being in Data Science","text":"’s bit advice want get data science:possibilities endless, don’t stick mainly STEM work data science. Data science tech lie intersections multiple industries. friends ’ve studied migration patterns using data science techniques. can anything field!Take time learn Python, R SQL. promise , languages know, better. Plus, makes marketable ☺️.CS major data science! Data science can applied field study. social sciences natural sciences.Apply summer internships fall! Whether want research industry, start looking roles early . Handshake great tool finding internships research opportunities.Network, network, network! never know open doors . Knowing someone goes long way sometimes.\nAlways ask help! Closed mouths don’t get fed.Take CS classes like Data Structures, Databases, Introduction Programming, etc. help learn fundamentals.can’t stress enough, makes happy! know lot people tell go money , peace happiness worth much end.","code":""},{"path":"breaking-into-tech-black-girl-edition.html","id":"affirmations-for-black-girls","chapter":"58 Breaking into tech: black girl edition","heading":"58.5 Affirmations for Black Girls","text":"Last, least, affirmations. Tech can tough us sometimes, ’s little something get rough days:lovedI belong hereI deservingI enoughI confidentI smartI validI can thisI brilliantI aloneI worthyI intelligentI make time take care myselfI gentle mistakesI capable anything put mind toI deserve nice things, everything desire within reachRemember industry needs worked hard get !","code":""},{"path":"breaking-into-tech-black-girl-edition.html","id":"resources","chapter":"58 Breaking into tech: black girl edition","heading":"58.6 Resources","text":"favorite resources folks want learn computer science, data science, programming, building community tech, etc:https://www.geeksforgeeks.org/ (one best CS sites ever learning programming, data structures interview prep)https://www.youtube.com/c/GeeksforGeeksVideos (GeeksforGeeks YouTube channel)https://hbcu20x20.com/ (Black job network, ample opportunities mock interviews, etc)https://leetcode.com/problemset// (tech interview prep)https://www.udemy.com/ (ample free courses!)https://www.edx.org/ (perfect resource data science data analytics)https://www.colorstack.org/ (tech organization Black Latinx students)https://experience.afrotech.com/ (one best Black tech conferences)https://www.renderatl.com/ (one best Black tech conferences)https://drive.google.com/file/d/18q40D9Gws-H7IvXcS0ZERJsuZ-FYfcHz/view?usp=sharing (link Python textbook)","code":""},{"path":"homepage-artwork.html","id":"homepage-artwork","chapter":"59 Homepage Artwork","heading":"59 Homepage Artwork","text":"Xingye FengI interested creating artwork community contribution welcome page, fun way contribute EDAV class can create something informative catchy people understand class contents community contribution.created two artworks, first one summary info page EDAV course content. includes R graphics learned far EDAV class. background R studio logo, ggplot2 logo emphasizes class.second artwork cover sheet community contribution contents. includes four topics: Cheat sheet, tutorials, interview questions case studies. corresponding icon represents feature.theme artworks blue, since frequently used color data visualization universally blue. Also, blue theme color R studio, major development environment use class. addition, Blue color professional suits topic class.display artwork community contribution welcome page, classmates’ good contribution works disclose following pages.Resources:\n1. EDAV info website: https://edav.info/\n2. Flaticon website: https://Flaticon.com/","code":""},{"path":"design-homepage-for-class-community.html","id":"design-homepage-for-class-community","chapter":"60 Design homepage for class community","heading":"60 Design homepage for class community","text":"Morris HsiehDear Prof, TAs, fellow students:choose design home page (images, text, /artwork) class community website contribute community. link website :https://edavcommunitycontribution.wordpress.com/Links external site.website includes introduction part, overview course lecturer, blog posts (fellows’ sharing).Sincerely,\nMorris Hsieh","code":""},{"path":"github-initial-setup.html","id":"github-initial-setup","chapter":"61 Github initial setup","heading":"61 Github initial setup","text":"Joyce Robbins","code":""},{"path":"github-initial-setup.html","id":"create-new-repo","chapter":"61 Github initial setup","heading":"61.1 Create new repo","text":"Create new repository copying template: http://www.github.com/jtr13/cctemplate following instructions README.","code":""},{"path":"github-initial-setup.html","id":"pages-in-repo-settings","chapter":"61 Github initial setup","heading":"61.2 Pages in repo settings","text":"Change source gh-pagesMay trigger GHA get work","code":""},{"path":"github-initial-setup.html","id":"add-packages-to-description-file","chapter":"61 Github initial setup","heading":"61.3 Add packages to DESCRIPTION file","text":"Need better process…Downloaded submissions CourseWorksCreate DESCRIPTION file. Add add dependencies projthis::proj_update_deps()https://twitter.com/ijlyttle/status/1370776366585614342Add Imports real DESCRIPTION file.Found problematic packages looking reverse dependencies packages failed install:devtools::revdep()Also used pak::pkg_deps_tree()Problems:magickrJava dependency qdap","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"tutorial-for-pull-request-mergers","chapter":"62 Tutorial for pull request mergers","heading":"62 Tutorial for pull request mergers","text":"","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"general","chapter":"62 Tutorial for pull request mergers","heading":"62.1 General","text":"following checklist steps perform merging pull request. point, ’re sure , request review one PR leaders.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"check-branch","chapter":"62 Tutorial for pull request mergers","heading":"62.2 Check branch","text":"PR submitted non-main branch.PR submitted main branch, provide instructions fix problem:Close PR.Close PR.Follow instructions forgetting branch committed pushed GitHub: https://edav.info/github#fixing-mistakesFollow instructions forgetting branch committed pushed GitHub: https://edav.info/github#fixing-mistakesIf trouble 2., delete local folder project, delete fork GitHub, start .trouble 2., delete local folder project, delete fork GitHub, start .Open new PR.Open new PR.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"examine-files-that-were-added-or-modified","chapter":"62 Tutorial for pull request mergers","heading":"62.3 Examine files that were added or modified","text":"ONE .Rmd file.ONE .Rmd file.additional resources resources/<project_name>/ folder.additional resources resources/<project_name>/ folder.files root directory besides .Rmd file.files root directory besides .Rmd file.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"check-.rmd-filename","chapter":"62 Tutorial for pull request mergers","heading":"62.4 Check .Rmd filename","text":".Rmd filename words joined underscores, white space. (Update: need branch name.).Rmd filename can contain lowercase letters. (Otherwise filenames sort nicely repo home page.)","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"check-.rmd-file-contents","chapter":"62 Tutorial for pull request mergers","heading":"62.5 Check .Rmd file contents","text":"file contain YAML header --- line.second line blank, followed author name(s).first line start single hashtag #, followed single whitespace, title.additional single hashtag headers chapter. (, new chapters created.)hashtag headers followed numbers since hashtags create numbered subheadings. Correct: ## Subheading. Incorrect: ## 3. Subheading.file contains setup chunk .Rmd file, contain setup label. (bookdown render fail duplicate chunk labels.)\n.e. use {r, include=FALSE} instead {r setup, include=FALSE}.\nSee sample .RmdLinks internal files must contain resources/<project_name>/ path, : ![Test Photo](resources/sample_project/election.jpg)file contain install.packages(), write functions, setwd(), getwd().’s anything else looks odd ’re sure, assign jtr13 review explain issue.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"request-changes","chapter":"62 Tutorial for pull request mergers","heading":"62.6 Request changes","text":"problems checks listed , explain pull request merged request changes following steps:, add changes requested label pull request.job pull request done now. contributors fix requests, review either move forward merge explain changes still need made.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"steps-to-merge-the-pr","chapter":"62 Tutorial for pull request mergers","heading":"62.7 Steps to Merge the PR","text":"click “Merge” things .","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"update-the-branch","chapter":"62 Tutorial for pull request mergers","heading":"62.7.1 Update the branch","text":"“Update Branch” visible toward end Conversation tab pull request, click . ensure working --date versions _bookdown.yml DESCRIPTION.Next make changes files contributor’s branch.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"add-the-filename-of-the-chapter-to-_bookdown.yml","chapter":"62 Tutorial for pull request mergers","heading":"62.7.2 Add the filename of the chapter to _bookdown.yml","text":"Go “Files Changed” copy filename .Rmd file.Open branch submitted PR following steps:\naccess PR branch:\n\nMake sure PR branch checking PR branch name shown (main):\nOpen branch submitted PR following steps:access PR branch:Make sure PR branch checking PR branch name shown (main):Add name new file single quotes followed comma labelled section (eg. Cheatsheets, Tutorials etc).Add name new file single quotes followed comma labelled section (eg. Cheatsheets, Tutorials etc).Save edited version.Save edited version.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"add-part-names-to-.rmd-for-every-first-article-in-part","chapter":"62 Tutorial for pull request mergers","heading":"62.7.3 (Add part names to .Rmd for every first article in part)","text":"adding first chapter PART.One person manage , otherwise hard keep project organized.every first article part, add chapter name top .Rmd file, propose changes. example like .\n","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"add-new-libraries-to-description.","chapter":"62 Tutorial for pull request mergers","heading":"62.7.4 Add new libraries to DESCRIPTION.","text":"Check .Rmd libraries needed. missing, add DESCRIPTION file contributor’s branch, manner edited _bookdown.yml file.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"merge-the-pull-request","chapter":"62 Tutorial for pull request mergers","heading":"62.7.5 Merge the pull request","text":"’re sure things correctly, assign one maintainers @jtr13 review merge PR.Return PR main page repo www.github.com/jtr13/...Return PR main page repo www.github.com/jtr13/...necessary resolve merge conflicts clicking resolve merge conflicts button:necessary resolve merge conflicts clicking resolve merge conflicts button:delete lines <<<<<<< xxxx, ======= >>>>>>>> main edit file desired. Click “Marked resolved” button green “Commit merge” button. –>Click “Merge pull request” “Confirm merge”. Add thank note perhaps emoji :tada:.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"check-actions","chapter":"62 Tutorial for pull request mergers","heading":"62.7.6 Check Actions","text":"minutes, click Actions tabs check whether build successful: green dot indicates successful run, red X indicates failed run.minutes, click Actions tabs check whether build successful: green dot indicates successful run, red X indicates failed run.Check log figure went wrong, can, fix . ’re sure , problem, just open issue linking failed run others can help (important can fix problems quickly). (click revert merge).Check log figure went wrong, can, fix . ’re sure , problem, just open issue linking failed run others can help (important can fix problems quickly). (click revert merge).","code":""}]
