[{"path":"index.html","id":"welcome","chapter":"1 Welcome!","heading":"1 Welcome!","text":"Let’s add content welcome page.Submit pull request .construction","code":""},{"path":"community-contribution.html","id":"community-contribution","chapter":"2 Community Contribution","heading":"2 Community Contribution","text":"fairly open-ended assignment provides opportunity receive credit contributing collective learning class, perhaps beyond. reflect minimum 3 hours work. complete assignment must submit short description contribution. appropriate, attach relevant files.many ways can contribute:organize lead workshop particular topic (date may assignment due date need schedule )help students find final project partnersgive well-rehearsed 5 minute lightning talk class datavis topic (theory tool) (email set date – may assignment due date need schedule )create video tutorial (length)create cheatsheet resourcewrite tutorial tool ’s well documentedbuild viz product (ex. htmlwidget RStudio add-) class use[idea](Note: translations allowed)may draw expand existing resources. , critical cite sources.","code":""},{"path":"community-contribution.html","id":"important-logistics","chapter":"2 Community Contribution","heading":"2.1 IMPORTANT LOGISTICS","text":"","code":""},{"path":"community-contribution.html","id":"groups","chapter":"2 Community Contribution","heading":"2.1.1 Groups","text":"may work partner choosing. work alone, need join group 1, simply submit work CourseWorks solo assignment.work partner, add group CC page People tab. Ed Discussion can used find partners similar interests.","code":""},{"path":"community-contribution.html","id":"what-to-submit","chapter":"2 Community Contribution","heading":"2.1.2 What to submit","text":"cases something tangible upload, tutorial, cheatsheet, etc. Alternatively may submit link material online (YouTube video, etc.) ’s nothing tangible include longer description (see 2.).cases something tangible upload, tutorial, cheatsheet, etc. Alternatively may submit link material online (YouTube video, etc.) ’s nothing tangible include longer description (see 2.).explanation motivation project, need addresses, evaluation project including learned / might differently next time. (1/2 page)explanation motivation project, need addresses, evaluation project including learned / might differently next time. (1/2 page)","code":""},{"path":"community-contribution.html","id":"submitting-your-assignment","chapter":"2 Community Contribution","heading":"2.1.3 Submitting your assignment","text":"must submit assignment twice: CourseWorks (can graded) class, details follow.CourseWorks submission (assignment): submit work .Rmd rendered .pdf .html file, just problem sets. work lend format, write assignment text box .CourseWorks submission (assignment): submit work .Rmd rendered .pdf .html file, just problem sets. work lend format, write assignment text box .Class (GitHub) submission: detail provided separate assignment.Class (GitHub) submission: detail provided separate assignment.","code":""},{"path":"community-contribution.html","id":"grading","chapter":"2 Community Contribution","heading":"2.1.4 Grading","text":"graded quality work, originality, effort invested. sources used must cited.","code":""},{"path":"github-submission-instructions.html","id":"github-submission-instructions","chapter":"3 GitHub submission instructions","heading":"3 GitHub submission instructions","text":"chapter gives information need upload community contribution. Please read entire document carefully making submission. particular note fact bookdown requires different .Rmd format ’re used , must make changes beginning file described submitting.","code":""},{"path":"github-submission-instructions.html","id":"background","chapter":"3 GitHub submission instructions","heading":"3.1 Background","text":"web site makes use bookdown package render collection .Rmd files nicely formatted online book chapters subchapters. job submit slightly modified version community contribution .Rmd file GitHub repository source files web site stored. backend, admins divide chapters book sections order .community contribution different format, create short .Rmd file explains , includes links relevant files, slides, etc. can post GitHub repo (another online site.)","code":""},{"path":"github-submission-instructions.html","id":"preparing-your-.rmd-file","chapter":"3 GitHub submission instructions","heading":"3.2 Preparing your .Rmd file","text":"submit ONE Rmd file.completing modifications, .Rmd look like sample .Rmd.Create concise, descriptive name project. instance, name base_r_ggplot_graph something similar work contrasting/working base R graphics ggplot2 graphics. Check .Rmd filenames file make sure name isn’t already taken. project name words joined underscores, white space. Use .Rmd .rmd. addition, letters must lowercase. Create copy .Rmd file new name.Create concise, descriptive name project. instance, name base_r_ggplot_graph something similar work contrasting/working base R graphics ggplot2 graphics. Check .Rmd filenames file make sure name isn’t already taken. project name words joined underscores, white space. Use .Rmd .rmd. addition, letters must lowercase. Create copy .Rmd file new name.Completely delete YAML header (section top .Rmd includes name, title, date, output, etc.) including --- line.Completely delete YAML header (section top .Rmd includes name, title, date, output, etc.) including --- line.Choose short, descriptive, human readable title project title show table contents – look examples panel left. Capitalize first letter (“sentence case”). first line document, enter single hashtag, followed single whitespace, title. important follow format bookdown renders title header. use single # headers anywhere else document.Choose short, descriptive, human readable title project title show table contents – look examples panel left. Capitalize first letter (“sentence case”). first line document, enter single hashtag, followed single whitespace, title. important follow format bookdown renders title header. use single # headers anywhere else document.second line blank, followed name(s):\n# Base R vs. ggplot2\n\nAaron Burr Alexander Hamilton\n\ncontent starts . second line blank, followed name(s):project requires data, please use built-dataset read directly URL, :\ndf <- readr::read_csv(\"https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv\")  absolutely must include data file, please use small one, many reasons desirable keep repository size small possible.project requires data, please use built-dataset read directly URL, :df <- readr::read_csv(\"https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv\")  absolutely must include data file, please use small one, many reasons desirable keep repository size small possible.included setup chunk .Rmd file, please remember remove label setup chunk, .e., use:\n{r, include=FALSE}\ninstead :\n{r setup, include=FALSE}included setup chunk .Rmd file, please remember remove label setup chunk, .e., use:instead :project requires libraries installed included document, please adhere following conventions. evaluate install.packages() statements document. Consumers .Rmd file won’t want packages get installed knit document. Include library() statements top .Rmd file, title, name, setup, content. chapter requires installation package source (GitHub installation), please add comment identifying . Please mention well PR. example library() section install statements won’t evaluated:\n\n# remotes::install_github(\"twitter/AnomalyDetection\")\nlibrary(\"AnomalyDetection\") # must installed sourceIf project requires libraries installed included document, please adhere following conventions. evaluate install.packages() statements document. Consumers .Rmd file won’t want packages get installed knit document. Include library() statements top .Rmd file, title, name, setup, content. chapter requires installation package source (GitHub installation), please add comment identifying . Please mention well PR. example library() section install statements won’t evaluated:developed .Rmd file moving library() statements rest file content, highly recommended knit review document . may change namespace available section code development, causing function work exhibit unexpected behavior.file contain getwd() / setwd() calls (never use scripts anyway!) write statements.Want get fancy? See optional tweaks section .","code":"# Base R vs. ggplot2\n\nAaron Burr and Alexander Hamilton\n\nYour content starts here. {r, include=FALSE}{r setup, include=FALSE}\n# remotes::install_github(\"twitter/AnomalyDetection\")\nlibrary(\"AnomalyDetection\") # must be installed from source"},{"path":"github-submission-instructions.html","id":"submission-steps","chapter":"3 GitHub submission instructions","heading":"3.3 Submission steps","text":"submit work, following “Workflow #4” – submitting pull request someone else’s repository write access. Instructions available lecture slides topic well tutorial. repeated abbreviated form, specific instructions naming conventions, content information, important details.Fork cc22tt repo (repo) GitHub account.Fork cc22tt repo (repo) GitHub account.Clone/download forked repo local computer.Clone/download forked repo local computer.Create new branch name project name, case sample_project. skip step. merge PR doesn’t come branch. already forgot , check tutorial fix .Create new branch name project name, case sample_project. skip step. merge PR doesn’t come branch. already forgot , check tutorial fix .Copy modified .Rmd file name root directory branch. example, sample_project.Rmd.Copy modified .Rmd file name root directory branch. example, sample_project.Rmd.include .html file. (order bookdown package work, .Rmd files rendered behind scenes.)include .html file. (order bookdown package work, .Rmd files rendered behind scenes.)[OPTIONAL] resources (images) included project, create folder resources/. example, resources/sample_project/. Put resources files . sure change links .Rmd include resources/.../, example:\n![Test Photo](resources/sample_project/pumpkins.jpg)[OPTIONAL] resources (images) included project, create folder resources/. example, resources/sample_project/. Put resources files . sure change links .Rmd include resources/.../, example:![Test Photo](resources/sample_project/pumpkins.jpg)ready submit project, push branch remote repo. Follow tutorial create pull request.ready submit project, push branch remote repo. Follow tutorial create pull request.point back forth begin team managing pull requests. asked make changes, simply make changes local branch, save, commit, push GitHub. new commits added pull request; need , , create new pull request. (, based circumstances, make sense close pull request start new one, tell .)point back forth begin team managing pull requests. asked make changes, simply make changes local branch, save, commit, push GitHub. new commits added pull request; need , , create new pull request. (, based circumstances, make sense close pull request start new one, tell .)pull request merged, ’s fine delete local clone (folder) well forked repository GitHub account.pull request merged, ’s fine delete local clone (folder) well forked repository GitHub account.","code":""},{"path":"github-submission-instructions.html","id":"optional-tweaks","chapter":"3 GitHub submission instructions","heading":"3.4 Optional tweaks","text":"prefer links chapter open new tabs, add {target=\"_blank\"} link, :\n[edav.info](edav.info){target=\"_blank\"}prefer links chapter open new tabs, add {target=\"_blank\"} link, :[edav.info](edav.info){target=\"_blank\"}Note headers (##, ###, etc.) converted numbered headings : ## –> 3.1 ### –> 3.1.1  headings appear chapter subheadings sub-subheadings navigation panel left. Think logical structure users navigate chapter. recommend using ## ### headings since “sub-sub-subheadings” 4.1.3.4 generally unnecessary look messy.Note headers (##, ###, etc.) converted numbered headings : ## –> 3.1 ### –> 3.1.1  headings appear chapter subheadings sub-subheadings navigation panel left. Think logical structure users navigate chapter. recommend using ## ### headings since “sub-sub-subheadings” 4.1.3.4 generally unnecessary look messy.Unfortunately, ’s simple way preview chapter ’s actually merged project. (bookdown preview_chapter() option works entire book rendered least become complex require packages project grows.) really want preview , fork clone minimal bookdown repo, add .Rmd file, click “Build book” button Build tab (next Git), open .html files _book folder web browser see rendered book.  ’re interested bookdown options, see official reference book.  useful tweaks share? Submit issue PR.Unfortunately, ’s simple way preview chapter ’s actually merged project. (bookdown preview_chapter() option works entire book rendered least become complex require packages project grows.) really want preview , fork clone minimal bookdown repo, add .Rmd file, click “Build book” button Build tab (next Git), open .html files _book folder web browser see rendered book.  ’re interested bookdown options, see official reference book.  useful tweaks share? Submit issue PR.","code":""},{"path":"github-submission-instructions.html","id":"faq","chapter":"3 GitHub submission instructions","heading":"3.5 FAQ","text":"","code":""},{"path":"github-submission-instructions.html","id":"what-should-i-expect-after-creating-a-pull-request","chapter":"3 GitHub submission instructions","heading":"3.5.1 What should I expect after creating a pull request?","text":"Within week create pull request, apply label assign classmate “PR merger” review files submit see meet requirements.Within week create pull request, apply label assign classmate “PR merger” review files submit see meet requirements.take time can process pull requests, long see pull request repo, don’t worry.take time can process pull requests, long see pull request repo, don’t worry.PR merger contacts regarding pull request, usually means files fail meet requirements. explain wrong, please fix soon possible.PR merger contacts regarding pull request, usually means files fail meet requirements. explain wrong, please fix soon possible.","code":""},{"path":"github-submission-instructions.html","id":"what-if-i-catch-mistakes-before-my-pull-request-is-merged","chapter":"3 GitHub submission instructions","heading":"3.5.2 What if I catch mistakes before my pull request is merged?","text":"Just make changes branch, commit push GitHub. automatically added pull request.","code":""},{"path":"github-submission-instructions.html","id":"what-if-i-catch-mistakes-after-my-pull-request-is-merged","chapter":"3 GitHub submission instructions","heading":"3.5.3 What if I catch mistakes after my pull request is merged?","text":"may submit additional pull requests fix material site. edits small, fixing typos, easiest make edits directly GitHub, following instructions. merge first pull requests edits, please patient.","code":""},{"path":"github-submission-instructions.html","id":"other-questions","chapter":"3 GitHub submission instructions","heading":"3.5.4 Other questions","text":"additional questions, please ask Discussions section respond.Thank contributions!","code":""},{"path":"sample-project.html","id":"sample-project","chapter":"4 Sample project","heading":"4 Sample project","text":"Joe Biden Donald TrumpThis chapter gives sample layout Rmd file.Test Photo","code":""},{"path":"r-window-functions-cheatsheet.html","id":"r-window-functions-cheatsheet","chapter":"6 R window functions cheatsheet","heading":"6 R window functions cheatsheet","text":"Gokul Sunilkumar Pooja SrinivasanWhat window function?window function performs calculation across set table rows somehow related current row. Although functionalities might sound similar aggregate functions, window functions cause rows become grouped single output row like non-window aggregate calls . Instead, rows retain separate identities. Behind scenes, window function able access just current row query result.Although usage window functions common database systems related applications, single source lookup help ease developers’ work programming. Hence, come cheatsheet .Check cheatsheet : https://github.com/gokul-sunilkumar/RWindowFunctions/blob/main/RWindowFunctionsCheatSheet.pdf","code":""},{"path":"rmd-chunk-option-cheat-sheet.html","id":"rmd-chunk-option-cheat-sheet","chapter":"7 Rmd chunk option cheat sheet","heading":"7 Rmd chunk option cheat sheet","text":"Yunchen JiangMotivation ContributionAs graduate student undergraduate degree actuarial mathematics, course almost first exposure Rstudio. received Pset, first thing confused code chunk r markdown. project, contribute making condensed cheat sheet commonly used chunk options refine understanding rmd files simultaneously help r beginners similar situation .Cheat sheetChunk options written chunk headers form tag=value like :\n{r -chunk, echo=FALSE, fig.height=4, dev=‘jpeg’}\n…special chunk option chunk label (e.g., -chunk example). chunk label need tag. prefer form tag=value, also use chunk option label explicitly:\n{r, label=‘-chunk’, echo=FALSE, fig.height=4, dev=‘jpeg’}\n…can also write chunk options body code chunk #| like :\n{r}\n#| -chunk, echo = FALSE,\n#| fig.height=4, dev=‘jpeg’\n…syntax, chunk options must written continuous lines beginning chunk body. can break options onto many lines wish lines must start special comment prefix #|can also use YAML syntax write options inside chunk form tag: value. Normally provide one option per line like :\n{r}\n#| echo: false\n#| fig.width: 4\n#| dev: ‘jpeg’\n…chunk label chunk assumed unique within document. especially important cache plot filenames, filenames based chunk labels.list commonly used chunk options knitr documented format “option: (default value; type value)“.Code evaluation:eval: (TRUE; logical numeric) Whether evaluate code chunk. can also numeric vector choose R expression(s) evaluate.\ne.g., eval = c(1, 3, 4) evaluate first, third, fourth expressions, eval = -(4:5) evaluate expressions except fourth fifth.Text output:echo: (TRUE; logical numeric) Whether display source code output document. Besides TRUE/FALSE, shows/hides source code, can also use numeric vector choose R expression(s) echo chunk.\ne.g., echo = 2:3 means echo 2nd 3rd expressions, echo = -4 means exclude 4th expression.results: (‘markup’) Mark text output appropriate environments depending output format. example, R Markdown, text output character string “[1] 1 2 3”, actual output knitr produces :case, results=‘markup’ means put text output fenced code blocks.warning: (TRUE; logical) Whether preserve warnings (produced warning()) output. FALSE, warnings printed console instead output document:\n{r}\nwithCallingHandlers(\nexpr = .numeric(c(“1”, “”)),\nwarning = function(w) warn <<- paste(“** warning:”, w$message, “**“)\n)\nWarning: NAs introduced coercion[1] 1 NA{r, warning = false}\nwithCallingHandlers(\nexpr = .numeric(c(“1”, “”)),\nwarning = function(w) warn <<- paste(“** warning:”, w$message, “**“)\n)\n[1] 1 NAIt can also take numeric values indices select subset warnings include output. Note values reference indices warnings (e.g., 3 means “third warning thrown chunk”) indices expressions allowed emit warnings.error: (True; logical) Whether preserve errors (stop()). default, errors code chunks Rmd document halt R. want show errors without stopping R, may use chunk option error = TRUE:\n{r,error = TRUE}\n1 + “”\nsee error message output document compile Rmd document: Error 1 + “”: non-numeric argument binary operator. R Markdown, error = FALSE default, means R stop error running code chunks.include: (TRUE; logical) Whether include chunk output output document. FALSE, nothing written output document, code still evaluated plot files generated plots chunk, can manually insert figures later.Code decoration:comment: (‘##’; character) prefix added line text output. default, text output commented ##, readers want copy run source code output document, can select copy everything chunk, since text output masked comments (ignored running copied text). Set comment = ’’ remove default ##.prompt: (FALSE; logical) Whether add prompt characters R code. TRUE, knitr add > start line code displayed final document. Note adding prompts can make difficult readers copy R code output, prompt = FALSE may better choice.highlight: (TRUE) Whether syntax highlight source code.Cache:cache: (FALSE; logical) Whether cache code chunk. caching turned via chunk option cache = TRUE, knitr write R objects generated code chunk cache database, can reloaded next time. evaluating code chunks second time, cached chunks skipped (unless modified), objects created chunks loaded previously saved databases (.rdb .rdx files), files saved chunk evaluated first time, cached files found (e.g., may removed hand).cache.path: (‘cache/’; character) prefix used generate paths cache files. R Markdown, default value based input filename, e.g., cache paths chunk label FOO file INPUT.Rmd form INPUT_cache/FOO_..cache.lazy: (TRUE; logical) Whether lazyLoad() directly load() objects. large objects, lazyloading may work, cache.lazy = FALSE may desirabledependson: (NULL; character numeric) character vector chunk labels specify chunks chunk depends . option applies cached chunks —sometimes objects cached chunk may depend cached chunks, chunks changed, chunk must updated accordingly. dependson numeric vector, means indices chunk labels, e.g., dependson = 1 means chunk depends first chunk document, dependson = c(-1, -2) means depends previous two chunks (negative indices stand numbers chunks chunk, note always relative current chunk).Plots:fig.width, fig.height: (7; numeric) Width height plot (inches), used graphics device.fig.ext: (NULL; character) File extension figure output. NULL, derived graphical device; see knitr:::auto_exts details.fig.asp: (NULL; numeric) aspect ratio plot, .e., ratio height/width. fig.asp specified, height plot (chunk option fig.height) calculated fig.width * fig.asp.fig.dim: (NULL; numeric) numeric vector length 2 provide fig.width fig.height, e.g., fig.dim = c(5, 7) shorthand fig.width = 5, fig.height = 7. fig.asp fig.dim provided, fig.asp ignored (warning).fig.align: (‘default’; character) Alignment figures output document. Possible values default, left, right, center. default make alignment adjustments.fig.path: (‘figure/’; character) prefix used generate figure file paths. fig.path chunk labels concatenated generate full paths. may contain directory like figure/prefix-; directory created exist.fig.show: (‘asis’; character) show/arrange plots. Possible values follows:asis: ‘hide’, knitr generate plots created chunk, include final document. ‘hold’, knitr delay displaying plots created chunk end chunk. ‘animate’, knitr combine plots created chunk animation.","code":"[1] 1 2 3"},{"path":"helpful-ggplot2-extensions-cheatsheet.html","id":"helpful-ggplot2-extensions-cheatsheet","chapter":"8 Helpful ggplot2 Extensions Cheatsheet","heading":"8 Helpful ggplot2 Extensions Cheatsheet","text":"Frank Li Liang ZhuangCurrently, total 117 ggplot2 extensions extensions add additional features ggplot2 make powerful. cheatsheet includes useful information four ggplot2 extensions: ggbump, ggradar, ggpol, treemapify.ggbump: creates varies bump charts ggplot. Bump charts good plot path nodes statistical significance.ggradar: creates radar charts. Radar charts useful data values multiple common variables widely used performance analysis.ggpol: adds additional features ggplot2 including GeomArcbar, GeomParliament, GeomCircle, GeomTshignlight, FacetShare, GeomBartext, GeomBoxjitter. details : https://erocoar.github.io/ggpol/.treemapify: creates tree maps ggplot2. useful \ndata hierarchy, country GDP company’s stock market share.extensions, use link: https://exts.ggplot2.tidyverse.org/gallery/cheatsheet available : https://github.com/leolisticierism/EDAV_Community_Contribution/blob/main/EDAV%20Community%20Contribution%20Cheat%20Sheet.pdf","code":""},{"path":"commonly-used-graph-cheatcheet.html","id":"commonly-used-graph-cheatcheet","chapter":"9 Commonly-used graph cheatcheet","heading":"9 Commonly-used graph cheatcheet","text":"Wangtao Zheng, UNI:wz2618During R coding exercises EDAV homework, realized necessity creating R code library/cheatsheet include codes used draw commonly-applied graphs data visualization. way, one need memorize code used drawing different graphs, saves time.R codes used draw commonly-used graphs thus included cheatsheet. Graphs useful data visualization projects, scatterplot, boxplot, bar plot, histogram, etc., included cheatsheet. also included coding methods various modifications graphs convenience reasons, cheatsheet can help members community different circumstances.can find cheatsheet :https://github.com/zwt950715/EDAV-Fall-2022-Community-Contribution/blob/main/%20Useful%20Cheatsheet%20For%20Data%20Visualization.pdfThis cheatsheet covers materials first two problem sets EDAV course fall 2022. Thus, types graphs coding method can added cheatsheet make useful R programming EDAV purposes. Hope cheatsheet can help learning, looking forward valuable advices!","code":""},{"path":"git-and-version-control.html","id":"git-and-version-control","chapter":"10 Git and version control","heading":"10 Git and version control","text":"Dianjing Fan Yijia HeVersion control one essential techniques programmers. Version control Git helpful programmers software teams manage changes codes time. allows developers collaboratively work faster, smarter, efficiently reducing development time communication costs. can keep track every modification codes contributor without conflicting concurrent work. addition, knowledge practice version control Git usually required programmers workplace setting. time, well taught school courses explicitly mentioned job description cases. Therefore, create cheat sheet Git beginners learn understand Git within short time. project, gained deeper understanding version control Git terms use Git, terminology, , practiced must-know commands used Git. might add real-world examples using Git commands, record short tutorial video beginners, include knowledge Github next time.can find cheatsheet :\nhttps://github.com/BrownSugarBobaMilkTea/edav_git_cheatsheet/blob/main/git_cheatsheet.pdf","code":""},{"path":"useful-plots-for-bioinfomatics-in-r.html","id":"useful-plots-for-bioinfomatics-in-r","chapter":"11 Useful plots for Bioinfomatics in R","heading":"11 Useful plots for Bioinfomatics in R","text":"Xiangming HuangThis cheat sheet includes simple ways create survival curve, heatmap, volcano plot. plots useful visualization statistical analysis Bioinformatics. Many R packages specialized Biological analysis available free. packages avoided cheatsheet possible help readers understand data processed plots constructed elementary way.link: https://github.com/xmh3698/plotsForBio/blob/main/plotsForBio.pdf","code":""},{"path":"different-ways-to-create-world-map-in-r.html","id":"different-ways-to-create-world-map-in-r","chapter":"12 Different ways to create world map in R","heading":"12 Different ways to create world map in R","text":"Yuhang Qiu Yinqi Wang","code":""},{"path":"different-ways-to-create-world-map-in-r.html","id":"explanation-of-motivation","chapter":"12 Different ways to create world map in R","heading":"12.1 Explanation of Motivation","text":"Plotting world map good visualization strategy someone wants analyze data country country, even continent continent. Using world map also helps us terms visualizing locations distances (countries, cities, etc.). Making world map also good geographic researches. examples use world map : Gini around world, countries developed undeveloped. lot topics world map can visualize. community contribution project, ’ve learned make world map (different style ways) using multiple packages “rnaturalearth” “rworldmap” got refreshing thorough understanding packages methods. might differently next time trying record video tutorial packages methods ’ve introduced discussed cheatsheet.link cheat sheet :https://github.com/a844574798/worldMapCheatSheet/blob/main/WorldMapCheatsheet.pdf","code":""},{"path":"color-in-r-python-and-jshtmlcss.html","id":"color-in-r-python-and-jshtmlcss","chapter":"13 Color in R, Python, and JS/HTML/CSS","heading":"13 Color in R, Python, and JS/HTML/CSS","text":"Shujie HuColor choice matters. Either simple visualization user interface design, good color selection easiest way achieve incredible results. right color selection supports better information readability enhances viewer navigation capabilities. can also fulfill subconscious aesthetic user needs even stimulate intuitive interactions. User interface design, color central part online marketing strategy. Researchers various fields investigating impacts color brought viewer, numerous amount color palettes created programming languages, matter programming language use, always can find plenty packages use.However, don’t know name specific color palette, ’s hard even look internet. Therefore, motivation project collect beautiful color palettes visualization languages used far. cheatsheet addresses need find good color scheme presenting data students may encounter visualization projects, either R, Python, JS/HTML/CSS. also serves brief tutorial using package. presenting colors name, easy locate colors want help compare colors package, thereby saving time browsing internet.evaluation project, think cheatsheet serves purpose well. ’ve learned many color packages. color packages picked ones think beautiful cover almost every possible colors presented cheatsheet. also learned hexadecimal triplets represent colors web programming languages, found several useful resources may need future projects. cheasheet presents part research done, since order create cheatsheet need know lot able summarize findings briefly.cheatsheet available :\nhttps://github.com/tracyhsj/Color/blob/main/Colors%20palettes%20in%20R%2C%20Python%2C%20and%20HTML:CSS%20.pdfTest Photo","code":""},{"path":"color-palettes-for-data-visualization.html","id":"color-palettes-for-data-visualization","chapter":"14 Color Palettes for Data Visualization","heading":"14 Color Palettes for Data Visualization","text":"Zixiao Zhang Yingyi Zhu","code":""},{"path":"color-palettes-for-data-visualization.html","id":"explanation-of-the-motivation","chapter":"14 Color Palettes for Data Visualization","heading":"14.1 Explanation of the motivation","text":"Data visualization mainly uses graphics clearly effectively present results data analysis. addition choosing right plot express trends relationships data, choosing suitable color scheme also significant. color represents unique message, color combination can show data type difference categories. Good use color can reduce level understanding data trying convey help viewer understand meaning data intuitively. Poor use color can opposite effect. example, many colors can disrupt viewer’s attention prevent understanding true meaning plot.set cheat sheet provide users color-matching tips introduce packages can use R help color matching. community contribution project, offer several color schemes. process, learned colors appear scheme. learn use colors right situations. might creates video tutorial picking right color next time.Check cheatsheet :https://github.com/XIXXII/colorPalettesCheatSheet/blob/master/colorPalattesForDV.pdf","code":""},{"path":"data-visualization-using-seaborn.html","id":"data-visualization-using-seaborn","chapter":"15 Data visualization using Seaborn","heading":"15 Data visualization using Seaborn","text":"Sida Huang Longxiang ZhangVisualization data can help data scientists perform exploratory analysis data efficiently. taking EDAV course, learned exploratory data visualization analysis using R language. However, considering many students familiar Python language, decided present content EDAV course Python.main content community contribution tutorial exploratory data visualization analysis using Python third-party library seaborn. designed Community Contribution based 9-th 13-rd chapters edav information website.detailed tutorials can found : https://github.com/amethystorm/EDAV_CC20.","code":""},{"path":"machine-learning-cheatsheet-for-r-and-python.html","id":"machine-learning-cheatsheet-for-r-and-python","chapter":"16 Machine Learning Cheatsheet for R and Python","heading":"16 Machine Learning Cheatsheet for R and Python","text":"Jingyi FengThe popular languages training machine learning models R Python (extensive packages resources). community contribution, build cheat sheet data pre-processing, machine learning, simple data visualization R Python. structure cheat sheet follows:Data processing (import file, scaling, train test split)Supervised learning techniques (regression models, classification models)Unsupervised learning techniques (PCA, K-Means)Evaluating model performance (RMSE, R2, accuracy, confusion matrix, ROC-AUC, F1-score)Preliminary data visualization (ggplot2, seaborn)cheat sheet available : https://github.com/jenniferfjy/Community_Contribution/blob/main/MLCheatSheet.pdf","code":""},{"path":"data-visualization-with-r-ggplot2-vs.-matlab.html","id":"data-visualization-with-r-ggplot2-vs.-matlab","chapter":"17 Data Visualization with R ggplot2 vs. Matlab","heading":"17 Data Visualization with R ggplot2 vs. Matlab","text":"Yuxin LinMatlab popular programming language area mathematical computation calculus, linear algebra, matrix manipulation.[1] Also, similar R, Matlab powerful tool data analytics, machine learning, data visualization. However, common application domain, tutorials plotting Matlab introduce deal simple simulated array inputs rather data set files.Motivated assignment analyzing auditory fMRI data Matlab applied machine learning course, cheatsheet created beginners Matlab programming. cheat sheet provides codes plotting graphs highly frequently used process data exploration results visualization, language R package ggplot2 Matlab. Although codes plotting Matlab appears trivial, noted Matlab differs R programming language functions, like use histograms bar plots. side--side contrast, cheat sheet expected help people experience R familiar Matlab plotting smooth start.Link: https://github.com/linlinlin-yx/R-ggplot2-vs-Matlab-Plot/blob/main/r_vs_matlab.pdf","code":""},{"path":"data-visualization-with-r-ggplot2-vs.-matlab.html","id":"reference","chapter":"17 Data Visualization with R ggplot2 vs. Matlab","heading":"17.0.1 Reference","text":"[1] https://www.geeksforgeeks.org/differences--matlab--r-programming-language/[2] https://www.mathworks.com/help/matlab/creating_plots/types--matlab-plots.html[3] https://www.mathworks.com/help/matlab/matlab_prog/access-data---table.html[4] ggplot2 cheatsheet (https://ggplot2.tidyverse.org/#cheatsheet)[5] edav.info (https://edav.info/spatial-data.html)","code":""},{"path":"preprocessing-and-visualization-of-time-series-data.html","id":"preprocessing-and-visualization-of-time-series-data","chapter":"19 Preprocessing and Visualization of Time Series Data","heading":"19 Preprocessing and Visualization of Time Series Data","text":"Siyuan DingIn tutorial, learn visualize time series data. reaching time series data, always organized want, need preprocessing first visualize . use time series dataset Covid-19 vaccination 2020-12-14 2022-10-30 example tutorial, dataset available https://raw.githubusercontent.com/govex/COVID-19/master/data_tables/vaccine_data/us_data/time_series/time_series_covid19_vaccine_doses_admin_US.csv goal visualize relationship vaccinated doses state 2022, 2022-01-01.use three packages tutorial: dplyr, lubridate tidyr manipulate dataset preprocessing, ggplot2 visualization.","code":"\n# The packages can be installed by command: install.packages()\nlibrary(dplyr) \nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(tidyr)"},{"path":"preprocessing-and-visualization-of-time-series-data.html","id":"preprocess-on-a-time-series-data","chapter":"19 Preprocessing and Visualization of Time Series Data","heading":"19.1 Preprocess on a Time Series Data","text":"part, first work transforming original data downloaded online data frame work . look data see whether problem data points discipline Time Series Data. Finally deal missing values. preprocessings done, data good visualize analyze.","code":""},{"path":"preprocessing-and-visualization-of-time-series-data.html","id":"transform-dataset","chapter":"19 Preprocessing and Visualization of Time Series Data","heading":"19.1.1 Transform DataSet","text":"first load get overall look dataset, find data 61 rows 698 columns. visualize relationship vaccinated doses state, need three things: state name, state population state vaccination population day. achieve , can use select dplyr package r. focus data 2022 example .Now, table 305 columns, make dataframe works better, woule like four columns Province_State, Population, Date, Vaccination_Doses Date comes colnames table Vaccination_Doses number population state date comes original entries date columns. achieve , can temporarily ignore Population column since identical state, can just full join later work vaccination population first. duplicate state name number dates times, build matrix store value vaccination dose state day extracting original dataset using subset dplyr transform dataframe putting together.Now transformed dataframe vaccinated doses, can full join Population state. province available population data, deal , can use drop_na function tidyr. functino let us drop rows according column na values.time, got transformed dataframe information needed, remember, time series dataset, need careful Date column! must check whether entries date type need sort date visualization date type like string.found data type date ! Now, need transform date type using lubridate package. lubridate, can manipulate dates easily, many functions packages. string daymonthyear, example “12032000”, can use dmy() return date format “2000-03-12”; Similarly, string monthdayyear format, can use mdy(), can use ymd() data yearmonthday format. Just remember m represents month, d represents day, y represents year able find correct function need. example case, entries Date now string yearmonthday format, use ymd() . use function, first need remove ‘X’ character beginning date, can achieve substring function.can double check data type, find date format now. dataframe transformed good use.","code":"\n# load data\nvaccination_all <- read.csv(\"https://raw.githubusercontent.com/govex/COVID-19/master/data_tables/vaccine_data/us_data/time_series/time_series_covid19_vaccine_doses_admin_US.csv\")\ncolnames(vaccination_all)[1:15]##  [1] \"UID\"            \"iso2\"           \"iso3\"           \"code3\"         \n##  [5] \"FIPS\"           \"Admin2\"         \"Province_State\" \"Country_Region\"\n##  [9] \"Lat\"            \"Long_\"          \"Combined_Key\"   \"Population\"    \n## [13] \"X2020.12.14\"    \"X2020.12.15\"    \"X2020.12.16\"\ncolnames(vaccination_all)[(ncol(vaccination_all)-5):ncol(vaccination_all)]## [1] \"X2022.11.07\" \"X2022.11.08\" \"X2022.11.09\" \"X2022.11.10\" \"X2022.11.11\"\n## [6] \"X2022.11.12\"\nvaccination_df <- vaccination_all %>% select(Province_State, Population, `X2022.01.01`:tail(names(vaccination_all),1))\ncolnames(vaccination_df)[1:15]##  [1] \"Province_State\" \"Population\"     \"X2022.01.01\"    \"X2022.01.02\"   \n##  [5] \"X2022.01.03\"    \"X2022.01.04\"    \"X2022.01.05\"    \"X2022.01.06\"   \n##  [9] \"X2022.01.07\"    \"X2022.01.08\"    \"X2022.01.09\"    \"X2022.01.10\"   \n## [13] \"X2022.01.11\"    \"X2022.01.12\"    \"X2022.01.13\"\ncolnames(vaccination_df)[(ncol(vaccination_df)-5):ncol(vaccination_df)]## [1] \"X2022.11.07\" \"X2022.11.08\" \"X2022.11.09\" \"X2022.11.10\" \"X2022.11.11\"\n## [6] \"X2022.11.12\"\nvaccination_df <- vaccination_df[,-2]\n# Find number of States\nstate <- vaccination_df$Province_State\nState_num <- length(state)\n# Find number of Days\ndate <- colnames(vaccination_df)[c(-1)]\ntime_window <- dim(vaccination_df)[2]-1\n# Then in the transformed dataframe, each state should occur for time_window times\nState <- rep(state, each = time_window)\n# Then in the transformed dataframe, each date should occur for State_num times\nDate <- rep(date, State_num)\n# Build a matrix to contain the vaccination doses\nvac_matrix <- matrix()\n# We select the daily vaccination doses for each state and store them in a Matrix\nfor (i in 1:State_num){\n  vac_matrix <- rbind(vac_matrix,matrix(unlist(vaccination_df%>%subset(Province_State==state[i]))[-1]))\n}\n# Drop the first column, which is NA\nvac_matrix <- vac_matrix[2:length(vac_matrix)]\n# Get the transformed dataset\nVac_DF <- cbind.data.frame(State, Date, vac_matrix)\ncolnames(Vac_DF) <- c(\"State\", \"Date\", \"Vaccinated Doses\")\nVac_DF$`Vaccinated Doses` = as.numeric(Vac_DF$`Vaccinated Doses`)\nhead(Vac_DF)##     State        Date Vaccinated Doses\n## 1 Alabama X2022.01.01          5624234\n## 2 Alabama X2022.01.02          5624234\n## 3 Alabama X2022.01.03          5624234\n## 4 Alabama X2022.01.04          5678299\n## 5 Alabama X2022.01.05          5681793\n## 6 Alabama X2022.01.06          5695747\nstate_pop <- vaccination_all %>% select(Province_State, Population)\nVac_DF <- Vac_DF %>% full_join(state_pop, by = c (\"State\" = \"Province_State\"))\nVac_DF <- Vac_DF %>% drop_na(Population)\nhead(Vac_DF)##     State        Date Vaccinated Doses Population\n## 1 Alabama X2022.01.01          5624234    4903185\n## 2 Alabama X2022.01.02          5624234    4903185\n## 3 Alabama X2022.01.03          5624234    4903185\n## 4 Alabama X2022.01.04          5678299    4903185\n## 5 Alabama X2022.01.05          5681793    4903185\n## 6 Alabama X2022.01.06          5695747    4903185\nstate <- unique(Vac_DF$State)\nState_num <- length(state)\ndate <- unique(Vac_DF$Date)\ntime_window <- length(date)\n# Check data type of the Date column\nclass(Vac_DF$Date)## [1] \"character\"\n# We first remove the 'X' before the date\nVac_DF$Date <- substring(Vac_DF$Date,2)\nVac_DF$Date <- ymd(Vac_DF$Date )\nhead(Vac_DF)##     State       Date Vaccinated Doses Population\n## 1 Alabama 2022-01-01          5624234    4903185\n## 2 Alabama 2022-01-02          5624234    4903185\n## 3 Alabama 2022-01-03          5624234    4903185\n## 4 Alabama 2022-01-04          5678299    4903185\n## 5 Alabama 2022-01-05          5681793    4903185\n## 6 Alabama 2022-01-06          5695747    4903185\nclass(Vac_DF$Date)## [1] \"Date\""},{"path":"preprocessing-and-visualization-of-time-series-data.html","id":"deal-with-missing-values","chapter":"19 Preprocessing and Visualization of Time Series Data","heading":"19.1.2 Deal with Missing Values","text":"moving , first check missing values.found missing values, need worry missing values case.missing values example case, two choices, either impute missing values remove . make future analysis accurate, better try impute . Since data time series data, vaccinated doses day 1 greater doses day 2, conversely, vaccinated doses day 2 less doses day 1, impute missing value closest non missing value, achieve fill function tidyr package. function impute missing value previous next value, argument .direction can define direction impute.Since missing values, can move deal problematic data.","code":"\nsum(is.na(Vac_DF$`Vaccinated Doses`))## [1] 0"},{"path":"preprocessing-and-visualization-of-time-series-data.html","id":"deal-with-problematic-data-points","chapter":"19 Preprocessing and Visualization of Time Series Data","heading":"19.1.3 Deal with Problematic Data Points","text":"moving visualization analysis, need careful whether data cumulative ! data cumulative, value monotonically, example case, since data vaccination doses state day, vaccinated doses day 2 less value day 1. need check whether data day 2 smaller value day 1, assign value day 2 value day 1.Now finished preprocessings. brief conclusion, find dataset online, first load transform usable dataframe. usually requires functions dplyr working time series data, also need functions lubridate make date date type variable instead character can order later visualization. imputing missing values, tidyr popular package use since impute missing value value nearest date. need look data points, deal problematic data points according dataset , example case, working vaccinated doses along time, must monotonically increasing variable. finishing steps, move visualizing data.","code":"\nfor (i in 1:length(state)){\n  for (j in 2:time_window){\n    if ((Vac_DF %>% subset(State == state[i]) %>% select(`Vaccinated Doses`))[j,] < \n        (Vac_DF %>% subset(State == state[i]) %>% select(`Vaccinated Doses`))[j-1,]){\n      Vac_DF[\"Vaccinated Doses\"][Vac_DF[\"State\"] == state[i]][j] <- Vac_DF[\"Vaccinated Doses\"][Vac_DF[\"State\"] == state[i]][j-1]\n    }\n  }\n}"},{"path":"preprocessing-and-visualization-of-time-series-data.html","id":"visualize-a-time-series-data","chapter":"19 Preprocessing and Visualization of Time Series Data","heading":"19.2 Visualize a Time Series Data","text":"visualize time series data, mainly focus trend. want see data changes time, like geom_line ggplot2 show changes time state.graph give us much informaiton tell state, make graph varries color according states command color = State.graph looks much better now can find line corresponded state color. can easily find state highest vaccinated doses California. since California large vaccinated doses, range plot great make trend clear. may want check states’ population. get deeper insight, can visualize population state using geom_bar.bar plot give much insights without ordering, order easily find populatioin order state, can order plot using factor().ordering barplot, find California highest population among states result vaccinated doses greatest amount California give much information California great population . order get deeper insights, can visualize vaccinated doses rate, divide vaccinated doses population. Since like see trends time difference state, use geom_line() color state.Now can get better look find District Columbia highest vaccinated doses rate proves important us look vaccinated doses rate instead absolute vaccinated doses since population affect results.Now got great visualization time series data.","code":"\nggplot(data = Vac_DF, mapping = aes(x = Date, y = `Vaccinated Doses`, group = State)) +\n  geom_line() +\n  ggtitle(\"Vaccinated Doses V.S. Date\")\nggplot(data = Vac_DF, mapping = aes(x = Date, y = `Vaccinated Doses`, group = State, color = State)) +\n  geom_line() +\n  ggtitle(\"Vaccinated Doses V.S. Date\")\nggplot(data = Vac_DF, mapping = aes(x= Population, y = State)) + \n  geom_bar(stat=\"identity\")\norder <- unique(Vac_DF %>% select(Population, State) %>% arrange(Population) %>% mutate(State = factor(State)))\nPop_Order_DF <- Vac_DF %>% mutate(State = factor(State, levels = order$State, ordered = TRUE))\nggplot(data = Pop_Order_DF, mapping = aes(x= Population, y = State)) + \n  geom_bar(stat=\"identity\")\nVac_Rate_DF <- Vac_DF %>% mutate(`Vaccinated Doses Rate` = `Vaccinated Doses`/Population)\n\nggplot(data = Vac_Rate_DF, mapping = aes(x = Date, y = `Vaccinated Doses Rate`, group = State, color = State)) +\n  geom_line() +\n  ggtitle(\"Vaccinated Doses Rate V.S. Date\")"},{"path":"preprocessing-and-visualization-of-time-series-data.html","id":"conclusion","chapter":"19 Preprocessing and Visualization of Time Series Data","heading":"19.3 Conclusion","text":"conclusion, dealing time series data, need spend time preprocessing, including transforming date usable dataframe using dplyr tidyrpackage, dealing missing values, problematic data. transforming data, important watch date type! need date class instead character, can order visualizing, date format can converted functions lubridate package. finish theses preprocessing works, can start visualization part using ggplot2, try remove potential affecting factors order get clear look data trend time.","code":""},{"path":"how-to-use-sqldf.html","id":"how-to-use-sqldf","chapter":"20 How to use sqldf","heading":"20 How to use sqldf","text":"Conor Ryan","code":"\nlibrary(sqldf)\nlibrary(tidyverse)"},{"path":"how-to-use-sqldf.html","id":"motivation","chapter":"20 How to use sqldf","heading":"20.1 Motivation","text":"sqldf library lets work dataframes database tables, can query whatever SQL-style manipulation want, without worrying logistics managing databases. can useful various dataframe manipulations, often need preparing data visualization.reasons thought library use tutorial:Github page kind mess, official CRAN documentation particularly user-friendly.pretty cool tool think : overhead extra work, can just call SQL dataframe.option use SQL incredibly useful dealing working many languages. need quick R visualization just working Python, might easier just manipulate data via SQL rather figure exact R syntax thing.Certain data manipulation just suited SQL syntax, like complicated left joins window functions.scale data gets large memory, library offers impressive advantages. Even can load large dataset memory, slow; way faster initial manipulation (like filtering data 100-fold) library, reasonable deal resulting dataframe.approach significant improvement something like dbplyr knitr SQL engine. approach still requires manual management connections tables. Additionally, knitr hardly suited non-report-style work R. sqldf usable wider variety scenarios.","code":""},{"path":"how-to-use-sqldf.html","id":"usage","chapter":"20 How to use sqldf","heading":"20.2 Usage","text":"","code":""},{"path":"how-to-use-sqldf.html","id":"basics","chapter":"20 How to use sqldf","heading":"20.2.1 Basics","text":"’ve installed sqldf, really easy loading library writing SQL:create database, load data table, cleanup table. package handled behind scenes.can realistic, basic manipulation. R might :SQL can :","code":"\nsqldf('select * from iris') |> head()##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\niris |>\n  filter(Petal.Length > 2.0) |>\n  mutate(Sepal_Product = Sepal.Length * Sepal.Width) |>\n  group_by(Species) |>\n  summarize(mean_sepal_product=mean(Sepal_Product)) |>\n  head()## # A tibble: 2 × 2\n##   Species    mean_sepal_product\n##   <fct>                   <dbl>\n## 1 versicolor               16.5\n## 2 virginica                19.7\nsqldf('\n  select Species, avg(`Sepal.Length` * `Sepal.Width`) as mean_sepal_product\n  from iris\n  where `Petal.Length` > 2.0\n  group by 1\n') |>\n  head()##      Species mean_sepal_product\n## 1 versicolor            16.5262\n## 2  virginica            19.6846"},{"path":"how-to-use-sqldf.html","id":"more-advanced-sql-tasks","chapter":"20 How to use sqldf","heading":"20.2.2 More advanced SQL tasks","text":"library becomes powerful use things SQL uniquely good . example, although simple join matching column condition relatively easy R (Python), following sort condition annoying accomplish:Similarly, window functions become far accessible package:unique tasks might even preferable just use SQL rather R dataframe manipulation. fine; every tool can everything prefectly – SQL excels specific things.","code":"\nsqldf('\n  select a.Species, b.Species, avg(a.`Sepal.Width`) as `a.Width.Avg`\n  from iris a\n  join iris b\n    on a.species != b.species\n    and a.`Sepal.Length` > b.`Sepal.Length`\n    and a.`Sepal.Width` < b.`Sepal.Width`\n  group by 1,2\n') |>\n  head()##      Species    Species a.Width.Avg\n## 1 versicolor     setosa    2.764037\n## 2 versicolor  virginica    2.738889\n## 3  virginica     setosa    2.901931\n## 4  virginica versicolor    2.737284\nsqldf('\n  select Species, avg(`Sepal.Length`) over (partition by Species order by `Sepal.Length` desc rows between unbounded preceding and current row) as running_mean\n  from iris\n') |>\n  head()##   Species running_mean\n## 1  setosa     5.800000\n## 2  setosa     5.750000\n## 3  setosa     5.733333\n## 4  setosa     5.675000\n## 5  setosa     5.640000\n## 6  setosa     5.600000"},{"path":"how-to-use-sqldf.html","id":"alternate-data-sources","chapter":"20 How to use sqldf","heading":"20.2.3 Alternate data sources","text":"also don’t already dataframe -memory use library. Suppose iris .csv machine:wanted immediately get memory rows filtered :great didn’t ever “useless” version dataframe ever code; immediately get version filtering done., data lives .csv remote host?Hopefully can see options powerful. Although iris small, sometimes data large, may want deal loading many millions rows R going filter anyway. example later Performance section.","code":"\n# disabled because we were asked to not write any data\nwrite.table(iris, 'iris.csv', sep = \",\", quote = FALSE, row.names = FALSE)\n# disabled because we were asked to not write any data\nread.csv.sql('iris.csv',  sql = 'select * from file where \"Petal.Length\" > 2.0') |>\n  head()\nread.csv.sql(\n  'https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv',\n  sql = 'select * from file where \"Petal.Length\" > 2.0'\n) |>\n  head()##   sepal.length sepal.width petal.length petal.width      variety\n## 1          7.0         3.2          4.7         1.4 \"Versicolor\"\n## 2          6.4         3.2          4.5         1.5 \"Versicolor\"\n## 3          6.9         3.1          4.9         1.5 \"Versicolor\"\n## 4          5.5         2.3          4.0         1.3 \"Versicolor\"\n## 5          6.5         2.8          4.6         1.5 \"Versicolor\"\n## 6          5.7         2.8          4.5         1.3 \"Versicolor\""},{"path":"how-to-use-sqldf.html","id":"advanced-database-usage","chapter":"20 How to use sqldf","heading":"20.2.4 Advanced database usage","text":"hood, sqldf actually loads dataframe temporary database table. want, can also manage database intelligently. contrived use case, worth knowing. Suppose ’re dealing lot data plan two subsequent queries. better read dataframe table reuse table. can accomplished via:can also just pass database administrative command function well. example, manage entire database (create new schemas, tables, adjust permissions) really wanted . Although appropriate tool might worth considering.","code":"\nsqldf() # keep iris as a table in the db## <SQLiteConnection>\n##   Path: :memory:\n##   Extensions: TRUE\nsqldf('select * from iris') |> # iris now loaded as a table. can reuse it.\n  head()##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\nsqldf() # connection closed and iris table deleted## NULL"},{"path":"how-to-use-sqldf.html","id":"performance","chapter":"20 How to use sqldf","heading":"20.3 Performance","text":"Certain tasks actually end optimal done sqldf. example, following arbitrarily large join, fans nearly billion rows, took roughly 30 seconds laptop finish. (Although task nonsense, real data, one may come across use case actually needs something similar.)following R equivalent, third ‘join’, took longer four took SQL. preserve computer, attempt fourth merge (commented ).library also becomes helpful dealing large datasets disk. example, arbitrary .csv ~600MB (include file project, feel free try large file). require load entire file R first, finished 30 seconds.worthwhile improvement; R equivalent completed 45 seconds.difference becomes meaningful dataset’s size increases relative machine’s memory. file several GB, preprocessing temporary database table becomes increasingly efficient relative pure R. observed marginal version optimization . functionality especially useful know never need refer back full file (meaning use e.g. transformed version ).","code":"\nsqldf('\n  select count(*)\n  from iris a\n  join iris b using (species)\n  join iris c using (species)\n  join iris d using (species)\n  join iris e using (species)\n')##    count(*)\n## 1 937500000\nmerge(iris, iris, by=\"Species\") |>\n  merge(iris, by=\"Species\") |>\n  merge(iris, by=\"Species\") |>\n  #merge(iris, by=\"Species\") |>\n  nrow()## [1] 18750000\n# disabled because I cannot provide this (intentionally) large file\nread.csv.sql(\n  '~/Downloads/star2002-1.csv',\n  sql='select `X1`, avg(`X807`) from file where `X4518` > 5500 group by 1'\n) |>\n  head()\n# disabled because I cannot provide this (intentionally) large file\nread.csv(file = '~/Downloads/star2002-1.csv') |>\n  filter(X4518 > 5500) |>\n  group_by(X1) |>\n  summarize(some_avg=mean(X807)) |>\n  head()"},{"path":"how-to-use-sqldf.html","id":"combining-with-ggplot","chapter":"20 How to use sqldf","heading":"20.4 Combining with ggplot","text":"can also combine sqldf manipulations ggplot easily make visualizations. use sqldf scenarios excels , outlined , becomes powerful. infinite combinations , one simple illustrative example:","code":"\nsqldf('\n      select Species, avg(`Sepal.Width`) as avg_width\n      from iris\n      group by 1\n') |>\n  ggplot(aes(x=reorder(Species, avg_width), y=avg_width)) +\n  geom_bar(stat='identity') +\n  coord_flip() +\n  xlab('Species')"},{"path":"how-to-use-sqldf.html","id":"conclusion-1","chapter":"20 How to use sqldf","heading":"20.5 Conclusion","text":"sqldf important option available manipulating data. clear, replacement knowing use R general. One restrict using sqldf uniquely advantageous SQL-style work, don’t want deal writing perfect R. guide useful anyone new library exactly scenarios one might opt use .Personally, ’m glad chose deep dive library create guide. educational many ways, like: learning databases, understanding R can blend SQL, elucidating things R vs. SQL excel . certainly referring back document, think makes easy review exactly quickly use library, without getting --weeds nuts bolts (much existing documentation , opinion). One thing ’d like dedicate effort next time exactly replicating complex SQL commands R; knowing likely useful point, even practical right now. Finally, wish known library sooner, know sure optimize parts workflow going foward.","code":""},{"path":"how-to-use-sqldf.html","id":"references","chapter":"20 How to use sqldf","heading":"20.6 References","text":"https://github.com/ggrothendieck/sqldfhttps://cran.r-project.org/web/packages/sqldf/index.htmlhttps://www.geeksforgeeks.org/window-functions--sql/","code":""},{"path":"googlevis-in-r.html","id":"googlevis-in-r","chapter":"21 googleVis in R","heading":"21 googleVis in R","text":"Sushant Prabhu Kiyan Mohebbizadeh","code":""},{"path":"googlevis-in-r.html","id":"introducing-googlevis","chapter":"21 googleVis in R","heading":"21.1 Introducing googleVis","text":"GoogleVis package R allows users R use Google Charts API.interface R Google Charts allows users access Google Charts’ interactive charts. googleVis allows users use data R data frames create Google Charts without uploading data onto Google.Demonstrating using googleVis Library - Installation Usage","code":"install.packages('googleVis')\nlibrary(googleVis)"},{"path":"googlevis-in-r.html","id":"why-use-googlevis","chapter":"21 googleVis in R","heading":"21.2 Why use googleVis ?","text":"googleVis package allows users create interactive visualizations R’s popular visualization package (ggplot) allow.Although packages work conjunction ggplot make interactive visualizations, googleVis offers holistic package allows unique interactive visualizations.using Google Charts, one able create wide variety visualizations ranging typical bar line graphs mapping timeline charts one package.visualizations created googleVis add level interest consumer due interactive layer viewers able gather specific bits information hovering clicking values visualizations. allows increased aesthetics, also information transferred viewers.","code":""},{"path":"googlevis-in-r.html","id":"googlevis-rendering-interaction","chapter":"21 googleVis in R","heading":"21.3 googleVis Rendering & Interaction","text":"output googleVis can either embedded HTML file read dynamically. visualizations often rendered online web format. Therefore, browser internet connection required view interactive version output compared ggplotFor use R, googleVis allows user render Shiny file allows preview interaction within R. used preview chart final render.googleVis package R allows users R use Google Charts API.interface R Google Charts allows users access Google Charts’ interactive charts.googleVis allows users use data R data frames create Google Charts without uploading data onto Google.","code":""},{"path":"googlevis-in-r.html","id":"basic-graphs-line-bar-combo","chapter":"21 googleVis in R","heading":"21.3.1 Basic Graphs (Line, Bar, Combo)","text":"charts best used comparisons groups. seen examples, comparisons costs owning different pets.line graph shows different variables flow within among groups. audience able determine within group trends seeing lines intersect within group. Showing trends variables lines groups allows us make comparisons among various groups clarity.organizing variables certain way, one able get sense population trends.bar column chart essentially just rotated axis. allow great group comparisons well comparisons among groups. However, charts best used -group comparisons.Combo charts great multiple variable comparisons allow user get best worlds. carefully selecting variables represented bars ones lines, user able best show relationship within groups trends population.","code":"\ndf = data.frame(pet=c('cat', 'dog', 'hamster', 'snake'),\n                food_cost_monthly=c(50, 100, 10, 40),\n                medical_cost_monthly=c(30, 60, 5, 50))\nLine <- gvisLineChart(df)\n\nBar <- gvisBarChart(df)\n\nColumn <- gvisColumnChart(df)\n\nSteppedArea <- gvisSteppedAreaChart(df, xvar=\"pet\", \n                                    yvar=c(\"food_cost_monthly\", \"medical_cost_monthly\"),\n                                    options=list(isStacked=TRUE))\n\nCombo <- gvisComboChart(df, xvar=\"pet\",\n                        yvar=c(\"food_cost_monthly\", \"medical_cost_monthly\"),\n                        options=list(seriesType=\"bars\",\n                                     series='{1: {type:\"line\"}}'))\nplot(Line)\nplot(Bar)\nplot(Column)\nplot(SteppedArea)\nplot(Combo)"},{"path":"googlevis-in-r.html","id":"googlevis-histogram-chart","chapter":"21 googleVis in R","heading":"21.3.2 googleVis Histogram Chart","text":"histogram allows users represent distribution one particular group variable showing frequency particular group variable within range. charts googleVis advantage regular histograms almost histogram allows recommends specific information regarding counts different points visualization, however, googleVis audience can look distribution access specific metrics interaction well.","code":"\ndf <- iris\nHistogram <- gvisHistogram(data.frame(Sepal_Width = df$Sepal.Width))\n\nplot(Histogram)"},{"path":"googlevis-in-r.html","id":"googlevis-alluvialsankey-chart","chapter":"21 googleVis in R","heading":"21.3.3 googleVis Alluvial/Sankey Chart","text":"Alluvial charts best show movement sample population among different variables. example movement students within school class class represented. visualization can helpful data ordinal timeline specific values. googleVis, audience exposed general trends clean looking chart well specifics graph interaction.","code":"\ndf <- data.frame(From=c(rep(\"Math\",3), rep(\"Science\", 3)),\n                    To=c(rep(c('Lunch', 'Art', 'Music'),2)),\n                    Weight=c(17,15,13,5,12,8))\n\nAlluvial <- gvisSankey(df, from=\"From\", to=\"To\", weight=\"Weight\")\n\nplot(Alluvial)"},{"path":"googlevis-in-r.html","id":"googlevis-geographic-chart","chapter":"21 googleVis in R","heading":"21.3.4 googleVis Geographic Chart","text":"Map visualizations googleVis incredibly easy create manipulate. useful comparing different geographic areas . googleVis automatically color scales values interaction allows map simple clean, get specific values hovering particular geographic area.","code":"\ndf = data.frame(country=c('US', 'CN', 'BR', 'IS', 'RU', 'TH', 'TR', 'ID', 'MX', 'IR' ),\n                incarceration_rate = c(2068800, 1690000, 811707, 478600, 471490, 309282, 291198, 266259, 220866, 189000))\n\nG <- gvisGeoChart(df, locationvar = \"country\", colorvar = \"incarceration_rate\",\n                  options=list(\n                         gvis.editor=\"Edit the Geo Chart !\"))\n\nplot(G)"},{"path":"googlevis-in-r.html","id":"googlevis-gauge-chart","chapter":"21 googleVis in R","heading":"21.3.5 googleVis Gauge Chart","text":"gauge charts interactive, however offer unique way model data always within certain range. example, temperatures, speeds, pressure, etc. chart allows quick comparison groups aesthetic value presentation.","code":"\ntemperature <- data.frame(city=c('Las Vegas', 'Los Angeles', 'Pheonix', 'Dallas', 'Houston', 'Miami'),\n                          temp=c(115, 103, 120, 110, 112, 101))\nGauge <-  gvisGauge(temperature, \n                    options=list(min=0, max=150, greenFrom=0,\n                                 greenTo=50, yellowFrom=50, yellowTo=100,\n                                 redFrom=100, redTo=150, width=400, height=300))\n\nplot(Gauge)"},{"path":"googlevis-in-r.html","id":"googlevis-tabular-chart","chapter":"21 googleVis in R","heading":"21.3.6 googleVis Tabular Chart","text":"data formatted table can paged sorted. flexible option select single rows either keyboard mouse. also powers sorting rows across dimensions columns dataset. navigation paged tabular information smooth simple.","code":"\n## Tabular Data Un-Paged\nPopulation_Tabular_Unpaged <- gvisTable(Population[1:30,],\n                                        formats=list(Population=\"#,###\",'% of World Population'='#.#%'))\n\nplot(Population_Tabular_Unpaged)\n## Tabular Data Paged\nPopulation_Tabular_paged <- gvisTable(Population[1:30,], \n                                      formats=list(Population=\"#,###\",'% of World Population'='#.#%'),\n                                      options=list(page='enable',\n                                                   height='automatic',\n                                                   width='automatic'))\n\nplot(Population_Tabular_paged)"},{"path":"googlevis-in-r.html","id":"googlevis-tree-map-chart","chapter":"21 googleVis in R","heading":"21.3.7 googleVis Tree Map Chart","text":"googleVis tree map visual representation data tree, node 0 children, 1 parent barring root node. One can specify many levels display simultaneously, optionally display deeper levels. One can move tree person left-clicks node, moves back tree person right-clicks graph.total size graph determined size elements contained graph.googleVis tree map chart captures relative sizes data categories, helps quick insight datapoints bigger contributors category. Color helps scrutinize datapoints underperforming / overperforming) compared siblings category.","code":"\nCountry_Tree <- gvisTreeMap(Regions, \"Region\", \"Parent\", \"Val\", \"Fac\", \n                     options=list(width=800, height=500, fontSize=15,\n                                  minColor='#cfe2f3',midColor='#6fa8dc',maxColor='#0b5394',\n                                  headerHeight=10,fontColor='black',showScale=TRUE))\n\nplot(Country_Tree)"},{"path":"googlevis-in-r.html","id":"googlevis-annotation-chart","chapter":"21 googleVis in R","heading":"21.3.8 googleVis Annotation Chart","text":"Annotation charts useful, interactive time series like line charts enable annotations.annotated charts leveraged highlight specific data value-add contextual notes within visualization.answer “?” kind questions, well defined annotations highlight significance data chart, keen detail textual description / annotation.One can also slice interactive timeline chart look snapshot data aesthetically pleasing also provides great detail insights within visualization. annotation charts SVG (scalable vector graphics) /VML (vector graphics rendering ).","code":"\nStock_Annotation <- gvisAnnotationChart(Stock, datevar=\"Date\",numvar=\"Value\", idvar=\"Device\", titlevar=\"Title\",\n                                        annotationvar=\"Annotation\",\n                                        options=list(displayAnnotations=TRUE,\n                                        chart=\"{chartArea:{backgroundColor:'#ebf0f7'}}\",\n                                        legendPosition='newRow',width=800, height=450,\n                                        scaleColumns='[0,1]',scaleType='allmaximized'))\n\nplot(Stock_Annotation)"},{"path":"googlevis-in-r.html","id":"googlevis-calendar-chart","chapter":"21 googleVis in R","heading":"21.3.9 googleVis Calendar Chart","text":"googleVis calendar chart definitive visualization can used show activity course longer duration time, example months years decades. One can illustrate variation 1 quantity depending days given week, trends timeline period.calendar charts demonstrate data records, events, daily, weekly, monthly, yearly calendar. highly interactive one can view value hovering particular time entire timeperiod.","code":"\nCalendar_Temp <- gvisCalendar(Cairo, datevar=\"Date\", numvar=\"Temp\",\n                    options=list(title=\"Cairo's variation in Daily\n                                 temperature\",height=400,width=1000,\n                                 calendar=\"{yearLabel: { fontName:'sans-serif',\n                                 fontSize: 20, color: 'black', bold: true},\n                                 cellSize: 10,cellColor:{stroke: 'black', strokeOpacity: 0.2},\n                                 focusedCellColor: {stroke:'red'}}\"), chartid=\"Calendar\")\n\nplot(Calendar_Temp)"},{"path":"googlevis-in-r.html","id":"googlevis-timeline-chart","chapter":"21 googleVis in R","heading":"21.3.10 googleVis Timeline Chart","text":"googleVis Timeline chart great fascinating way visualizing different dates / events. example, showing duration Presidents & Vice Presidents / Sessions Congress timeline period. exact times durations given one interactively hovers bars.timeline charts versatile visuals illustrating sequence events chronologically. provides amazing aid conceptualize event sequences / processes gain valuable insights, sometimes maybe summarize historical events, time frame minutes, hours, years datewise.","code":"\nPosition_Timeline_Data <- data.frame(Position=c(rep(\"President\", 4), rep(\"Vice\", 4)),\n                    Name=c(\"William Clinton\",\"George Bush\", \"Barack Obama\", \"   Donald Trump\",\n                          \" Albert Gore\",\"Dick Cheney\", \"Biden, Jr.\", \"Michael Pence\"),\n                    start=as.Date(x=rep(c(\"1993-01-20\",\"2001-01-20\", \"2009-01-20\",\"2017-01-20\"),2)),\n                    end=as.Date(x=rep(c(\"2001-01-20\",\"2009-01-20\", \"2017-01-20\", \"2021-01-20\"),2)))\n\nTimeline <- gvisTimeline(data=Position_Timeline_Data, \n                         rowlabel=\"Name\",\n                         barlabel=\"Position\",\n                         start=\"start\", \n                         end=\"end\",\n                         options=list(timeline=\"{groupByRowLabel:false}\",\n                                      backgroundColor='#e3f4ff', \n                                      height=400,colors=\"['#0e407d', '#78b2ff', '#3737ab']\"))\n\nplot(Timeline)"},{"path":"googlevis-in-r.html","id":"googlevis-gantt-chart","chapter":"21 googleVis in R","heading":"21.3.11 googleVis Gantt Chart","text":"googleVis Gantt charts help teams plan work around deadlines allocate resources efficiently.Project planners also leverage Gantt charts maintain bird’s eye high level view projects track . depict relationship start end dates tasks, milestones, dependent tasks entire timeline project. Gantt chart illustrates breakdown project component tasks effectively.","code":"\ndaysToMilliseconds <- function(days){days * 24 * 60 * 60 * 1000}\ndat <- data.frame(taskID = c(\"PS\", \"EDA\", \"R\", \"ML\", \"DP\"),\n                 taskName = c(\"Identify Problem Statement\", \"EDA Analysis\", \"Research\",\n                              \"Machine Learning Modelling\", \"Data Preprocessing\"),\n                 resource = c(NA, \"write\", \"write\", \"complete\", \"write\"),\n                 start = c(as.Date(\"2022-10-01\"), NA, as.Date(\"2022-10-02\"), as.Date(\"2022-10-08\"), NA),\n                 end = as.Date(c(\"2022-10-04\", \"2022-10-08\", \"2022-10-08\",\n                                 \"2022-10-13\", \"2022-10-05\")),\n                 duration = c(NA, daysToMilliseconds(c(3, 1, 1, 1))),\n                 percentComplete = c(100, 25, 20, 0, 100),\n                 dependencies = c(NA, \"PS, DP\", NA,\n                 \"EDA\", \"PS\"))\n\nGantt_Tasks <- gvisGantt(dat, taskID = \"taskID\",taskName = \"taskName\",resource = \"resource\",\n                         start = \"start\",end = \"end\",duration = \"duration\",percentComplete = \"percentComplete\",\n                         dependencies = \"dependencies\",\n                         options = list(height = 300,\n                         gantt = \"{criticalPathEnabled:true,innerGridHorizLine: {\n                         stroke: '#e3f4ff',strokeWidth: 2},innerGridTrack: {fill: '#e8f3fa'},innerGridDarkTrack:\n                         {fill: '#c7e9ff'},labelStyle: {fontName: 'sans-serif',fontSize: 16}}\"))\n\nplot(Gantt_Tasks)"},{"path":"googlevis-in-r.html","id":"googlevis-merging-charts","chapter":"21 googleVis in R","heading":"21.3.12 googleVis Merging Charts","text":"googleVis Merge chart provides flexibility merging two gvis-objects, either next one gvis-object. objects arranged HTML table format.multiples charts view allows split individual charts Bar, Column, Line Geographic, Tabular etc. charts multiple charts, separated. numerous use cases like showing product sales per region providing information . gives lot flexibility report creation delivery aesthetically.","code":"\nGeographic <- gvisGeoChart(Exports,\n                           locationvar=\"Country\",colorvar=\"Profit\",\n                           options=list(width=400, height=200))\n\nTabular <- gvisTable(Exports,\n                     options=list(width=400, height=400))\n\nMerged_Charts <- gvisMerge(Geographic, Tabular, horizontal=FALSE, tableOptions=\"bgcolor=\\\"#7cdeb5\\\"\")\n\nplot(Merged_Charts)"},{"path":"googlevis-in-r.html","id":"use-googlevis-in-rstudio","chapter":"21 googleVis in R","heading":"21.4 Use googleVis in RStudio","text":"Using googleVis RStudio straightforward. default, RStudio renders charts new webpage -hand, view within RStudio,\n> View RStudio Viewer just use view locallyTo Knit Rmd Markdown file HTML, perform following command set Chunk option results asis {r ChartExample, results='asis', tidy=FALSE} plot(Chart, 'chart')","code":"plot(Chart)plot(Chart, browser=rstudioapi::viewer)"},{"path":"googlevis-in-r.html","id":"googlevis-references","chapter":"21 googleVis in R","heading":"21.4.1 googleVis References","text":"DocumentationGoogle ChartsDemoPaperCRAN-Stable VersionThank learning googleVis us !","code":""},{"path":"tutorial-for-vector-fields-in-r.html","id":"tutorial-for-vector-fields-in-r","chapter":"22 Tutorial for vector fields in r","heading":"22 Tutorial for vector fields in r","text":"Sebastian Steiner & Elliot Frank","code":""},{"path":"tutorial-for-vector-fields-in-r.html","id":"getting-started-with-vector-fields","chapter":"22 Tutorial for vector fields in r","heading":"22.1 Getting Started with Vector Fields","text":"Vector field graphs number important applications throughout science,\nengineering, math. tutorial, ’ll explain basic components \nvector field graphs, build R.research, documentation available use improvement, thus \ngoal provide accessible explanation build vector graphs R. \nkeep things clear, ’ll build example data set go, explaining \nrequired data components vector field graphs. Hopefully, reading \ntutorial, ’ll able easily put together vector graphs using \ndata sets.example data set, observation represents one arrow, vector, \nplotted graph. vectors communicate movement, arrow \nstarting point end point. dive , ’ve listed four\ndata columns required building vector graph, build\nexample data set.x_axis: horizontal value starting point given vectory_axis: vertical value starting point given vectorx_pull: strength force pulling vector horizontal directiony_pull: strength force pulling vector vertical directionFirst, require two data columns serving x y coordinates \nplacement arrow graph. initial coordinates, \ncall x_axis y_axis, place base arrow vector\ngraph. Stated otherwise, together x_axis y_axis columns provide \nstarting point arrow graph.code , start building example data set placing base\nvector positive integer values 10 X 10 grid, assigning\nvalues x_axis y_axis columns.graphing plot, ’ve assigned x_axis y_axis values axis values\nentire chart, , columns place arrow. geom_segment\nfunction plots lines, ’ve added endpoint 0.05 x_axis \nbase vector can seen .plotting starting points vectors, now need \ndetermine vectors end. determined \nx_pull y_pull columns. ’s important note columns\nprovide end coordinates, measure directions \nvector pulled. x_pull y_pull variables indicate far\nbase arrow extend given direction.’ll notice code , x_pull y_pull values added\nstarting point values (x_axis & y_axis) geom_segment function.\nvariables within ‘aes’, dictate end points vector, conveniently\nnamed ‘xend’ ‘yend’. altering ‘xend’ ‘yend’ values, place\ncoordinates vector’s endpoint.example, set y_pull equal 0.5, ’ll notice \narrows pulled upward direction.Conversely, set x_pull equal 0.5, ’ll notice \narrows pulled right direction.set x_pull y_pull equal 0.5, x y forces \noffsetting, arrows point 45 degree angle.provide another example, ’ve input random numbers x_pull y_pull\nvalues, show format completely flexible doesn’t require\nconsistent value changes x_pull y_pull variables.","code":"\n# Creating a blank data frame with four required columns\ndata_frame = data.frame(x_axis = numeric(), y_axis = numeric())\n\n# Generating evenly distributed values for x y coordinates\nfor(i in 1:10) {\n  for(j in 1:10) {\n    vec <- c(i, j) \n    data_frame[nrow(data_frame) + 1, ] <- vec\n  }\n}\n\n# Plotting points for illustrative purposes\nggplot(data_frame, aes(x = x_axis, y = y_axis)) +\n    scale_x_continuous(breaks = seq(0,10,1)) + \n    scale_y_continuous(breaks = seq(0,10,1)) +\n    geom_segment(aes(xend = x_axis+(0.05), yend = y_axis)) +\n    coord_fixed()\ndata_frame$x_pull <- 0\ndata_frame$y_pull <- 0.5\n\n# Plotting points for illustrative purposes\nggplot(data_frame, aes(x = x_axis, y = y_axis)) +\n    scale_x_continuous(breaks = seq(0,10,1)) + \n    scale_y_continuous(breaks = seq(0,10,1)) +\n    geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = y_axis + (y_pull)), \n                 arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n    coord_fixed()\ndata_frame$x_pull <- 0.5\ndata_frame$y_pull <- 0\n\n# Plotting points for illustrative purposes\nggplot(data_frame, aes(x = x_axis, y = y_axis)) +\n    scale_x_continuous(breaks = seq(0,10,1)) + \n    scale_y_continuous(breaks = seq(0,10,1)) +\n    geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = y_axis + (y_pull)), \n                 arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n    coord_fixed()\ndata_frame$x_pull <- 0.5\ndata_frame$y_pull <- 0.5\n\nggplot(data_frame, aes(x = x_axis, y = y_axis)) +\n    scale_x_continuous(breaks = seq(0,10,1)) + \n    scale_y_continuous(breaks = seq(0,10,1)) +\n    geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = y_axis + (y_pull)), \n                     arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n    coord_fixed()\ndata_frame$x_pull <- runif(nrow(data_frame), min=-0.5, max=0.5)\ndata_frame$y_pull <- runif(nrow(data_frame), min=-0.5, max=0.5)\n\nggplot(data_frame, aes(x = x_axis, y = y_axis)) +\n    scale_x_continuous(breaks = seq(0,10,1)) + \n    scale_y_continuous(breaks = seq(0,10,1)) +\n    geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = y_axis + (y_pull)), \n                     arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n    coord_fixed()"},{"path":"tutorial-for-vector-fields-in-r.html","id":"tips-and-tricks-to-plotting-vector-fields","chapter":"22 Tutorial for vector fields in r","heading":"22.2 Tips and Tricks to Plotting Vector Fields","text":"Now ’ve covered basics, ’ll provide guidance make\nhigh-quality vector field graphs.","code":""},{"path":"tutorial-for-vector-fields-in-r.html","id":"understanding-arrow-options","chapter":"22 Tutorial for vector fields in r","heading":"22.2.1 Understanding arrow options","text":"plotting arrows geom_segment, can control features arrow\nfollowing line:length = unit(0.1, “cm”) defines size arrow headsize = 0.25  defines arrow thicknessFor example, set 1, get following:arrow adjustments produce low-quality plot, highlight options\none represent arrows vector fields.","code":"arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25)\ndata_frame$x_pull <- 0.5\ndata_frame$y_pull <- 0.5\n\nggplot(data_frame, aes(x = x_axis, y = y_axis)) +\n    scale_x_continuous(breaks = seq(0,10,1)) + \n    scale_y_continuous(breaks = seq(0,10,1)) +\n    geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = y_axis + (y_pull)), \n                     arrow = arrow(length = unit(1, \"cm\")), size = 1) +\n    coord_fixed()"},{"path":"tutorial-for-vector-fields-in-r.html","id":"axis-scaling","chapter":"22 Tutorial for vector fields in r","heading":"22.2.2 Axis Scaling","text":"Vector fields commonly represent flows space. Therefore, moving 1 unit \nx-direction distance moving 1 unit y-direction. 1:1\nratio preserved, vector field becomes distorted difficult \ninterpret. Therefore, axis ggplot must fixed using following code:illustration discussion - note cases, vectors \nform (0.5,0.5). distortion appears small, easily avoidable\npreserves accurate relationship x y axes.","code":"coord_fixed()\n#Creating homogeneous vector field (1,1)\ndata_frame$x_pull <- 0.5\ndata_frame$y_pull <- 0.5\n\n#No coord_fixed\nggplot(data_frame, aes(x = x_axis, y = y_axis)) +\n  scale_x_continuous(breaks = seq(0,10,1)) + \n  scale_y_continuous(breaks = seq(0,10,1)) +\n  geom_segment(aes(xend = x_axis + (x_pull), \n                  yend = y_axis + (y_pull)), \n                  arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25)  +\n  ggtitle(\"Distorted plot without coord_fixed()\")\n#Coord_fixed\nggplot(data_frame, aes(x = x_axis, y = y_axis)) +\n        scale_x_continuous(breaks = seq(0,10,1)) + \n        scale_y_continuous(breaks = seq(0,10,1)) +\n        geom_segment(aes(xend = x_axis + (x_pull), \n                        yend = y_axis + (y_pull)), \n                        arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n        coord_fixed() +\n        ggtitle(\"High-quality plot with coord_fixed()\")"},{"path":"tutorial-for-vector-fields-in-r.html","id":"arrow-length","chapter":"22 Tutorial for vector fields in r","heading":"22.2.3 Arrow Length","text":"important consider length arrows plotting vector fields, \ninput data may cause arrows overlap, making plot difficult interpret\n(see example ).can see, even simple plot vectors pointing \ndirection, overlapping arrows makes impossible see origin point \narrows middle. Therefore, recommend scaling x_pull y_pull vectors,\nshown (note good practice scale x_pull y_pull \namount).","code":"\ndata_frame$x_pull <- 5\ndata_frame$y_pull <- 1\n\n# Plotting points for illustrative purposes\nggplot(data_frame, aes(x = x_axis, y = y_axis)) +\n    scale_x_continuous(breaks = seq(0,10,1)) + \n    scale_y_continuous(breaks = seq(0,10,1)) +\n    geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = y_axis + (y_pull)), \n                 arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n    coord_fixed()\n# Assume this is the raw x_pull & y_pull data\ndata_frame$x_pull <- 5\ndata_frame$y_pull <- 1\n\n# Scale vector data\ndata_frame$x_pull <- data_frame$x_pull / 10\ndata_frame$y_pull <- data_frame$y_pull / 10\n\n# Plotting points for illustrative purposes\nggplot(data_frame, aes(x = x_axis, y = y_axis)) +\n    scale_x_continuous(breaks = seq(0,10,1)) + \n    scale_y_continuous(breaks = seq(0,10,1)) +\n    geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = y_axis + (y_pull)), \n                 arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n    coord_fixed()"},{"path":"tutorial-for-vector-fields-in-r.html","id":"arrow-color","chapter":"22 Tutorial for vector fields in r","heading":"22.2.4 Arrow Color","text":"scaling arrows, absolute length arrows loses meaning,\ndistorts relative strength vectors graph. qualitative\nrepresentation acceptable cases plotting vector fields, \npossible add color arrows based magnitude - see . Moreover,\ncolor bar can add title explaining units colors\nrepresent (e.g. m/s). example, ’ll create new data set x_axis\ny_axis values ranging -10 10.following plot, changed arrow color based x_pull\nvalue, rather magnitude. useful flow one direction\nimportant .","code":"\nvector_frame = data.frame(x_axis = numeric(), y_axis = numeric())\n\n# Generating evenly distributed values for x y coordinates\nfor(i in -10:10) {\n  for(j in -10:10) {\n    vec <- c(i, j) \n    vector_frame[nrow(vector_frame) + 1, ] <- vec\n  }\n}\n\nvector_frame$x_pull <- with(vector_frame, -x_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\nvector_frame$y_pull <- with(vector_frame, 2*y_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\nvector_frame$mag    <- sqrt( (vector_frame$x_pull^2) + (vector_frame$y_pull)^2 ) \n\nggplot(vector_frame, aes(x = x_axis, y = y_axis, colour=mag) )+\n    scale_colour_continuous(name = \"*Units\") +\n    scale_x_continuous(breaks = seq(-10,10,1)) + \n    scale_y_continuous(breaks = seq(-10,10,1)) +\n    geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = y_axis + (y_pull)), \n                     arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n    geom_vline(xintercept=0, size=0.15) + geom_hline(yintercept=0, size=0.15) +\n    coord_fixed()\nvector_frame = data.frame(x_axis = numeric(), y_axis = numeric())\n\n# Generating evenly distributed values for x y coordinates\nfor(i in -10:10) {\n  for(j in -10:10) {\n    vec <- c(i, j) \n    vector_frame[nrow(vector_frame) + 1, ] <- vec\n  }\n}\n\nvector_frame$x_pull <- with(vector_frame, -x_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\nvector_frame$y_pull <- with(vector_frame, 2*y_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\n\nggplot(vector_frame, aes(x = x_axis, y = y_axis, colour=x_pull) )+\n    scale_colour_continuous(low = \"dodgerblue\", high = \"darkred\") +\n    scale_x_continuous(breaks = seq(-10,10,1)) + \n    scale_y_continuous(breaks = seq(-10,10,1)) +\n    geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = y_axis + (y_pull)), \n                     arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n    geom_vline(xintercept=0, size=0.15) + geom_hline(yintercept=0, size=0.15) +\n    coord_fixed()"},{"path":"tutorial-for-vector-fields-in-r.html","id":"spacing","chapter":"22 Tutorial for vector fields in r","heading":"22.3 Spacing","text":"Vector fields often describe flows continuous space, means \ninfinite number vectors plot. overcome , usually sample\nuniformly spaced points field plot vectors. Choosing point\nspacing important -sample much, lose information.\nhand , don’t -sample enough, vector field becomes cluttered.shown , extremes spacing, vector fields difficult interpret.\nHence, must optimal spacing two. illustrated , \noptimal spacing also depends arrow length closer spacing requires shorter\narrow lengths, larger spacing allows longer arrow lengths. words,\ntrade-vector spacing vector length.","code":"\nvector_frame1 = data.frame(x_axis = numeric(), y_axis = numeric())\n\n# Generating a dense vector field\nfor(i in seq(-10, 10, by=0.4)) {\n  for(j in seq(-10, 10, by=0.4)) {\n    vec <- c(i, j) \n    vector_frame1[nrow(vector_frame1) + 1, ] <- vec\n  }\n}\n\nvector_frame1$x_pull <- with(vector_frame1, -x_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\nvector_frame1$y_pull <- with(vector_frame1, 2*y_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\n\np1 <- ggplot(vector_frame1, aes(x = x_axis, y = y_axis) )+\n        scale_x_continuous(breaks = seq(-10,10,2)) + \n        scale_y_continuous(breaks = seq(-10,10,2)) +\n        geom_segment(aes(xend = x_axis + (x_pull), \n                         yend = y_axis + (y_pull)), \n                         arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n        geom_vline(xintercept=0, size=0.15) + geom_hline(yintercept=0, size=0.15) +\n        ggtitle(\"Not enough spacing\") +\n        theme(text=element_text(size=9)) +\n        coord_fixed()\n\nvector_frame2 = data.frame(x_axis = numeric(), y_axis = numeric())\n\n# Generating a sparse vector field\nfor(i in seq(-10, 10, by=5)) {\n  for(j in seq(-10, 10, by=5)) {\n    vec <- c(i, j) \n    vector_frame2[nrow(vector_frame2) + 1, ] <- vec\n  }\n}\n\nvector_frame2$x_pull <- with(vector_frame2, -x_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\nvector_frame2$y_pull <- with(vector_frame2, 2*y_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\n\np2 <- ggplot(vector_frame2, aes(x = x_axis, y = y_axis) )+\n        scale_x_continuous(breaks = seq(-10,10,2)) + \n        scale_y_continuous(breaks = seq(-10,10,2)) +\n        geom_segment(aes(xend = x_axis + (x_pull), \n                         yend = y_axis + (y_pull)), \n                         arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n        geom_vline(xintercept=0, size=0.15) + geom_hline(yintercept=0, size=0.15) +\n        ggtitle(\"Too much spacing\") +\n        theme(text=element_text(size=9)) +\n        coord_fixed()\n\np1 + p2\nvector_frame1 = data.frame(x_axis = numeric(), y_axis = numeric())\n\n# Vector field with larger spacing and arrows\n\nfor(i in seq(-10, 10, by=2)) {\n  for(j in seq(-10, 10, by=2)) {\n    vec <- c(i, j) \n    vector_frame1[nrow(vector_frame1) + 1, ] <- vec\n  }\n}\n\nvector_frame1$x_pull <- with(vector_frame1, -2*x_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\nvector_frame1$y_pull <- with(vector_frame1, 3*y_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\n\np1 <- ggplot(vector_frame1, aes(x = x_axis, y = y_axis) )+\n        scale_x_continuous(breaks = seq(-10,10,2)) + \n        scale_y_continuous(breaks = seq(-10,10,2)) +\n        geom_segment(aes(xend = x_axis + (x_pull), \n                         yend = y_axis + (y_pull)), \n                         arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n        geom_vline(xintercept=0, size=0.15) + geom_hline(yintercept=0, size=0.15) +\n        ggtitle(\"Opitmal with more spacing\") +\n        theme(text=element_text(size=9)) +\n        coord_fixed()\n\nvector_frame2 = data.frame(x_axis = numeric(), y_axis = numeric())\n\n# Vector field with smaller spacing and arrows\n\nfor(i in seq(-10, 10, by=1)) {\n  for(j in seq(-10, 10, by=1)) {\n    vec <- c(i, j) \n    vector_frame2[nrow(vector_frame2) + 1, ] <- vec\n  }\n}\n\nvector_frame2$x_pull <- with(vector_frame2, -x_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\nvector_frame2$y_pull <- with(vector_frame2, y_axis/(sqrt((x_axis^2) + (y_axis^2)) + 4))\n\np2 <- ggplot(vector_frame2, aes(x = x_axis, y = y_axis) )+\n        scale_x_continuous(breaks = seq(-10,10,2)) + \n        scale_y_continuous(breaks = seq(-10,10,2)) +\n        geom_segment(aes(xend = x_axis + (x_pull), \n                         yend = y_axis + (y_pull)), \n                         arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n        geom_vline(xintercept=0, size=0.15) + geom_hline(yintercept=0, size=0.15) +\n        ggtitle(\"Optimal with less spacing\") +\n        theme(text=element_text(size=9)) +\n        coord_fixed()\n\np1 + p2"},{"path":"tutorial-for-vector-fields-in-r.html","id":"dealing-with-3d","chapter":"22 Tutorial for vector fields in r","heading":"22.4 Dealing with 3D","text":"far looked vector fields 2 dimensions. However, common \nvector fields 3 dimensions. tempting try building 3D\nplots vector fields, strongly recommend confusing. \nshown plot , taken page.Therefore, first thing consider plotting 3D vector\nfields : actually need plot 3 dimensions?cases, one dimension may small contribution overall flow, \nexcluded plot. check comparing contributions\ndirection magnitude vector using following formulas\nx^2 / (x^2 + y^2 + z^2) , y^2 / (x^2 + y^2 + z^2) & z^2 / (x^2 + y^2 +z^2 ).Another aspect consider symmetry environment flow. \nexample, flow cylindrical symmetric radial direction; words,\nsay flow z-direction, cross-sectional flows identical\nz-x z-y plots, see graph visual explanation.possible remove one axis due small contribution, symmetry\nvector field, recommend plotting 3 cross-sectional vector fields \nx-y, x-z y-z planes. plot, first thing consider \ncross-sections taken 3D space. recommend cross-sections\nmidpoint direction adjusting exact slices appropriate.\ncode , create 3D vector field, show extract midpoints\naxis, plot vector fields cross-sections midpoints.summary, visualizing 3D vector fields, avoid plotting 3D.\nInstead, consider one dimension small contribution , ,\neliminate plot. also think whether symmetries \nvector field allow exclude dimension plot. neither option \npossible, take cross-sections midpoints axes plot \nflows 2D.","code":"\nvector_frame = data.frame(x_axis = numeric(), y_axis = numeric(), z_axis = numeric()) \n\n# Generating a vector field in 3D space\nfor(i in -5:5) {\n  for(j in -5:5) {\n    for(k in -5:5) {\n      vec <- c(i, j, k) \n      vector_frame[nrow(vector_frame) + 1, ] <- vec\n    }\n  }\n}\n\nvector_frame$x_pull <- with(vector_frame, -x_axis/(sqrt((x_axis^2) + (y_axis^2) + (z_axis^2) ) + 1))\nvector_frame$y_pull <- with(vector_frame, y_axis/(sqrt((x_axis^2) + (y_axis^2) + (z_axis^2) ) ))\nvector_frame$z_pull <- with(vector_frame, -z_axis/(sqrt((x_axis^2) + (y_axis^2) + (z_axis^2) ) +0.5))\n\n# Finding midpoints of each axis\nx_range <- range(vector_frame$x_axis)\nmid_x <- (x_range[2] + x_range[1]) / 2\n\ny_range <- range(vector_frame$x_axis)\nmid_y <- (y_range[2] + y_range[1]) / 2\n\nz_range <- range(vector_frame$z_axis)\nmid_z <- (z_range[2] + z_range[1]) / 2\n\n# Extracting each cross-section\nxy_plot <- vector_frame[vector_frame$z_axis == mid_z,]\nxz_plot <- vector_frame[vector_frame$y_axis == mid_y,]\nyz_plot <- vector_frame[vector_frame$x_axis == mid_x,]\n\n#Plotting the 3 cross-sections\nxy <- ggplot(xy_plot, aes(x = x_axis, y = y_axis) )+\n        scale_x_continuous(breaks = seq(-5,5,1)) + \n        scale_y_continuous(breaks = seq(-5,5,1)) +\n        geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = y_axis + (y_pull)), \n                     arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n        geom_vline(xintercept=0, size=0.15) + geom_hline(yintercept=0, size=0.15) +\n        ggtitle(\"x-y cross-section\") +\n        coord_fixed() +\n        theme(text=element_text(size=8))\n\nxz <- ggplot(xz_plot, aes(x = x_axis, y = z_axis) )+\n        scale_x_continuous(breaks = seq(-5,5,1)) + \n        scale_y_continuous(breaks = seq(-5,5,1)) +\n        geom_segment(aes(xend = x_axis + (x_pull), \n                     yend = z_axis + (z_pull)), \n                     arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n        geom_vline(xintercept=0, size=0.15) + geom_hline(yintercept=0, size=0.15) +\n        ggtitle(\"x-z cross-section\") +\n        coord_fixed() +\n        theme(text=element_text(size=8))\n\nyz <- ggplot(yz_plot, aes(x = y_axis, y = z_axis) )+\n        scale_x_continuous(breaks = seq(-5,5,1)) + \n        scale_y_continuous(breaks = seq(-5,5,1)) +\n        geom_segment(aes(xend = y_axis + (y_pull), \n                     yend = z_axis + (z_pull)), \n                     arrow = arrow(length = unit(0.1, \"cm\")), size = 0.25) +\n        geom_vline(xintercept=0, size=0.15) + geom_hline(yintercept=0, size=0.15) +\n        ggtitle(\"y-z cross-section\") +\n        coord_fixed() +\n        theme(text=element_text(size=8))\n\nxy + xz + yz + plot_layout(ncol=2)"},{"path":"tutorial-for-vector-fields-in-r.html","id":"application","chapter":"22 Tutorial for vector fields in r","heading":"22.5 Application","text":"Now ’ve covered fundamentals plotting vector fields, briefly\ndiscuss applications.","code":""},{"path":"tutorial-for-vector-fields-in-r.html","id":"flow","chapter":"22 Tutorial for vector fields in r","heading":"22.5.1 Flow","text":"Using vector fields visualize flow extremely important fluid mechanics.\nExamples application include:Identifying regions turbulence designing airplanes race carsDiscovering sources sinks pressure fields meteorologyModelling flow blood vessels stents","code":""},{"path":"tutorial-for-vector-fields-in-r.html","id":"stability-analysis","chapter":"22 Tutorial for vector fields in r","heading":"22.5.2 Stability Analysis","text":"Stability analysis models long-term affects small perturbations initial\nconditions dynamical systems. Using differential equations describe\nenvironments, can plot vector fields system understand \nnature fixed points system (ie fixed points, stable, unstable,\netc…). figure illustrates several types fixed points:Source: https://www.sciencedirect.com/science/article/pii/S0021929018302239Examples applications stability analysis include modelling:Firing rates computational neuroscienceGene regulation networksPopulation dynamicsTherefore, given differential equations describing system, can generate\ndata frame vectors points environment. , using skills\nlearnt tutorial can plot vector fields, find fixed points \ndetermine nature.","code":""},{"path":"tutorial-for-vector-fields-in-r.html","id":"conclusion-2","chapter":"22 Tutorial for vector fields in r","heading":"22.5.3 Conclusion","text":"tutorial covered following topics:generate vector fields adjust componentsthe trade-arrow length arrow spacingmethods plotting vector fields 3 dimensionsapplications vector fields flow analysis stability analysisUsing tools, hope reader better understanding \nconstruct vector fields, elements consider creating high-quality vector\nfields applied real-world.","code":""},{"path":"tutorial-for-vector-fields-in-r.html","id":"reflection","chapter":"22 Tutorial for vector fields in r","heading":"22.5.4 Reflection","text":"research, limited documentation plotting vector fields R.\n, resources exist, Vectorfield Recipe,\nfeel tutorial dives deep quickly. Therefore, goal \ntutorial provide clear, easily accessible introduction building graphs\nvector fields R. Moreover, engineering students familiar \nvector fields, wanted share advice produce high-quality plots\nbriefly explain plots used research development., believe successfully captured basic principles \naccessible format readers hope tutorial contributes better\nunderstanding vector fields R. experience plotting vector\nfields Python, learned R. also learned \nexplicitly express elements consider plotting high-quality vector fields.terms work, think may useful use actual vector\nfield data sets, rather synthetically produced data, may provide\nreader practical exposure handling data. said ,\nmajority vector field data simulated, believe omission \nsignificant. Another area work include illustrative\ntutorials walking reader produce vector field plots specific\nflow analysis stability analysis. However, must careful making\nadditions analyses applications requires deep knowledge \ntopic, confuse reader fall trap tutorials\ndive “deep quick”.","code":""},{"path":"tutorial-for-vector-fields-in-r.html","id":"extra","chapter":"22 Tutorial for vector fields in r","heading":"22.5.5 Extra","text":"Overall, motivation project provide clear, easily accessible\ntutorial building vector graphs. engineering students, encountered\nvector graphs many classes, found existing documentation (Vectorfield Recipe) unnecessarily\nconfusing, also overlooking basics getting started. , learned\nmake vector graphs, feel successfully capture basic principles\naccessible format readers. don’t feel \nmajor changes ’d make improving article.","code":""},{"path":"data-cleaning-with-r.html","id":"data-cleaning-with-r","chapter":"23 Data cleaning with r","heading":"23 Data cleaning with r","text":"Jinchen Liu Yudu Chen","code":"\n#install.packages(openintro)\nlibrary(openintro)\n\n#install.packages('dplyr')\nlibrary(dplyr)\n\n#install.packages('tidyr')\nlibrary(tidyr)\n\n#install.packages(imputeTS)\nlibrary(imputeTS)\n\n#install.packages(caret)\nlibrary(caret)"},{"path":"data-cleaning-with-r.html","id":"data-cleaning-with-r-1","chapter":"23 Data cleaning with r","heading":"23.1 Data cleaning with R","text":"real world, data sets handling often data scientists can readily use. might contain duplicate entries entries supposed unique. might contain missing values, problematic tasks training predicative modeling. data features might vastly different scale, induces instability float point arithmatics inaccurate measurement feature importance training machine learning model. discuss use R clean Data sets situations take place.","code":""},{"path":"data-cleaning-with-r.html","id":"import-data-set","chapter":"23 Data cleaning with r","heading":"23.2 Import data set","text":"explore techniques data cleaning using “Airquality” dataset base R duke_forest data set openintro. data set available baseR require import outside source.","code":"\nairQuality_preProcessing <- airquality\n\nhead(airQuality_preProcessing, 10)##    Ozone Solar.R Wind Temp Month Day\n## 1     41     190  7.4   67     5   1\n## 2     36     118  8.0   72     5   2\n## 3     12     149 12.6   74     5   3\n## 4     18     313 11.5   62     5   4\n## 5     NA      NA 14.3   56     5   5\n## 6     28      NA 14.9   66     5   6\n## 7     23     299  8.6   65     5   7\n## 8     19      99 13.8   59     5   8\n## 9      8      19 20.1   61     5   9\n## 10    NA     194  8.6   69     5  10\nduke_forest_copy <- duke_forest"},{"path":"data-cleaning-with-r.html","id":"introduction","chapter":"23 Data cleaning with r","heading":"23.3 Introduction","text":"real world, data sets handling often data scientists can readily use. might contain duplicate entries entries supposed unique. might contain missing values, problematic tasks training predicative modeling. data features might vastly different scale, induces instability float point arithmatics inaccurate measurement feature importance training machine learning model. discuss use R clean Data sets situations take place.","code":""},{"path":"data-cleaning-with-r.html","id":"data-frame-modification-general-modification","chapter":"23 Data cleaning with r","heading":"23.4 Data frame modification (general modification)","text":"time, engineer data sets data analyzation . present tricks modify dat sets R.","code":""},{"path":"data-cleaning-with-r.html","id":"create-dataframe-by-arrays","chapter":"23 Data cleaning with r","heading":"23.4.1 Create DataFrame by arrays","text":"Creating DataFrame two arrays, two arrays columns df.Array length .order matters.another way create dataFrame, method produce result.","code":"\nfirst_column <- c(\"value_1\", \"value_2\", \"value_3\")\nsecond_column <- c(\"value_4\", \"value_5\", \"value_6\")\nthird_column <- c(\"value_7\", \"value_8\", \"value_9\")\ndf <- data.frame(first_column, second_column)\ndf##   first_column second_column\n## 1      value_1       value_4\n## 2      value_2       value_5\n## 3      value_3       value_6\ndf <- data.frame (first_column  = c(\"value_1\", \"value_2\", \"value_3\"),\n                  second_column = c(\"value_4\", \"value_5\", \"value_6\"),\n                  third_column = c(\"value_7\", \"value_8\", \"value_9\")\n                  )\ndf##   first_column second_column third_column\n## 1      value_1       value_4      value_7\n## 2      value_2       value_5      value_8\n## 3      value_3       value_6      value_9"},{"path":"data-cleaning-with-r.html","id":"create-dataframe-by-combining-two-dataframeusing-cbindrbind","chapter":"23 Data cleaning with r","heading":"23.4.2 Create DataFrame by combining two dataFrame(using cbind/rbind)","text":"combine two dataframe columns,order matters.combine two dataframe rows, two DataFrames must column name order.","code":"\n# create a new dataFrame\ndf1 <- data.frame (first_column  = c(\"value_1\", \"value_2\", \"value_3\"),\n                  second_column = c(\"value_4\", \"value_5\", \"value_6\"),\n                  third_column = c(\"value_7\", \"value_8\", \"value_9\")\n                  )\ndf2 <- data.frame (third_column  = c(\"value_7\", \"value_8\", \"value_9\"),\n                  fourth_column = c(\"value_10\", \"value_11\", \"value_12\")\n                  )\nbigger_df <- cbind(df1, df2)\nprint(bigger_df)##   first_column second_column third_column third_column fourth_column\n## 1      value_1       value_4      value_7      value_7      value_10\n## 2      value_2       value_5      value_8      value_8      value_11\n## 3      value_3       value_6      value_9      value_9      value_12\ndf3 <- data.frame (third_column = c(\"value_7\", \"value_8\", \"value_9\"),\n                  fourth_column = c(\"value_10\", \"value_11\", \"value_12\")\n                  )\nbigger_df2 <- rbind(df2,df3)\nprint(bigger_df2)##   third_column fourth_column\n## 1      value_7      value_10\n## 2      value_8      value_11\n## 3      value_9      value_12\n## 4      value_7      value_10\n## 5      value_8      value_11\n## 6      value_9      value_12"},{"path":"data-cleaning-with-r.html","id":"addingchange-row-name","chapter":"23 Data cleaning with r","heading":"23.4.3 adding/change row name","text":"","code":"\nprint(bigger_df2)##   third_column fourth_column\n## 1      value_7      value_10\n## 2      value_8      value_11\n## 3      value_9      value_12\n## 4      value_7      value_10\n## 5      value_8      value_11\n## 6      value_9      value_12\nrownames(bigger_df2) <- LETTERS[16:21]\n\nbigger_df2 <- data.frame(bigger_df2,\n                   row.names = LETTERS[16:21])\n# both ways produce the same result\nprint(bigger_df2)##   third_column fourth_column\n## P      value_7      value_10\n## Q      value_8      value_11\n## R      value_9      value_12\n## S      value_7      value_10\n## T      value_8      value_11\n## U      value_9      value_12\n# rownames can change the row name.\nrownames(bigger_df2) <- 1:6\nprint(bigger_df2)##   third_column fourth_column\n## 1      value_7      value_10\n## 2      value_8      value_11\n## 3      value_9      value_12\n## 4      value_7      value_10\n## 5      value_8      value_11\n## 6      value_9      value_12\n## get row names\nrownames(bigger_df2)## [1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\""},{"path":"data-cleaning-with-r.html","id":"changing-column-names","chapter":"23 Data cleaning with r","heading":"23.4.4 changing column names","text":"","code":"\nprint(df)##   first_column second_column third_column\n## 1      value_1       value_4      value_7\n## 2      value_2       value_5      value_8\n## 3      value_3       value_6      value_9\n# change all column names\ncolnames(df) <- c('C1','C2')\nprint(df)##        C1      C2      NA\n## 1 value_1 value_4 value_7\n## 2 value_2 value_5 value_8\n## 3 value_3 value_6 value_9\n# change specific column name\ncolnames(df)[1] <- c('new column name')\nprint(df)##   new column name      C2      NA\n## 1         value_1 value_4 value_7\n## 2         value_2 value_5 value_8\n## 3         value_3 value_6 value_9\n#get column names\ncolnames(df)## [1] \"new column name\" \"C2\"              NA"},{"path":"data-cleaning-with-r.html","id":"adding-array-to-to-dataframe-as-new-column","chapter":"23 Data cleaning with r","heading":"23.4.5 Adding array to to dataFrame as new column","text":"","code":"\nprint(bigger_df2)##   third_column fourth_column\n## 1      value_7      value_10\n## 2      value_8      value_11\n## 3      value_9      value_12\n## 4      value_7      value_10\n## 5      value_8      value_11\n## 6      value_9      value_12\nbigger_df_new = bigger_df2\n\nbigger_df_new$new <- c(3, 3, 6, 7, 8, 12)\n\nbigger_df_new['new'] <- c(3, 3, 6, 7, 8, 12)\n\nnew <- c(3, 3, 6, 7, 8, 12)\nbigger_df_new <- cbind(df, new)\n# three ways produce the same result.\nprint(bigger_df_new)##   new column name      C2      NA new\n## 1         value_1 value_4 value_7   3\n## 2         value_2 value_5 value_8   3\n## 3         value_3 value_6 value_9   6\n## 4         value_1 value_4 value_7   7\n## 5         value_2 value_5 value_8   8\n## 6         value_3 value_6 value_9  12"},{"path":"data-cleaning-with-r.html","id":"adding-column-by-combination-of-other-columns","chapter":"23 Data cleaning with r","heading":"23.4.6 adding column by combination of other columns","text":"","code":"\n# using mutate() from dplyr\ndf_math <- data.frame (first_column  = c(1, 2, 3),\n                  second_column = c(4, 5, 6)\n                  )\n\n## add new column as mathematical operation of other columns.\noutput <- mutate(df_math,\n                 sum = (first_column + second_column) / 2)\nprint(output)##   first_column second_column sum\n## 1            1             4 2.5\n## 2            2             5 3.5\n## 3            3             6 4.5\n## add new column by boolean operation of other columns.\noutput <- mutate(df_math,\n                 divisible_by2 = case_when(\n                   first_column%%2 == 0 ~ \"yes\",\n                   TRUE ~ \"No\" # otherwise not divisiable by 2.\n                 ))\nprint(output)##   first_column second_column divisible_by2\n## 1            1             4            No\n## 2            2             5           yes\n## 3            3             6            No"},{"path":"data-cleaning-with-r.html","id":"joining-two-table-2","chapter":"23 Data cleaning with r","heading":"23.4.7 joining two table [2]","text":"Bewaring order dataframe merge function matters(right join left)","code":"\nemp_df=data.frame(\n  emp_id=c(1,2,3,4,5,6),\n  name=c(\"Smith\",\"Rose\",\"Williams\",\"Jones\",\"Brown\",\"Brown\"),\n  superior_emp_id=c(-1,1,1,2,2,2),\n  dept_id=c(10,20,10,10,40,50),\n  dept_branch_id= c(101,102,101,101,104,105)\n)\n\ndept_df=data.frame(\n  dept_id=c(10,20,30,40),\n  dept_name=c(\"Finance\",\"Marketing\",\"Sales\",\"IT\"),\n  dept_branch_id= c(101,102,103,104)\n)\n# inner join\nprint(merge(x = emp_df, y = dept_df, by = \"dept_id\"))##   dept_id emp_id     name superior_emp_id dept_branch_id.x dept_name\n## 1      10      1    Smith              -1              101   Finance\n## 2      10      3 Williams               1              101   Finance\n## 3      10      4    Jones               2              101   Finance\n## 4      20      2     Rose               1              102 Marketing\n## 5      40      5    Brown               2              104        IT\n##   dept_branch_id.y\n## 1              101\n## 2              101\n## 3              101\n## 4              102\n## 5              104\n# outer join \nprint(merge(x = emp_df, y = dept_df, by = \"dept_id\", all = TRUE)) # all means containing all rows##   dept_id emp_id     name superior_emp_id dept_branch_id.x dept_name\n## 1      10      1    Smith              -1              101   Finance\n## 2      10      3 Williams               1              101   Finance\n## 3      10      4    Jones               2              101   Finance\n## 4      20      2     Rose               1              102 Marketing\n## 5      30     NA     <NA>              NA               NA     Sales\n## 6      40      5    Brown               2              104        IT\n## 7      50      6    Brown               2              105      <NA>\n##   dept_branch_id.y\n## 1              101\n## 2              101\n## 3              101\n## 4              102\n## 5              103\n## 6              104\n## 7               NA\n# left join\nprint(merge(x = emp_df, y = dept_df, by = \"dept_id\", all.x = TRUE)) # all.x means containing all rows in x##   dept_id emp_id     name superior_emp_id dept_branch_id.x dept_name\n## 1      10      1    Smith              -1              101   Finance\n## 2      10      3 Williams               1              101   Finance\n## 3      10      4    Jones               2              101   Finance\n## 4      20      2     Rose               1              102 Marketing\n## 5      40      5    Brown               2              104        IT\n## 6      50      6    Brown               2              105      <NA>\n##   dept_branch_id.y\n## 1              101\n## 2              101\n## 3              101\n## 4              102\n## 5              104\n## 6               NA\n# right join\nprint(merge(x = emp_df, y = dept_df, by = \"dept_id\", all.y = TRUE)) # all.y means containing all rows in y##   dept_id emp_id     name superior_emp_id dept_branch_id.x dept_name\n## 1      10      1    Smith              -1              101   Finance\n## 2      10      3 Williams               1              101   Finance\n## 3      10      4    Jones               2              101   Finance\n## 4      20      2     Rose               1              102 Marketing\n## 5      30     NA     <NA>              NA               NA     Sales\n## 6      40      5    Brown               2              104        IT\n##   dept_branch_id.y\n## 1              101\n## 2              101\n## 3              101\n## 4              102\n## 5              103\n## 6              104"},{"path":"data-cleaning-with-r.html","id":"pivoting-dataframe-understanding-pivot_longer","chapter":"23 Data cleaning with r","heading":"23.4.8 pivoting dataframe, understanding pivot_longer","text":"","code":"\n# pivot_longer transform a data frame from a wide format to a long format by converting feature names to a categorical feature.\n\n# pivot_longer is from the package tidyr.\nneeds_pivoting <- data.frame(sticker_type=c('A', 'B', 'C', 'D'),\n                 sparrow=c(12, 15, 19, 19),\n                 eagle=c(22, 29, 18, 12))\n\nprint(needs_pivoting)##   sticker_type sparrow eagle\n## 1            A      12    22\n## 2            B      15    29\n## 3            C      19    18\n## 4            D      19    12\nneeds_pivoting %>% pivot_longer(cols=c('sparrow', 'eagle'), # the columns(feature names) to be pivoted\n                                names_to='species', # the name of the column of features\n                                values_to='price') # the name of the column of values## # A tibble: 8 × 3\n##   sticker_type species price\n##   <chr>        <chr>   <dbl>\n## 1 A            sparrow    12\n## 2 A            eagle      22\n## 3 B            sparrow    15\n## 4 B            eagle      29\n## 5 C            sparrow    19\n## 6 C            eagle      18\n## 7 D            sparrow    19\n## 8 D            eagle      12"},{"path":"data-cleaning-with-r.html","id":"binning-using-cut","chapter":"23 Data cleaning with r","heading":"23.4.8.1 binning using cut()","text":"also good way categorize feature","code":"\nhour_df <- data.frame(shop_name=c('MAC', 'Tangro', 'cummington', 'Burger King', 'judgement', 'KFC', 'ye', 'Dungeon', 'Razer', 'yeah sir', 'Koban wife', 'string'),\n                 operating_hours=c(2, 5, 4, 7, 7, 8, 5, 4, 5, 11, 13, 8),\n                 rebounds=c(7, 7, 4, 6, 3, 8, 9, 9, 12, 11, 8, 9))\nhour_df##      shop_name operating_hours rebounds\n## 1          MAC               2        7\n## 2       Tangro               5        7\n## 3   cummington               4        4\n## 4  Burger King               7        6\n## 5    judgement               7        3\n## 6          KFC               8        8\n## 7           ye               5        9\n## 8      Dungeon               4        9\n## 9        Razer               5       12\n## 10    yeah sir              11       11\n## 11  Koban wife              13        8\n## 12      string               8        9\nnew_hour_df <- hour_df %>% mutate(operating_hours_bin = cut(operating_hours, breaks=c(0,5,9,13)))\nnew_hour_df##      shop_name operating_hours rebounds operating_hours_bin\n## 1          MAC               2        7               (0,5]\n## 2       Tangro               5        7               (0,5]\n## 3   cummington               4        4               (0,5]\n## 4  Burger King               7        6               (5,9]\n## 5    judgement               7        3               (5,9]\n## 6          KFC               8        8               (5,9]\n## 7           ye               5        9               (0,5]\n## 8      Dungeon               4        9               (0,5]\n## 9        Razer               5       12               (0,5]\n## 10    yeah sir              11       11              (9,13]\n## 11  Koban wife              13        8              (9,13]\n## 12      string               8        9               (5,9]"},{"path":"data-cleaning-with-r.html","id":"binning-using-case","chapter":"23 Data cleaning with r","heading":"23.4.8.2 binning using case()","text":"","code":"\nhour_df2 <- data.frame(shop_name=c('MAC', 'Tangro', 'cummington', 'Burger King', 'judgement', 'KFC', 'ye', 'Dungeon', 'Razer', 'yeah sir', 'Koban wife', 'string'),\n                 operating_hours=c(2, 5, 4, 7, 7, 8, 5, 4, 5, 11, 13, 8),\n                 rebounds=c(7, 7, 4, 6, 3, 8, 9, 9, 12, 11, 8, 9))\nhour_df##      shop_name operating_hours rebounds\n## 1          MAC               2        7\n## 2       Tangro               5        7\n## 3   cummington               4        4\n## 4  Burger King               7        6\n## 5    judgement               7        3\n## 6          KFC               8        8\n## 7           ye               5        9\n## 8      Dungeon               4        9\n## 9        Razer               5       12\n## 10    yeah sir              11       11\n## 11  Koban wife              13        8\n## 12      string               8        9\nnew_hour_df <- hour_df2 %>% mutate(operating_hours_bin = case_when( # logistics\n                   operating_hours <= 3 ~ 'very short',\n                   operating_hours <= 6 & operating_hours > 3 ~ 'short',\n                   operating_hours <= 10 & operating_hours > 6 ~ 'median',\n                   operating_hours > 10 ~ 'long',\n                   TRUE ~ 'what else will this be?'\n                 ))\nnew_hour_df##      shop_name operating_hours rebounds operating_hours_bin\n## 1          MAC               2        7          very short\n## 2       Tangro               5        7               short\n## 3   cummington               4        4               short\n## 4  Burger King               7        6              median\n## 5    judgement               7        3              median\n## 6          KFC               8        8              median\n## 7           ye               5        9               short\n## 8      Dungeon               4        9               short\n## 9        Razer               5       12               short\n## 10    yeah sir              11       11                long\n## 11  Koban wife              13        8                long\n## 12      string               8        9              median"},{"path":"data-cleaning-with-r.html","id":"ordering","chapter":"23 Data cleaning with r","heading":"23.4.9 ordering","text":"","code":"\nstudent_result_wild=data.frame(name=c(\"Ram\",\"Geeta\",\"John\",\"Paul\",\n                                 \"Cassie\",\"Geeta\",\"Paul\"),\n                          maths=c(7,8,8,9,10,8,9),\n                          science=c(5,7,6,8,9,7,8),\n                          history=c(7,7,7,7,7,7,7),\n                          id = c(9,2,3,5,13,2,5))\nstudent_result_wild # data with out ordering##     name maths science history id\n## 1    Ram     7       5       7  9\n## 2  Geeta     8       7       7  2\n## 3   John     8       6       7  3\n## 4   Paul     9       8       7  5\n## 5 Cassie    10       9       7 13\n## 6  Geeta     8       7       7  2\n## 7   Paul     9       8       7  5\n# order data by certain variable (ascending)\nstudent_result_wild[order(student_result_wild$id),]##     name maths science history id\n## 2  Geeta     8       7       7  2\n## 6  Geeta     8       7       7  2\n## 3   John     8       6       7  3\n## 4   Paul     9       8       7  5\n## 7   Paul     9       8       7  5\n## 1    Ram     7       5       7  9\n## 5 Cassie    10       9       7 13\n# order data by certain variable (descending)\nstudent_result_wild[order(-student_result_wild$id),]##     name maths science history id\n## 5 Cassie    10       9       7 13\n## 1    Ram     7       5       7  9\n## 4   Paul     9       8       7  5\n## 7   Paul     9       8       7  5\n## 3   John     8       6       7  3\n## 2  Geeta     8       7       7  2\n## 6  Geeta     8       7       7  2\n# order data by multiple variable, the second orders the duplicates in first variable\nstudent_result_wild[order(-student_result_wild$id, student_result_wild$science),]##     name maths science history id\n## 5 Cassie    10       9       7 13\n## 1    Ram     7       5       7  9\n## 4   Paul     9       8       7  5\n## 7   Paul     9       8       7  5\n## 3   John     8       6       7  3\n## 2  Geeta     8       7       7  2\n## 6  Geeta     8       7       7  2"},{"path":"data-cleaning-with-r.html","id":"duplicated-values","chapter":"23 Data cleaning with r","heading":"23.5 Duplicated Values","text":"many cases, observe duplicated values data set every instances supposed unique. now discussed handle duplicates.","code":""},{"path":"data-cleaning-with-r.html","id":"data-set-with-row-wise-duplicates","chapter":"23 Data cleaning with r","heading":"23.5.1 Data set with row-wise duplicates","text":"Although dataset use example, airquality data, duplicate, use illustrate techniques handling duplicate data.see data frame duplicate rows begin .Now randomly pick 5 instances data frame insert data frame induce duplicationWe now 5 duplicated instances:“duplicated(df)” returns boolean array value index indicates row index original data frame duplicated . can use ’duplicated(df)” extract duplicated rows:adding “!” “duplicated(df)”, can negate logics “duplicated(df)” access non duplicate rows data frame:duplicatesWe can create new reference data set duplicates. purpose reusing “data_duplicated_values”, just assign new reference :Now “data_duplicated_values” duplicates:Another way use “distinct()” function tidyverse package.","code":"\ndata_duplicated_values = airQuality_preProcessing\nprint(paste0('number of duplicated rows in the data is ',sum(duplicated(data_duplicated_values))))## [1] \"number of duplicated rows in the data is 0\"\nfor (i in 1: 5)\n{\n  data_duplicated_values[nrow(data_duplicated_values)+1,] = data_duplicated_values[floor(runif(1, min = 1, max = nrow(data_duplicated_values))),]\n}\nprint(paste0('number of duplicated rows in the data after insertion is ',sum(duplicated(data_duplicated_values))))## [1] \"number of duplicated rows in the data after insertion is 5\"\ndata_duplicated_values[duplicated(data_duplicated_values),]##     Ozone Solar.R Wind Temp Month Day\n## 154    NA     291 14.9   91     7  14\n## 155    36     118  8.0   72     5   2\n## 156    50     275  7.4   86     7  29\n## 157    NA     264 14.3   79     6   6\n## 158   118     225  2.3   94     8  29\nhead(data_duplicated_values[!duplicated(data_duplicated_values),], 10)##    Ozone Solar.R Wind Temp Month Day\n## 1     41     190  7.4   67     5   1\n## 2     36     118  8.0   72     5   2\n## 3     12     149 12.6   74     5   3\n## 4     18     313 11.5   62     5   4\n## 5     NA      NA 14.3   56     5   5\n## 6     28      NA 14.9   66     5   6\n## 7     23     299  8.6   65     5   7\n## 8     19      99 13.8   59     5   8\n## 9      8      19 20.1   61     5   9\n## 10    NA     194  8.6   69     5  10\nprint(paste0('duplicates in data frame formed by unique rows in original data frame is ',sum(duplicated(data_duplicated_values[!duplicated(data_duplicated_values),]))))## [1] \"duplicates in data frame formed by unique rows in original data frame is 0\"\ndata_duplicated_values <- data_duplicated_values[!duplicated(data_duplicated_values),]\nprint(paste0('number of duplicated value after re setting reference is ', sum(duplicated(data_duplicated_values))))## [1] \"number of duplicated value after re setting reference is 0\"\nfor (i in 1: 5)\n{\n  data_duplicated_values[nrow(data_duplicated_values)+1,] = data_duplicated_values[floor(runif(1, min = 1, max = nrow(data_duplicated_values))),]\n}\nprint(paste0('number of duplicate value in data frame before calling distinct is', sum(duplicated(data_duplicated_values))))## [1] \"number of duplicate value in data frame before calling distinct is5\"\ndata_duplicated_values <- data_duplicated_values %>% distinct()\nprint(paste0('number of duplicate value in data frame after calling distinct is', sum(duplicated(data_duplicated_values))))## [1] \"number of duplicate value in data frame after calling distinct is0\""},{"path":"data-cleaning-with-r.html","id":"duplicates-based-on-specific-columns","chapter":"23 Data cleaning with r","heading":"23.5.2 Duplicates based on specific columns","text":"times duplication elements certain column/columns desirable. want able remove rows duplication specified columns. first insert 5 rows duplicates “Day” “Month”, see “158” rows insertion:can remove rows duplicated “Day” “Month” combination using “distinct()”. “.keep_all = TRUE” argument makes sure duplicated combination Day Month, keep first row every duplicated combination keep variables Data. observe number rows dropping dupicated columns going back “153”, size data frame insert rows duplicated “Day” “Month” combinations.","code":"\nprint(paste0('number of rows before adding 5 duplicated rows on Day and Month is', nrow(data_duplicated_values)))## [1] \"number of rows before adding 5 duplicated rows on Day and Month is153\"\nfor (i in 1: 5)\n{\n  Ozone <- floor(runif(1,min = 0, max = 50))\n  Solor <- floor(runif(1,min = 0, max = 300))\n  Wind <- round(runif(1,min = 0, max = 20), 2)\n  Temp <- floor(runif(1,min = 0, max = 20))\n  \n  random_index = floor(runif(1, min = 1, max = nrow(data_duplicated_values)))\n  \n  Month <- data_duplicated_values[random_index, 'Month']\n  Day <- data_duplicated_values[random_index, 'Day']\n  data_duplicated_values[nrow(data_duplicated_values)+1,] = c(Ozone, Solor, Wind, Temp, Month, Day)\n  \n}\n\nprint(paste0('number of rows after adding 5 duplicated rows on Day and Month is', nrow(data_duplicated_values)))## [1] \"number of rows after adding 5 duplicated rows on Day and Month is158\"\ndata_duplicated_values<- data_duplicated_values %>% distinct(Day, Month, .keep_all = TRUE)\nprint(paste0('number of rows after removing 5 duplicated rows on Dat and Month is', nrow(data_duplicated_values)))## [1] \"number of rows after removing 5 duplicated rows on Dat and Month is153\""},{"path":"data-cleaning-with-r.html","id":"missing-values","chapter":"23 Data cleaning with r","heading":"23.6 Missing Values","text":"Encountering missing data dataset uncommon. collecting temperature data sensor might broken unable measure temperature. conduct public opinion surveying, interviewee might forget filling entires questionaires. Many data science job requires completeness data, training predicative model based numerical/categorical data. now discuss data cleaning R.","code":""},{"path":"data-cleaning-with-r.html","id":"na-in-r","chapter":"23 Data cleaning with r","heading":"23.6.1 NA in R","text":"R, missing value represented symbol “NA”.can observe, index 5, instance missing value “Ozone” “Solar.R” feasure, represented symbol “NA”.\n### Dropping rows missing data\nsimple approach drop rows missing data. task requires using features dataset, can opt drop rows least one missing value. However, task required using features, need drop rows missing values feastures specify. discuss cases.\n#### Drop rows containing missing Data\nSuppose want use features “airquality” dataset want drop rows least 1 missing values. first check number rows missing data dataset. ‘.na(df)’ command returns boolean array, truth value index indicate data element index data frame df NA . summing number “TRUE” values ‘.na(df)’, know total number missing values data set.see totally 44 missing least 1 missing values.\nknow number rows missing value, use “complete.cases(df)”. return array boolean boolean value index indicate data instance/case corresponding row index data frame df complete (NA missing value) . summing number false array, get total number rows least 1 missing value.visualize rows missing value, can access using boolean array “complete.cases()”. Due large number rows missing value, put 10 :now remove rows na. use “drop_na()” function tidyr library remove rows missing values. multiple ways drop rows missing value, go exhaustive .removing 42 rows missing values, 111 rows left, 42 rows removed total 153 rows original data frame.","code":"\nhead(airquality, 10)##    Ozone Solar.R Wind Temp Month Day\n## 1     41     190  7.4   67     5   1\n## 2     36     118  8.0   72     5   2\n## 3     12     149 12.6   74     5   3\n## 4     18     313 11.5   62     5   4\n## 5     NA      NA 14.3   56     5   5\n## 6     28      NA 14.9   66     5   6\n## 7     23     299  8.6   65     5   7\n## 8     19      99 13.8   59     5   8\n## 9      8      19 20.1   61     5   9\n## 10    NA     194  8.6   69     5  10\nprint(paste0('total number of missing values is ', sum(is.na(airQuality_preProcessing))))## [1] \"total number of missing values is 44\"\nprint(paste0(\"number of rows with at least 1 missing value is \", sum(!complete.cases(airQuality_preProcessing))))## [1] \"number of rows with at least 1 missing value is 42\"\nhead(airQuality_preProcessing[!complete.cases(airQuality_preProcessing), ], 10)\nairQuality_na_droped <- airQuality_preProcessing %>% drop_na()\nprint(paste0(\"number of missing values after we drop all NA is \",sum(is.na(airQuality_na_droped))))## [1] \"number of missing values after we drop all NA is 0\"\nprint(paste0(\"number of rows in data frame after extracting rows with NA is \", nrow(airQuality_na_droped)))## [1] \"number of rows in data frame after extracting rows with NA is 111\""},{"path":"data-cleaning-with-r.html","id":"drop-rows-with-missing-data-in-specified-columns","chapter":"23 Data cleaning with r","heading":"23.6.1.1 Drop rows with missing data in specified columns","text":"","code":""},{"path":"data-cleaning-with-r.html","id":"na-in-single-column","chapter":"23 Data cleaning with r","heading":"23.6.1.1.1 NA in single column","text":"can see dropping columns missing data wasteful don’t use features dataset. example, job require feature “Ozone”, don’t really care “Ozone” value missing irrelevant task. explore drop rows na specific columns.see “.na(df)” evaluate presence NA(missing value) entire data set. change argument df, entire data set, specific column, function evaluate specific columns data set return boolean array reflects presence missing value columns. example, want see number rows missing value “Ozone” column:37 rows NA “Ozone” column. drop rows missing value “Ozone”, put column name “Ozone” argument:37 rows extracted 153 columns, resulting 116 rows.","code":"\nprint(paste0('number of NA in Ozone column is ', sum(is.na(airQuality_preProcessing$Ozone))))## [1] \"number of NA in Ozone column is 37\"\nairQuality_na_droped_Ozone <- airQuality_preProcessing %>% drop_na(Ozone)\nprint(nrow(airQuality_na_droped_Ozone))## [1] 116"},{"path":"data-cleaning-with-r.html","id":"na-in-several-columns-and","chapter":"23 Data cleaning with r","heading":"23.6.1.1.2 NA in several columns (And)","text":"want drop rows NA specific column, example, rows NA “Ozone” “Solar.R” columns, can take advantage fact “.na(df)” boolean array, can perform element wise boolean operation two arrays dimension:see number rows missing value “Ozone” “Solar.R” 2. now visualize two rows:boolean array, can negate logics obtain rows don’t missing values “Ozone” “Solar.R” time.droped two rows 153 rows original dataset, 151 rows left.","code":"\nsum(is.na(airQuality_preProcessing$Ozone) & is.na(airQuality_preProcessing$Solar.R))## [1] 2\nairQuality_preProcessing[is.na(airQuality_preProcessing$Ozone) & is.na(airQuality_preProcessing$Solar.R), ]##    Ozone Solar.R Wind Temp Month Day\n## 5     NA      NA 14.3   56     5   5\n## 27    NA      NA  8.0   57     5  27\nairQuality_NA_Ozone_and_Solor <- airQuality_preProcessing[!(is.na(airQuality_preProcessing$Ozone) & is.na(airQuality_preProcessing$Solar.R)), ]\nnrow(airQuality_NA_Ozone_and_Solor)## [1] 151"},{"path":"data-cleaning-with-r.html","id":"na-in-several-columns-or","chapter":"23 Data cleaning with r","heading":"23.6.1.1.3 NA in several columns (Or)","text":"want drop columns NA “Ozone” “Solar.R”, two columns contains missing values. used boolean operation several boolean arrays , tedious feastures consider. use “complete.cases()” use specific columns dataset input. “complete.cases()” returns false single column rows data frame pass missing value, perform “” operation .42 rows missing values either “Ozone” “Solar.R”, . fact, missing values appear two columns, get total number rows missing values data set. can obtain datas missing value “Ozone” “Solar.R”.111 rows removing 42 rows missing value either “Ozone” “Solar.R”can also use drop_na() columns specified function arguement:","code":"\nsum(!(complete.cases(airQuality_preProcessing[,c(\"Ozone\",\"Solar.R\")])))## [1] 42\nairQuality_NA_Ozone_or_Solor<- airQuality_preProcessing[(complete.cases(airQuality_preProcessing[,c(\"Ozone\",\"Solar.R\")])), ]\nnrow(airQuality_NA_Ozone_or_Solor)## [1] 111\nairQuality_NA_Ozone_or_Solor<- airQuality_preProcessing %>% drop_na(c(\"Ozone\", \"Solar.R\"))\nnrow(airQuality_NA_Ozone_or_Solor)## [1] 111"},{"path":"data-cleaning-with-r.html","id":"drop-columns-with-certain-number-of-na","chapter":"23 Data cleaning with r","heading":"23.6.1.1.4 Drop columns with certain number of NA","text":"might uncommon, might want drop rows certain number NA, rows 3 NA even rows number NA equal number columns, means row column value NA. know number NA row, can use “rowSums()” functions:means row 1 row 10, row 5 2 NA row 6 10 1 NA. can drop rows number NA certain threshold. now insert 3 rows 3 NA data frame.Now last 3 rows 3 NA tail airQuality_3NA_Inserted. now check number rows equal 3 NA:Now 3 rows least 3 NA. now drop rows least 3 NA airQuality_3NA_Inserted selecting rows NA less 3 reset reference airQuality_3NA_Inserted:drop 3 rows least 3 NA, size airQuality_3NA_Inserted drops 156 153.","code":"\nhead(rowSums(is.na(airQuality_preProcessing)),10)##  [1] 0 0 0 0 2 1 0 0 0 1\nairQuality_3NA_Inserted <- airQuality_preProcessing\nfor (i in 1: 3)\n{\n  Ozone <- floor(runif(1,min = 0, max = 50))\n  Solor <- floor(runif(1,min = 0, max = 300))\n  Wind <- round(runif(1,min = 0, max = 20), 2)\n  Temp <- floor(runif(1,min = 0, max = 20))\n  \n  random_index = floor(runif(1, min = 1, max = nrow(airQuality_3NA_Inserted)))\n  \n  Month <- airQuality_3NA_Inserted [random_index, 'Month']\n  Day <- airQuality_3NA_Inserted [random_index, 'Day']\n  \n  \n  toInsert <- c(Ozone, Solor, Wind, Temp, Month, Day)\n  for (i in 1:3) {\n    random_index = floor(runif(1, min = 1, max = 6))\n    if (is.na(toInsert[random_index])) {\n      random_index = floor(runif(1, min = 1, max = 6))\n    }\n    toInsert <- replace(toInsert, random_index, NA)\n  }\n\n  airQuality_3NA_Inserted [nrow(airQuality_3NA_Inserted )+1,] <- toInsert\n  \n  \n}\ntail(airQuality_3NA_Inserted, 3)##     Ozone Solar.R Wind Temp Month Day\n## 154     2      NA 2.96   15    NA  23\n## 155    NA     103   NA   NA     9  29\n## 156    NA      NA 9.43    4    NA   8\nprint(paste('number of rows in airQuality_3NA_Inserted is', nrow(airQuality_3NA_Inserted)))## [1] \"number of rows in airQuality_3NA_Inserted is 156\"\nprint(paste0(\"number of rows with at least 3 NA in airQuality_3NA_Inserted is \", sum(rowSums(is.na(airQuality_3NA_Inserted)) >= 3)))## [1] \"number of rows with at least 3 NA in airQuality_3NA_Inserted is 2\"\nairQuality_3NA_Inserted <- airQuality_3NA_Inserted[!(rowSums(is.na(airQuality_3NA_Inserted)) >= 3), ]\n\nprint(paste0(\"number of rows in airQuality_3NA_Inserted after dropping is \", nrow(airQuality_3NA_Inserted)))## [1] \"number of rows in airQuality_3NA_Inserted after dropping is 154\""},{"path":"data-cleaning-with-r.html","id":"impute","chapter":"23 Data cleaning with r","heading":"23.6.2 Impute","text":"Dropping rows straight forward approach. However, dropping many drows induce huge data loss, detrimental task. can insert values specify entries missing values. minimize data loss. Though never know imputed values reflect true pattern data.","code":""},{"path":"data-cleaning-with-r.html","id":"impute-numerical-values","chapter":"23 Data cleaning with r","heading":"23.6.2.1 Impute numerical values","text":"","code":""},{"path":"data-cleaning-with-r.html","id":"impute-with-constant","chapter":"23 Data cleaning with r","heading":"23.6.2.1.1 Impute with constant","text":"column missing value,select constant think reflect real data fill missing entry column constant selected.Let’s say use constant 30 fill “Ozone” entries missing values.now fill:see NA Ozone column row 5 imputed constant 30.Pro\nEasy understand.\nEasy understand.Con\nValue assigned human intuition arbitrary might unrealistic.\nValue assigned human intuition arbitrary might unrealistic.","code":"\nairQuality_const_filled <- airQuality_preProcessing\nprint(paste0(\"number of missing values in Ozone column is \", sum(is.na(airQuality_const_filled$Ozone))))## [1] \"number of missing values in Ozone column is 37\"\nhead(airQuality_const_filled , 5)##   Ozone Solar.R Wind Temp Month Day\n## 1    41     190  7.4   67     5   1\n## 2    36     118  8.0   72     5   2\n## 3    12     149 12.6   74     5   3\n## 4    18     313 11.5   62     5   4\n## 5    NA      NA 14.3   56     5   5\nairQuality_const_filled$Ozone <- na_replace(airQuality_const_filled$Ozone, fill = 30, maxgap = Inf)\nprint(paste0(\"number of missing values in Ozone column is \", sum(is.na(airQuality_const_filled$Ozone))))## [1] \"number of missing values in Ozone column is 0\"\nhead(airQuality_const_filled , 5)##   Ozone Solar.R Wind Temp Month Day\n## 1    41     190  7.4   67     5   1\n## 2    36     118  8.0   72     5   2\n## 3    12     149 12.6   74     5   3\n## 4    18     313 11.5   62     5   4\n## 5    30      NA 14.3   56     5   5"},{"path":"data-cleaning-with-r.html","id":"impute-with-sample-statistics","chapter":"23 Data cleaning with r","heading":"23.6.2.1.2 Impute with Sample statistics","text":"can impute missing values column using sample statistics columns mean median. way fill missing value alter data statistics column. now fill “Ozone” column current “Mean” “Ozone” column “Solar.R” median “Solar.R” column. Note “na.rm = True” arguement neglect missing value computing sample statistics get real value answer.see longer missing values columns mean “Ozone” column median “Solar.R” unchanging impute.Pro\nEasy understand.\nchange sample statistics\nEasy understand.change sample statisticsCon\nMaybe distribution valid values columns skewed left right many outliers sample statistics reflect actual pattern data\nMaybe distribution valid values columns skewed left right many outliers sample statistics reflect actual pattern data","code":"\nairQuality_sstat_impute <- airQuality_preProcessing\nprint(paste0(\"Mean of Ozone before impute is \", mean(airQuality_sstat_impute$Ozone, na.rm = TRUE)))## [1] \"Mean of Ozone before impute is 42.1293103448276\"\nprint(paste0(\"Median of Solar.R before impute is \", median(airQuality_sstat_impute$Solar.R, na.rm = TRUE)))## [1] \"Median of Solar.R before impute is 205\"\nprint(paste0(\"number of missing values in Ozone column before impute is \", sum(is.na(airQuality_sstat_impute$Ozone))))## [1] \"number of missing values in Ozone column before impute is 37\"\nprint(paste0(\"number of missing values in Solar.R column before impute is \", sum(is.na(airQuality_sstat_impute$Solar.R))))## [1] \"number of missing values in Solar.R column before impute is 7\"\nairQuality_sstat_impute$Ozone <- na_mean(airQuality_sstat_impute$Ozone, option = \"mean\", maxgap = Inf)\nairQuality_sstat_impute$Solar.R <- na_mean(airQuality_sstat_impute$Solar, option = \"median\", maxgap = Inf)\n\nprint(paste0(\"Mean of Ozone after impute is \", mean(airQuality_sstat_impute$Ozone)))## [1] \"Mean of Ozone after impute is 42.1293103448276\"\nprint(paste0(\"Median of Solar.R after impute is \", median(airQuality_sstat_impute$Solar.R)))## [1] \"Median of Solar.R after impute is 205\"\nprint(paste0(\"number of missing values in Ozone column after impute is \", sum(is.na(airQuality_sstat_impute$Ozone))))## [1] \"number of missing values in Ozone column after impute is 0\"\nprint(paste0(\"number of missing values in Solar.R column after impute is \", sum(is.na(airQuality_sstat_impute$Solar.R))))## [1] \"number of missing values in Solar.R column after impute is 0\""},{"path":"data-cleaning-with-r.html","id":"impute-by-value-adjacent-to-missing-value","chapter":"23 Data cleaning with r","heading":"23.6.2.1.3 Impute by value adjacent to Missing value","text":"Often time-series data, value time point highly assosicated adjacent valid values. Therefore, want fill missing entries using valid data values .first fill missing values Ozone column last observed valid value:fill missing values Solar.R column next observed valid valueIf missing value beginning end data frame, rare, can remedy using “na_remaining” argument “na_locf()” handle remaining missing value filling. details can found imputeTS documentation.Pro\ntime series data, using adjacent value close capture time dependent pattern\ntime series data, using adjacent value close capture time dependent patternCon\ncloested adjacent value far away, filling adjacent value sustain time dependent pattern.\ncloested adjacent value far away, filling adjacent value sustain time dependent pattern.","code":"\nairQuality_Adjacent_impute <- airQuality_preProcessing\nhead(airQuality_Adjacent_impute, 10)##    Ozone Solar.R Wind Temp Month Day\n## 1     41     190  7.4   67     5   1\n## 2     36     118  8.0   72     5   2\n## 3     12     149 12.6   74     5   3\n## 4     18     313 11.5   62     5   4\n## 5     NA      NA 14.3   56     5   5\n## 6     28      NA 14.9   66     5   6\n## 7     23     299  8.6   65     5   7\n## 8     19      99 13.8   59     5   8\n## 9      8      19 20.1   61     5   9\n## 10    NA     194  8.6   69     5  10\nairQuality_Adjacent_impute$Ozone <- na_locf(airQuality_Adjacent_impute$Ozone, option = \"locf\")\nairQuality_Adjacent_impute$Solar.R <- na_locf(airQuality_Adjacent_impute$Solar.R, option = \"nocb\")\nhead(airQuality_Adjacent_impute, 10)##    Ozone Solar.R Wind Temp Month Day\n## 1     41     190  7.4   67     5   1\n## 2     36     118  8.0   72     5   2\n## 3     12     149 12.6   74     5   3\n## 4     18     313 11.5   62     5   4\n## 5     18     299 14.3   56     5   5\n## 6     28     299 14.9   66     5   6\n## 7     23     299  8.6   65     5   7\n## 8     19      99 13.8   59     5   8\n## 9      8      19 20.1   61     5   9\n## 10     8     194  8.6   69     5  10"},{"path":"data-cleaning-with-r.html","id":"impute-using-predicative-model","chapter":"23 Data cleaning with r","heading":"23.6.2.1.4 Impute using predicative model","text":"believe certain features data frame depends features, can use features fit predicative model predict missing value certain feature. need make sure “features” prediction based valid entries, need impute using method. now assume “Ozone” “Solar.R” features depends climate related features “Wind” “Temperature”, build linear regression models “Ozone” “Solar.R” based “Wind” “Temperature”.Since original data integer, now convert “Ozone” “Solar.R” back integer double.now filled missing values using linear regresison model prediction valid values columns.Pro\nfeature predict indeed depend features data set, missing value replace values reflect true pattern.\nfeature predict indeed depend features data set, missing value replace values reflect true pattern.Con\nDepending predicative model use, training prediction computationally expensive.\nfeature predict depend features data set select construct predicative model, filled missing value mis leading future analysis.\nDepending predicative model use, training prediction computationally expensive.feature predict depend features data set select construct predicative model, filled missing value mis leading future analysis.","code":"\nairQuality_lm_impute <- airQuality_preProcessing\n\n\nhead(airQuality_lm_impute, 10)##    Ozone Solar.R Wind Temp Month Day\n## 1     41     190  7.4   67     5   1\n## 2     36     118  8.0   72     5   2\n## 3     12     149 12.6   74     5   3\n## 4     18     313 11.5   62     5   4\n## 5     NA      NA 14.3   56     5   5\n## 6     28      NA 14.9   66     5   6\n## 7     23     299  8.6   65     5   7\n## 8     19      99 13.8   59     5   8\n## 9      8      19 20.1   61     5   9\n## 10    NA     194  8.6   69     5  10\nprint(paste0('number of missing values in Ozone column before impute is ', sum(is.na(airQuality_lm_impute$Ozone))))## [1] \"number of missing values in Ozone column before impute is 37\"\nprint(paste0('number of missing values in Solar.R column before impute is ', sum(is.na(airQuality_lm_impute$Solar.R))))## [1] \"number of missing values in Solar.R column before impute is 7\"\nlinear_model_Ozone <- lm(Ozone ~ Wind + Temp, data = airQuality_lm_impute)\nlinear_model_Solar.R <- lm(Solar.R ~ Wind + Temp, data = airQuality_lm_impute)\n\nairQuality_lm_impute$Ozone[is.na(airQuality_lm_impute$Ozone)] <- predict(linear_model_Ozone, \n                                                                         newdata = airQuality_lm_impute[is.na(airQuality_lm_impute$Ozone),c('Wind', 'Temp')])\n\n\nairQuality_lm_impute$Solar.R[is.na(airQuality_lm_impute$Solar.R)] <- predict(linear_model_Solar.R, \n                                                                         newdata = airQuality_lm_impute[is.na(airQuality_lm_impute$Solar.R),c('Wind', 'Temp')])\n\nhead(airQuality_lm_impute, 10)##        Ozone  Solar.R Wind Temp Month Day\n## 1   41.00000 190.0000  7.4   67     5   1\n## 2   36.00000 118.0000  8.0   72     5   2\n## 3   12.00000 149.0000 12.6   74     5   3\n## 4   18.00000 313.0000 11.5   62     5   4\n## 5  -11.67673 127.4317 14.3   56     5   5\n## 6   28.00000 159.5042 14.9   66     5   6\n## 7   23.00000 299.0000  8.6   65     5   7\n## 8   19.00000  99.0000 13.8   59     5   8\n## 9    8.00000  19.0000 20.1   61     5   9\n## 10  29.66190 194.0000  8.6   69     5  10\nairQuality_lm_impute$Ozone <- as.integer(airQuality_lm_impute$Ozone)\nairQuality_lm_impute$Solar.R <- as.integer(airQuality_lm_impute$Solar.R)\nhead(airQuality_lm_impute, 10)##    Ozone Solar.R Wind Temp Month Day\n## 1     41     190  7.4   67     5   1\n## 2     36     118  8.0   72     5   2\n## 3     12     149 12.6   74     5   3\n## 4     18     313 11.5   62     5   4\n## 5    -11     127 14.3   56     5   5\n## 6     28     159 14.9   66     5   6\n## 7     23     299  8.6   65     5   7\n## 8     19      99 13.8   59     5   8\n## 9      8      19 20.1   61     5   9\n## 10    29     194  8.6   69     5  10\nlinear_model_Ozone ## \n## Call:\n## lm(formula = Ozone ~ Wind + Temp, data = airQuality_lm_impute)\n## \n## Coefficients:\n## (Intercept)         Wind         Temp  \n##     -71.033       -3.055        1.840\nlinear_model_Solar.R## \n## Call:\n## lm(formula = Solar.R ~ Wind + Temp, data = airQuality_lm_impute)\n## \n## Coefficients:\n## (Intercept)         Wind         Temp  \n##     -76.362        2.211        3.075\nprint(paste0('number of missing values in Ozone column after impute is ', sum(is.na(airQuality_lm_impute$Ozone))))## [1] \"number of missing values in Ozone column after impute is 0\"\nprint(paste0('number of missing values in Solar.R column after impute is ', sum(is.na(airQuality_lm_impute$Solar.R))))## [1] \"number of missing values in Solar.R column after impute is 0\""},{"path":"data-cleaning-with-r.html","id":"impute-categorical-features","chapter":"23 Data cleaning with r","heading":"23.6.2.2 Impute categorical features","text":"imputing categorical features,Replacing NA categorical features stringReplacing NA categorical features mode [1]","code":"\nna_df <- data.frame(A = c(NA, 7, 8, 5, 3),\n                     B = c(4, 10, NA, 7, 4), \n                    fantasy = c(\"sad\", \"we\", NA, 'adf', 'NA'),\n                     C = c(1, 0, NA, 9, NA), \n                     D = c(\"tangro\", \"ok\", NA, 'yes', 'NA'))\ni1 <- !sapply(na_df, is.numeric)\n\nna_df[i1] <- lapply(na_df[i1], function(x)\n              replace(x, is.na(x), 'MISSING'))\nna_df##    A  B fantasy  C       D\n## 1 NA  4     sad  1  tangro\n## 2  7 10      we  0      ok\n## 3  8 NA MISSING NA MISSING\n## 4  5  7     adf  9     yes\n## 5  3  4      NA NA      NA\nna_df <- data.frame(A = c(NA, 7, 8, 5, 3),\n                     B = c(4, 10, NA, 7, 4), \n                    fantasy = c(\"sad\", \"we\", NA, 'adf', 'NA'),\n                     C = c(1, 0, NA, 9, NA), \n                     D = c(\"tangro\", \"ok\", NA, 'yes', 'NA'))\ni1 <- !sapply(na_df, is.numeric)\nMode <- function(x) { \n      ux <- sort(unique(x))\n      ux[which.max(tabulate(match(x, ux)))] \n}\nna_df[i1] <- lapply(na_df[i1], function(x)\n              replace(x, is.na(x), Mode(x[!is.na(x)])))\nna_df##    A  B fantasy  C      D\n## 1 NA  4     sad  1 tangro\n## 2  7 10      we  0     ok\n## 3  8 NA     adf NA     NA\n## 4  5  7     adf  9    yes\n## 5  3  4      NA NA     NA"},{"path":"data-cleaning-with-r.html","id":"data-scaling","chapter":"23 Data cleaning with r","heading":"23.7 Data scaling","text":"Scaling important data analysis exploration, Data without scaling can produce misleading result. Also scaling beneficial mathematics computation machine learning.","code":""},{"path":"data-cleaning-with-r.html","id":"standardization","chapter":"23 Data cleaning with r","heading":"23.7.1 standardization","text":"Standardization unify samples’ mean std 0 1, compare distribution among scaled samples.","code":"\n# we can use the scale from dplyr\nlibrary(dplyr)\nlibrary(openintro)\nscale_df = duke_forest_copy[, 2:5]\n# what happen if we did not scale.\nboxplot(scale_df)\n# after scaling\nscale_df %>% mutate_all(~(scale(.) %>% as.vector)) %>% boxplot() # apply scale() to every column"},{"path":"data-cleaning-with-r.html","id":"min-max-scaling","chapter":"23 Data cleaning with r","heading":"23.7.2 Min Max scaling","text":"MINMAX scaling good svm, don’t hesistate use :)","code":"\nminmax <- function(x, na.rm = TRUE) {\n    return((x- min(x)) /(max(x)-min(x)))\n}\nscale_df = duke_forest_copy[, 2:5]\n# what happen if we did not scale.\nboxplot(scale_df)\nscale_df %>% mutate_all(~(minmax(.) %>% as.vector)) %>% boxplot() # apply minmax() to every column"},{"path":"data-cleaning-with-r.html","id":"data-encoding","chapter":"23 Data cleaning with r","heading":"23.8 data encoding","text":"","code":""},{"path":"data-cleaning-with-r.html","id":"ordinal-encoding","chapter":"23 Data cleaning with r","heading":"23.8.1 ordinal encoding","text":"","code":"\nordinal <- function(x, order= unique(x)) {\n  x <- as.numeric(factor(x, levels = order, exclude = NULL))\n  x\n}\nencode_df <- data.frame(A = c(1, 7, 8, 5, 3),\n                     B = c(4, 10, NA, 7, 4), \n                    fantasy = c(\"sad\", \"we\", NA, 'adf', 'NA'),\n                     C = c(1, 0, NA, 9, NA), \n                     D = c(\"tangro\", \"ok\", 'NA', 'yes', 'NA'))\nprint('origin:')## [1] \"origin:\"\nc(encode_df[\"D\"])## $D\n## [1] \"tangro\" \"ok\"     \"NA\"     \"yes\"    \"NA\"\nprint('encoded:')## [1] \"encoded:\"\nordinal(encode_df[[\"D\"]])## [1] 1 2 3 4 3"},{"path":"data-cleaning-with-r.html","id":"one-hot-encoding-3","chapter":"23 Data cleaning with r","heading":"23.8.2 one-hot encoding [3]","text":"","code":"\nencode_df <- data.frame(A = c(1, 7, 8, 5, 3),\n                     B = c(4, 10, NA, 7, 4), \n                    fantasy = c(\"sad\", \"we\", \"seaweed\", 'adf', 'NA'),\n                     C = c(1, 0, NA, 9, NA), \n                     D = c(\"tangro\", \"ok\", 'NA', 'yes', 'NA'))\n\n\ndummy <- dummyVars(\" ~ .\", data=encode_df)\nnewdata <- data.frame(predict(dummy, newdata = encode_df)) \nnewdata##   A  B fantasyadf fantasyNA fantasysad fantasyseaweed fantasywe  C DNA Dok\n## 1 1  4          0         0          1              0         0  1   0   0\n## 2 7 10          0         0          0              0         1  0   0   1\n## 3 8 NA          0         0          0              1         0 NA   1   0\n## 4 5  7          1         0          0              0         0  9   0   0\n## 5 3  4          0         1          0              0         0 NA   1   0\n##   Dtangro Dyes\n## 1       1    0\n## 2       0    0\n## 3       0    0\n## 4       0    1\n## 5       0    0"},{"path":"data-cleaning-with-r.html","id":"reference-1","chapter":"23 Data cleaning with r","heading":"23.9 Reference","text":"[1] https://sparkbyexamples.com/r-programming/---left-join--r/#:~:text=%20to%20do%20left%20join%20on%20data%20frames%20in%20R,join%20data%20frames%20in%20R.[2]https://stackoverflow.com/questions/36377813/impute--frequent-categorical-value---columns--data-frame[3] https://datatricks.co.uk/one-hot-encoding--r-three-simple-methods","code":""},{"path":"introduction-to-shiny-web-apps.html","id":"introduction-to-shiny-web-apps","chapter":"24 Introduction to Shiny Web Apps","heading":"24 Introduction to Shiny Web Apps","text":"Wenxi Zhang (wz2615)","code":""},{"path":"introduction-to-shiny-web-apps.html","id":"motivation-1","chapter":"24 Introduction to Shiny Web Apps","heading":"24.1 Motivation","text":"Shiny R package R studio makes easy build interactive web apps straight R. Shiny allows create highly effective data reports visualization user can explore data set specifying data subset.think important address shiny tutorial following ways:Package analysis format users can easily ingest. data scientist, common present work progress within organization. Shiny makes easy deliver data-driven conclusions users don’t use R without technical background.Package analysis format users can easily ingest. data scientist, common present work progress within organization. Shiny makes easy deliver data-driven conclusions users don’t use R without technical background.official tutorial isn’t user friendly someone first learns shiny. provides comprehensive deep concepts Shiny take long finish whole lessons. Thus include heavily used functions concepts shiny give viewers general practical exposure using Shiny.official tutorial isn’t user friendly someone first learns shiny. provides comprehensive deep concepts Shiny take long finish whole lessons. Thus include heavily used functions concepts shiny give viewers general practical exposure using Shiny.Prototyping. Shiny provides rapid way prototyping shiny dashboards. user interface requirements often changes project progress, often easier determine requirements using prototype. Shiny, developers can quickly change edit Shiny dashboards, Shiny framework includes set pre-configured functions generate panels widgets. requirements settled, move Shiny app formal web development framework.Prototyping. Shiny provides rapid way prototyping shiny dashboards. user interface requirements often changes project progress, often easier determine requirements using prototype. Shiny, developers can quickly change edit Shiny dashboards, Shiny framework includes set pre-configured functions generate panels widgets. requirements settled, move Shiny app formal web development framework.Performing daily data analytics task. can save R code data analytics job Shiny. users background R, can also perform data manipulation visualization task. allows daily analytics task move away data scientists focus new challenges business decisions.Performing daily data analytics task. can save R code data analytics job Shiny. users background R, can also perform data manipulation visualization task. allows daily analytics task move away data scientists focus new challenges business decisions.","code":""},{"path":"introduction-to-shiny-web-apps.html","id":"how-to-host","chapter":"24 Introduction to Shiny Web Apps","heading":"24.2 How to host","text":"multiple way host shiny app.Install shiny server computerUpload app https://www.shinyapps.io/\nmonth 25 free active hours\nMaximum 5 apps free\nmonth 25 free active hoursMaximum 5 apps freeGoogle Cloud Run: https://code.markedmondson./shiny-cloudrun/","code":""},{"path":"introduction-to-shiny-web-apps.html","id":"create-your-first-shiny-app","chapter":"24 Introduction to Shiny Web Apps","heading":"24.3 Create your first Shiny app","text":"focus creating Shiny app Rstudio.\nseveral ways :File -> New File -> Shiny Web App\nSingle file ( tutorial uses)\nMultiple files\nSingle file ( tutorial uses)Multiple filesFile -> New File -> R Markdown -> ShinyFile -> New File -> R Markdown -> Template -> Flex Dashboard","code":""},{"path":"introduction-to-shiny-web-apps.html","id":"basic-struture","chapter":"24 Introduction to Shiny Web Apps","heading":"24.4 Basic Struture","text":"Shiny applications divided three parts:User Interface (UI)User Interface (UI)ServerServera call shinyApp functiona call shinyApp functionThe user interface (ui) responsible app presentation, server responsible app logic. Finally shinyApp() function creates Shiny app objects explicit UI/server pair.UI controls lay components displayed application page, like text, plots, widgets take user input.\nserver controls data displayed UI. ’s load wrangle data, transforming input UI outputs.simple example","code":"\nlibrary(shiny)\nui <- fluidPage(\n  # controls the layout and content of the application\n)\nserver <- function(input, output) {\n   # controls the interaction, modify output based on user input\n}\n#run app\nshinyApp(ui = ui, server = server)"},{"path":"introduction-to-shiny-web-apps.html","id":"the-ui","chapter":"24 Introduction to Shiny Web Apps","heading":"24.5 The UI","text":"may lay user interface app placing elements fluidPage function. Elements can seperated inputs outputs function.Next create visualization tool display k-means clustering Iris dataset. , need define UI panel return scatterplot.\nIris data set contains 3 classes 50 instances , class refers type iris plant. feature set contains “Sepal.Length”, “Sepal.Width”, “Petal.Length”, “Petal.Width”","code":""},{"path":"introduction-to-shiny-web-apps.html","id":"layout","chapter":"24 Introduction to Shiny Web Apps","heading":"24.5.1 layout","text":"First, let’s create UI panel left side. need define three panels: title, sidebar users’ input, main visualization work go.titlePanel sidebarLayout two popular elements create layout.titlePanel basically put App title .\nsidebarLayout two arguments: sidebarPanel mainPanel, place content sidebar main panels shown example1. can put values like text input/output functions Panels.","code":"\nui <- fluidPage(\n  titlePanel(\"title panel\"),\n\n  sidebarLayout(\n    sidebarPanel(\"sidebar panel\"),\n    mainPanel(\"main panel\")\n  )\n)"},{"path":"introduction-to-shiny-web-apps.html","id":"input-functions","chapter":"24 Introduction to Shiny Web Apps","heading":"24.5.2 Input functions","text":"can also add control widgets provide way users send messages Shiny app. Shiny widgets collect value user. user changes widget, value change well.many input functions create widgets. examples basic widgets.Next introduce common input function selectInput() shows box choices select .\nselectInput() 3 required arguments:\ninputId: input name access value (internal ues–> use input$inputId refer input value server)\nlabel: Label shown UI, NULL label.\nchoices: List values select .\ncan also use selected set initial selected value user specify.include select widget allows user select y variable output plot. also define variables choose attributes feature set, set default choice ‘Sepal.Width’. Similarly, can define selection box x variable numeric Input box user can define cluster numbers clustering example.","code":"\nselectInput(inputID='ycol', label='Y Variable', \n            choices=names(iris)[1:4],#\"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\" \n      selected = names(iris)[[2]])# set \"Petal.Length\" to be default"},{"path":"introduction-to-shiny-web-apps.html","id":"output-functions","chapter":"24 Introduction to Shiny Web Apps","heading":"24.5.3 Output functions","text":"display output, add output function fluidPage() *Output() function. list output functions turn R objects specific type outputJust like elements ui, can add output user interface inside sidebarPanel mainPanel.\nexample, place output mainPanel output scatter plot user defined x, y variables. may add plotOutput('plot') mainPanel create plot object, \"plot\" name given output plot. numericInput() used choosing cluster numbers, users allowed enter cluster numbers 1 9, default 3 clusters.complete ui portion Iris clustering problem.","code":"\nui <- fluidPage(\n  headerPanel('Iris k-means clustering'),\n  sidebarPanel(\n    selectInput('xcol', 'X Variable', names(iris)[1:4]),\n    selectInput('ycol', 'Y Variable', names(iris)[1:4],\n                selected = names(iris)[[2]]),\n    numericInput('clusters', 'Cluster count', value=3,#value: initial value\n                 min = 1, max = 9)#Minimum allowed value, Maximum allowed value\n  ),\n  mainPanel(\n    plotOutput('plot')\n  )\n)"},{"path":"introduction-to-shiny-web-apps.html","id":"the-server","chapter":"24 Introduction to Shiny Web Apps","heading":"24.6 The Server","text":"adding output object ui, need tell Shiny build object server function. , need toAccess input values input$ (example, input$xcol, input$ycol, input$clusters)Access input values input$ (example, input$xcol, input$ycol, input$clusters)builds list-like object named output contains code needed update R objects app. (Save objects display output$. case output output$plot)builds list-like object named output contains code needed update R objects app. (Save objects display output$. case output output$plot)Define text/charts/plots server function (Build objects display render*())Define text/charts/plots server function (Build objects display render*())entry output contain output one Shiny’s render* functions. functions capture R expression light pre-processing expression. render*() function creates type output wish make. Shiny re-run function every time needs update object.\ncommon render functions descriptions.following rules , reactivity automatically occurs whenever use input value build rendered Outputs.","code":""},{"path":"introduction-to-shiny-web-apps.html","id":"reactivity","chapter":"24 Introduction to Shiny Web Apps","heading":"24.6.1 Reactivity","text":"know input value changes whenever user changes input. input values notify render* functions R expression inside render*() input changes. notified input changes, object created render*() function return entire code block update .However, computational costy update R expression inside render*() whenever input updates. example, want use modified input several times, duplicate R expressions modifying input slows program. Thus better save reusable R expressions modifies input function. Luckily, reactive() function shiny job!reactive() builds reactive object respond every input value code. can save R expressions modifies input values reactive() call reactive expression like function.example, can save user specified two feature columns Iris dataset selectedData calculate k-mans model clusters base selectedData user specified cluster numbers.\nMoreover, selectedData won’t change input$xcol input$ycol aren’t changed. can avoid unnecessary computation putting expressions final render*() function, don’t need update iris[, c(input$xcol, input$ycol)] whenever input$clusters changes.","code":"\n  selectedData <- reactive({\n    iris[, c(input$xcol, input$ycol)]\n  })\n\n  clusters <- reactive({\n    kmeans(selectedData(), input$clusters)\n  })"},{"path":"introduction-to-shiny-web-apps.html","id":"display-output-with-render","chapter":"24 Introduction to Shiny Web Apps","heading":"24.6.2 Display output with render*()","text":"example, need define plot output. Thus use render function renderPlot() .discussed , renderPlot() respond every reactive value code, selectedData() clusters(). use two functions input data renderPlot(). two functions change, renderPlot() rerun r expressions inside.Now everything connected! overall server section looks like . Finally, need one statement app running –>shinyApp(ui = ui, server = server)","code":"\n  output$plot <- renderPlot({\n    par(mar = c(5.1, 4.1, 0, 1))\n    plot(selectedData(),\n         col = clusters()$cluster,\n         pch = 20, cex = 3)\n    points(clusters()$centers, pch = 4, cex = 4, lwd = 4)\n  })\nserver <- function(input, output) {\n\n  selectedData <- reactive({\n    iris[, c(input$xcol, input$ycol)]\n  })\n\n  clusters <- reactive({\n    kmeans(selectedData(), input$clusters)\n  })\n\n  output$plot <- renderPlot({\n    par(mar = c(5.1, 4.1, 0, 1))\n    plot(selectedData(),\n         col = clusters()$cluster,\n         pch = 20, cex = 3)\n    points(clusters()$centers, pch = 4, cex = 4, lwd = 4)\n  })\n\n\n}"},{"path":"introduction-to-shiny-web-apps.html","id":"conclusion-3","chapter":"24 Introduction to Shiny Web Apps","heading":"24.7 Conclusion","text":"general, Shiny great option presenting results combining visualization tool using R expressions. Though example went , covered basic structure Shiny, manipulate layout UI, connect logic behind input s outputs server function.Functions introduced used functions shiny. powerful interesting aspects shiny app like customizing appearance internal functions. highly recommend visit https://shiny.rstudio.com/ documentation gallery.","code":""},{"path":"introduction-to-shiny-web-apps.html","id":"evaluation","chapter":"24 Introduction to Shiny Web Apps","heading":"24.8 Evaluation","text":"experience dive deep learning shiny experimenting different data sets, find shiny powerful easy learn package generate intuitive interactive web apps, especially interactive plots. want report ongoing project, shiny great tool visualizing impact work. Just like example visualizing k-means clusters, gives users intuitive way understanding k-means clustering.also learnt Shiny unlimited possibilities improving efficiency work, creating interactive presentations project result solving daily analytic task. Moreover, Shiny enables users R background data analyzing work, manipulating widgets UI.Things might differently next time including complex useful internal functions server part, customizing appearance Shiny’s UI components using differnt themes.","code":""},{"path":"introduction-to-shiny-web-apps.html","id":"reference-2","chapter":"24 Introduction to Shiny Web Apps","heading":"24.9 Reference","text":"https://towardsdatascience.com/beginners-guide--creating--r-shiny-app-1664387d95b3https://shiny.rstudio.com/tutorial/","code":""},{"path":"python-missingno-library-tutorial.html","id":"python-missingno-library-tutorial","chapter":"25 Python missingno library tutorial","heading":"25 Python missingno library tutorial","text":"Yuyang Zhuo, Jiahao LaiCleaning data always take half time model-fitting project. deal missing data major problem cleaning data.\n, missingno Python library provides various ways visualize missing data help users dig reasons data complete.Please check github repo see tutorial:https://github.com/Jiahao-B-Lai/EDAV-CC-Group16/blob/main/5702_CC_Group16.ipynb","code":""},{"path":"data-explorer-tutorial-and-automation.html","id":"data-explorer-tutorial-and-automation","chapter":"26 Data Explorer Tutorial and Automation","heading":"26 Data Explorer Tutorial and Automation","text":"Akshay IyengarI created video tutorial shortcut known data explorer. package talked class, emphasized useful fast data exploration package. video teach classmates automation can useful useful time.especially one many packages help creating general visualizations data scientists.Youtube LinkSources:CRAN\nRpubs","code":""},{"path":"d-visualization-in-r.html","id":"d-visualization-in-r","chapter":"27 3D Visualization in R","heading":"27 3D Visualization in R","text":"Tianyu Han Shijia Huang","code":""},{"path":"d-visualization-in-r.html","id":"motivation-2","chapter":"27 3D Visualization in R","heading":"27.1 Motivation","text":"important part data visualization, 3D plotting makes data exploration part easier users allow visual display datasets.plotting data points three axes, 3D plots describe relationship three variables useful identify underlying patterns interactions shown 2D graphs.tutorial, introduce different packages 3D plots, including package Scatterplot3D, package plot3D, also plotly. end tutorial, one able choose suitable package /project.can learn project always different ways tackle problem using different R libraries packages. require extensive research trials order determine one works best given scenario.","code":""},{"path":"d-visualization-in-r.html","id":"scatterplot3d","chapter":"27 3D Visualization in R","heading":"27.2 Scatterplot3D","text":"Scatterplot3d R package displays multidimensional data 3D space.one function scatterplot3d() package.usage scatterplot3d() discussed examples .","code":""},{"path":"d-visualization-in-r.html","id":"load-data","chapter":"27 3D Visualization in R","heading":"27.2.1 Load Data","text":"use preloaded dataset USArrests example show information can draw 3D plot using scatterplot3d.","code":"\ndata(\"USArrests\")\nhead(USArrests)##            Murder Assault UrbanPop Rape\n## Alabama      13.2     236       58 21.2\n## Alaska       10.0     263       48 44.5\n## Arizona       8.1     294       80 31.0\n## Arkansas      8.8     190       50 19.5\n## California    9.0     276       91 40.6\n## Colorado      7.9     204       78 38.7"},{"path":"d-visualization-in-r.html","id":"create-matrix","chapter":"27 3D Visualization in R","heading":"27.2.2 Create matrix","text":"Scatterplot3d, dataframe provided must converted matrix. select Assault, Urban Population, Rape three axes.","code":"\nUSArrestsMatrix <- as.matrix(USArrests)\nx1 <- USArrestsMatrix[,2] ## Assault\ny1 <- USArrestsMatrix[,3] ## Urban Population\nz1 <- USArrestsMatrix[,4] ## Rape"},{"path":"d-visualization-in-r.html","id":"generate-3d-scatter-plot","chapter":"27 3D Visualization in R","heading":"27.2.3 Generate 3d scatter plot","text":"Creating graph using scatterplot3d. “highlight” gives color scale enables users understand relative position data point. “pch” specifies plotting shape, set pch = 16, small dot.can also remove box (grid) graph change color points. Note setting color, “highlight.3d” argument specified FALSEAdding labels graph, “cex” specifies font size.","code":"\nsp1 <- scatterplot3d(x1,y1,z1, highlight.3d = TRUE, pch = 16, angle = 45,\n                     xlab = \"Assault\",\n                     ylab = \"Urban Population\",\n                     zlab = \"Rape\")\nsp2 <- scatterplot3d(x1,y1,z1,  pch = 16, angle = 45,highlight.3d = FALSE,\n                     xlab = \"Assault\",\n                     ylab = \"Urban Population\",\n                     zlab = \"Rape\",\n                     grid = TRUE,\n                     box = FALSE,\n                     color = c(\"pink\")) \nsp3 <- scatterplot3d(x1,y1,z1, highlight.3d = TRUE, pch = 18, angle = 45,\n                     xlab = \"Assault\",\n                     ylab = \"Urban Population\",\n                     zlab = \"Rape\")\ntext(sp3$xyz.convert(USArrests[2:4]),labels = rownames(USArrests), cex = 0.5, color = 'pink')"},{"path":"d-visualization-in-r.html","id":"d-scatter-plot-with-x-y-plane-position","chapter":"27 3D Visualization in R","heading":"27.2.4 3D scatter plot with x-y plane position","text":"Use “Type = ‘h’” create vertical lines data point x-y plane.","code":"\nsp4 <- scatterplot3d(x1,y1,z1, highlight.3d = TRUE, pch = 18, angle = 45,\n                     xlab = \"Assault\",\n                     ylab = \"Urban Population\",\n                     zlab = \"Rape\",\n                     type = \"h\")"},{"path":"d-visualization-in-r.html","id":"d-plot-and-pca","chapter":"27 3D Visualization in R","heading":"27.3 3D plot and PCA","text":"data science, 3D plot can also used machine learning steps. example, plotting principal components 3D space, efficiently observe interaction important vectors input data.use preloaded data “Glass” perform principal component analysis 3D visualization components.","code":""},{"path":"d-visualization-in-r.html","id":"load-package-mlbench-and-use-the-glass-dataset.","chapter":"27 3D Visualization in R","heading":"27.3.1 Load Package “mlbench” and use the Glass dataset.","text":"","code":"\ndata(Glass)\nhead(Glass)##        RI    Na   Mg   Al    Si    K   Ca Ba   Fe Type\n## 1 1.52101 13.64 4.49 1.10 71.78 0.06 8.75  0 0.00    1\n## 2 1.51761 13.89 3.60 1.36 72.73 0.48 7.83  0 0.00    1\n## 3 1.51618 13.53 3.55 1.54 72.99 0.39 7.78  0 0.00    1\n## 4 1.51766 13.21 3.69 1.29 72.61 0.57 8.22  0 0.00    1\n## 5 1.51742 13.27 3.62 1.24 73.08 0.55 8.07  0 0.00    1\n## 6 1.51596 12.79 3.61 1.62 72.97 0.64 8.07  0 0.26    1"},{"path":"d-visualization-in-r.html","id":"data-cleaning","chapter":"27 3D Visualization in R","heading":"27.3.2 Data Cleaning","text":"Perform PCA dataset convert pca result dataframe. plot three components PCA results.Specify three colors .”shape” specifies three different shapes component.","code":"\nresults <- prcomp(Glass[,2:4], scale = TRUE)\nresults$rotation <- -1*results$rotation\nresults$rotation##           PC1        PC2       PC3\n## Na  0.4381565 -0.8763587 0.2000358\n## Mg -0.6582364 -0.1612544 0.7353380\n## Al  0.6121632  0.4538639 0.6475058\nresults$x <- -1*results$x\nhead(results$x)##          PC1         PC2       PC3\n## 1 -1.1222507 -0.76451910 0.5299814\n## 2 -0.2631728 -0.69696062 0.4746961\n## 3 -0.2128159 -0.14139775 0.5944634\n## 4 -0.7549327 -0.04089696 0.2632213\n## 5 -0.7521008 -0.14291459 0.1773877\n## 6 -0.5391614  0.71876865 0.5475329\npca.result <- results$x\npca.result <-data.frame(pca.result)\nhead(pca.result)##          PC1         PC2       PC3\n## 1 -1.1222507 -0.76451910 0.5299814\n## 2 -0.2631728 -0.69696062 0.4746961\n## 3 -0.2128159 -0.14139775 0.5944634\n## 4 -0.7549327 -0.04089696 0.2632213\n## 5 -0.7521008 -0.14291459 0.1773877\n## 6 -0.5391614  0.71876865 0.5475329\npca.result$Type <- (Glass$Type)"},{"path":"d-visualization-in-r.html","id":"define-color-and-shape-parameter.","chapter":"27 3D Visualization in R","heading":"27.3.3 Define color and shape parameter.","text":"","code":"\n## choose 6 colors for 6 glass types\ncolors <- c(\"#E69F00\", \"#56B4E9\",\"#B2182B\",\"#D1E5F0\",\"#92C5DE\",\"#2166AC\")\ncolors <- colors[as.numeric(pca.result$Type)]\n## choose 6 shapes for 6 glass types\nshape<-10:15\nshape<-shape[as.numeric(pca.result$Type)]"},{"path":"d-visualization-in-r.html","id":"generate-graph","chapter":"27 3D Visualization in R","heading":"27.3.4 Generate graph","text":"Plot result PCA analysis following step previous part. Adjust angle best visualization. example 3D plot can help us see contribution component classifying types glass.","code":"\nPCA3D <- scatterplot3d(pca.result[,1:3],\n                     color=colors,\n                     pch = shape, \n                     cex.symbols = 3,\n                     angle = 100)\nlegend(\"top\", legend = levels(pca.result$Type),\n       col =   c(\"#E69F00\", \"#56B4E9\",\"#B2182B\",\"#D1E5F0\",\"#92C5DE\",\"#2166AC\"),\n       pch = c(10,11,12,13,14), \n       inset = -0.1, xpd = TRUE, horiz = TRUE)"},{"path":"d-visualization-in-r.html","id":"other-usage-of-the-scatterplot3d-function","chapter":"27 3D Visualization in R","heading":"27.4 Other usage of the scatterplot3d function","text":"Sometimes hard imagine relationship two functions graph, plotting 3D space, visualize interaction dynamic environment.simple example graph interaction cos sin function.","code":"\nz <- seq(-15, 15, 0.05)\nx <- cos(z)\ny <- sin(z)\nscatterplot3d(x, y, z, highlight.3d=TRUE, col.axis =\"blue\",col.grid =\"lightblue\", main=\"an example of cosine and sine interaction\", pch=20)"},{"path":"d-visualization-in-r.html","id":"d-histogram","chapter":"27 3D Visualization in R","heading":"27.5 3D Histogram","text":"generate histogram 3d, can use plot 3D package. first initiate x-axis y-axis. , need create z matrix dimension |x| * |y|. can use hist3D function package help us generate 3D histogram need.","code":"\nx = c(1, 2)\ny = c(1, 2)\nz = c(1, 2, 2, 3)\nmat1 <- matrix(z,nrow=2,ncol=2,byrow=TRUE)\nhist3D(z=mat1, x = x, y= y)"},{"path":"d-visualization-in-r.html","id":"d-scatter-plot-using-plotly","chapter":"27 3D Visualization in R","heading":"27.6 3D scatter plot using plotly","text":"","code":""},{"path":"d-visualization-in-r.html","id":"demo-data","chapter":"27 3D Visualization in R","heading":"27.6.1 Demo Data","text":"order better demonstrate different features plotly 3D Scatterplot, selected sample data includes 40 observations household expenditure single men women. 5 variables observation:Housing: money(usd) spent housingFood: money(usd) spent foodGoods: money(usd) spent goodsService: money(usd) spent serviceGender: female male","code":"\nhousehold##    housing food goods service gender\n## 1      820  114   183     154 female\n## 2      184   74     6      20 female\n## 3      921   66  1686     455 female\n## 4      488   80   103     115 female\n## 5      721   83   176     104 female\n## 6      614   55   441     193 female\n## 7      801   56   357     214 female\n## 8      396   59    61      80 female\n## 9      864   65  1618     352 female\n## 10     845   64  1935     414 female\n## 11     404   97    33      47 female\n## 12     781   47  1906     452 female\n## 13     457  103   136     108 female\n## 14    1029   71   244     189 female\n## 15    1047   90   653     298 female\n## 16     552   91   185     158 female\n## 17     718  104   583     304 female\n## 18     495  114    65      74 female\n## 19     382   77   230     147 female\n## 20    1090   59   313     177 female\n## 21     497  591   153     291   male\n## 22     839  942   302     365   male\n## 23     798 1308   668     584   male\n## 24     892  842   287     395   male\n## 25    1585  781  2476    1740   male\n## 26     755  764   428     438   male\n## 27     388  655   153     233   male\n## 28     617  879   757     719   male\n## 29     248  438    22      65   male\n## 30    1641  440  6471    2063   male\n## 31    1180 1243   768     813   male\n## 32     619  684    99     204   male\n## 33     253  422    15      48   male\n## 34     661  739    71     188   male\n## 35    1981  869  1489    1032   male\n## 36    1746  746  2662    1594   male\n## 37    1865  915  5184    1767   male\n## 38     238  522    29      75   male\n## 39    1199 1095   261     344   male\n## 40    1524  964  1739    1410   male"},{"path":"d-visualization-in-r.html","id":"the-classic-3d-scatterplot","chapter":"27 3D Visualization in R","heading":"27.6.2 The classic 3D Scatterplot","text":"","code":"\nfig <- plot_ly(household, x = ~housing, y = ~food, z = ~goods + service)\nfig <- fig %>% layout(scene = list(xaxis = list(title = 'housing'),\n                                   yaxis = list(title = 'food'),\n                                   zaxis = list(title = 'goods and services')))\nfig"},{"path":"d-visualization-in-r.html","id":"adding-colors-to-3d-scatterplot","chapter":"27 3D Visualization in R","heading":"27.6.3 Adding colors to 3D Scatterplot","text":"order differentiate observations opposite genders, need add colors 3D scatter plot. done followed:","code":"\nfig <- plot_ly(household, x = ~housing, y = ~food, z = ~goods + service,\n               color = ~gender, colors = c('#17becf', '#d62728'))\nfig <- fig %>% add_markers()\nfig <- fig %>% layout(scene = list(xaxis = list(title = 'housing'),\n                                   yaxis = list(title = 'food'),\n                                   zaxis = list(title = 'goods and services')))\nfig"},{"path":"d-visualization-in-r.html","id":"adding-sizes-to-3d-scatterplot","chapter":"27 3D Visualization in R","heading":"27.6.4 Adding sizes to 3D Scatterplot","text":"interesting note size available fifth parameter helps us plot findings. example, used size plot overall expenditure household. help us visualize overall trend better.","code":"\nfig <- plot_ly(household, x = ~housing, y = ~food, z = ~goods + service,\n               color = ~gender, colors = c('#2ca02c', '#8c564b'), size = ~ housing + food + goods + service, sizes = c(500, 5000))\nfig <- fig %>% add_markers()\nfig <- fig %>% layout(scene = list(xaxis = list(title = 'housing'),\n                                   yaxis = list(title = 'food'),\n                                   zaxis = list(title = 'goods and services')))\nfig"},{"path":"d-visualization-in-r.html","id":"conclusion-4","chapter":"27 3D Visualization in R","heading":"27.7 Conclusion","text":"tutorial, introduced scatterplot3d, plot3d, also plotly. respective advantages. need interactive graph allows zooming rotating, plotly better choice. However, perform principal component analysis better visualize results, easier use scatterplot3d.","code":""},{"path":"d-visualization-in-r.html","id":"works-cited","chapter":"27 3D Visualization in R","heading":"27.8 Works Cited","text":"Ligges, Uwe, Martin Mächler. “Scatterplot3d - R Package Visualizing Multivariate Data.” Journal Statistical Software, vol. 8, . 11, Foundation Open Access Statistic, 2003, https://doi.org/10.18637/jss.v008.i11.http://www.sthda.com/english/wiki/scatterplot3d-3d-graphics-r-software--data-visualizationhttp://www.sthda.com/english/wiki/colors--r#:~:text=%20R%2C%20colors%20can%20be,taken%20from%20the%20RColorBrewer%20package.https://www.statology.org/principal-components-analysis--r/https://plotly.com/r/3d-scatter-plots/http://www.countbio.com/web_pages/left_object/R_for_biology/R_fundamentals/3D_histograms_R.html","code":""},{"path":"visualizing-time-series-data.html","id":"visualizing-time-series-data","chapter":"28 Visualizing Time Series Data","heading":"28 Visualizing Time Series Data","text":"Kate Lassiter","code":""},{"path":"visualizing-time-series-data.html","id":"starting-point","chapter":"28 Visualizing Time Series Data","heading":"28.0.0.1 Starting Point","text":"exploratory questions new data set:Strongly correlated columnsVariable meansSample variance, etc.Use familiar techniques:Summary statisticsHistogramsScatter plots, etc.careful lookahead!Incorporating information future past smoothing, prediction, etc. shouldn’t know yetCan happen time-shifting, smoothing, imputing dataCan bias model make predictions worthless","code":""},{"path":"visualizing-time-series-data.html","id":"working-with-time-series-ts-objects","chapter":"28 Visualizing Time Series Data","heading":"28.0.0.2 Working with time series (ts) objects","text":"Integration ts() objects ggplot2:ggfortify package\nautoplot()\ncustomizations ggplot2\nDon’t convert ts dataframe format\nautoplot()customizations ggplot2Don’t convert ts dataframe formatgridExtra package\nArrange 4 ggplot plots 4-panel grid\nArrange 4 ggplot plots 4-panel gridgrid package\nAdd title grid arrangement\nAdd title grid arrangement","code":"\ndax=autoplot(EuStockMarkets[,\"DAX\"])+\n  ylab(\"Price\")+\n  xlab(\"Time\")\n\ncac=autoplot(EuStockMarkets[,\"CAC\"])+\n  ylab(\"Price\")+\n  xlab(\"Time\")\n\nsmi=autoplot(EuStockMarkets[,\"SMI\"])+\n  ylab(\"Price\")+\n  xlab(\"Time\")\n\nftse=autoplot(EuStockMarkets[,\"FTSE\"])+\n  ylab(\"Price\")+\n  xlab(\"Time\")\ngrid.arrange(dax,cac,smi,ftse,top=textGrob(\"Stock market prices\"))"},{"path":"visualizing-time-series-data.html","id":"time-series-relevant-plotting","chapter":"28 Visualizing Time Series Data","heading":"28.0.0.3 Time series relevant plotting:","text":"Working data:Directly transforming ts() objects use ggplot2:complete.cases() easily remove NA rows - prevent ggplot warning\navoid irritations working ts objects\ncomplete.cases() easily remove NA rows - prevent ggplot warningavoid irritations working ts objectsLooking changes time:Plot differenced values\nHistogram/scatter plot lagged data\nShows change values, values change together\nTrend can hide true relationship, make two series appear highly predictive one another move together\nUse base package diff(), calculates difference point time t t+1\nHistogram/scatter plot lagged dataShows change values, values change togetherTrend can hide true relationship, make two series appear highly predictive one another move togetherUse base package diff(), calculates difference point time t t+1Exploring Time Lags:Lagged differences:Time series analysis: focused predicting future values past\nConcerned whether change one variable time t predicts change another variable time t+1\nlag() shift forward one\nShowing density using alpha\nTime series analysis: focused predicting future values pastConcerned whether change one variable time t predicts change another variable time t+1lag() shift forward oneShowing density using alphaNow apparent relationship: positive change SMI today won’t predict positive change DAX tomorrow. positive trend long term, little predict short termObservations:Careful time series data: use techniques, reshape dataChange values one time another vital concept","code":"\nnew=as.data.frame(EuStockMarkets)\nnew$SMI_diff=c(NA,diff(new$SMI))\nnew$DAX_diff=c(NA,diff(new$DAX))\n\np1 <- ggplot(new, aes(SMI,DAX))+\n  geom_point(shape = 21, colour = \"black\", fill = \"white\")\np2 <- ggplot(new[complete.cases(new),], aes(SMI_diff,DAX_diff))+\n  geom_point(shape = 21, colour = \"black\", fill = \"white\")\n\ngrid.arrange(p1,p2,top=textGrob(\"SMI vs DAX\"))\nnew$SMI_lag_diff=c(NA,lag(diff(new$SMI),1))\nggplot(new[complete.cases(new),], aes(SMI_lag_diff,DAX_diff))+\n  geom_point(shape = 21, colour = \"black\", fill = \"white\",alpha=0.4,size=2)"},{"path":"visualizing-time-series-data.html","id":"dynamics-of-time-series-data","chapter":"28 Visualizing Time Series Data","heading":"28.0.1 Dynamics of Time Series Data","text":"","code":""},{"path":"visualizing-time-series-data.html","id":"seasonality-cycle-trend","chapter":"28 Visualizing Time Series Data","heading":"28.0.1.1 Seasonality, Cycle, Trend","text":"Three aspects time series data:Seasonal:\nRecurs fixed period\nRecurs fixed periodCycle:\nRecurrent behaviors, variable time period\nRecurrent behaviors, variable time periodTrend:\nOverall drift higher/lower values long time period\nOverall drift higher/lower values long time periodThese can gathered visual inspection:Line plot\nClear trend\nConsider log transform differencing\n\nIncreasing variance\nMultiplicative seasonality\nSeasonal swings grow along overall values\n\nClear trend\nConsider log transform differencing\nConsider log transform differencingIncreasing varianceMultiplicative seasonality\nSeasonal swings grow along overall values\nSeasonal swings grow along overall valuesTime series decomposition:\nBreak data seasonal, trend, remainder components\nSeasonal component:\nLOESS smoothing January values, February values, etc.\nMoving window estimate smoothed value based point’s neighbors\n\nstats package\nstl()\n\nBreak data seasonal, trend, remainder componentsSeasonal component:\nLOESS smoothing January values, February values, etc.\nMoving window estimate smoothed value based point’s neighbors\nLOESS smoothing January values, February values, etc.Moving window estimate smoothed value based point’s neighborsstats package\nstl()\nstl()Observations\nClear rising trend\nObvious seasonality\nDifference two methods:\nparticular decomposition shows additive, multiplicative seasonality\nstart end time series highest residuals\nSettled average seasonal variance\n\n\nreveal information patterns need identified potentially dealt forecasting can occur\nClear rising trendObvious seasonalityDifference two methods:\nparticular decomposition shows additive, multiplicative seasonality\nstart end time series highest residuals\nSettled average seasonal variance\n\nparticular decomposition shows additive, multiplicative seasonality\nstart end time series highest residuals\nSettled average seasonal variance\nstart end time series highest residualsSettled average seasonal varianceBoth reveal information patterns need identified potentially dealt forecasting can occur","code":"\nautoplot(AirPassengers)+\n  xlab(\"Year\")+\n  ylab(\"Passengers\")\nautoplot(stl(AirPassengers, s.window = 'periodic'), ts.colour = 'red')+\n  xlab(\"Year\")"},{"path":"visualizing-time-series-data.html","id":"plotting-exploiting-the-temporal-axis","chapter":"28 Visualizing Time Series Data","heading":"28.0.1.2 Plotting: exploiting the temporal axis","text":"","code":""},{"path":"visualizing-time-series-data.html","id":"gannt-charts","chapter":"28 Visualizing Time Series Data","heading":"28.0.1.2.1 Gannt charts","text":"Shows overlapping time periods, duration event relative otherstimevis package","code":"\ndates=sample(seq(as.Date('1998-01-01'), as.Date('2000-01-01'), by=\"day\"), 16)\ndates=dates[order(dates)]\nprojects = paste0(\"Project \",seq(1,8)) \n\ndata <- data.frame(content = projects, \n                    start = dates[1:8],\n                    end = dates[9:16])\ntimevis(data)"},{"path":"visualizing-time-series-data.html","id":"using-month-and-year-creatively-in-line-plots","chapter":"28 Visualizing Time Series Data","heading":"28.0.1.2.2 Using month and year creatively in line plots","text":"forecast package\nggseasonplot()\nggmonthplot()\nsuppressMessages() prevents printing information outputted loading package\nggseasonplot()ggmonthplot()suppressMessages() prevents printing information outputted loading packageObservations\nmonths increased time others\nPassenger numbers peak July August\nLocal peak March years\nOverall increase across months years\nGrowth trend increasing (rate increase increasing)\nmonths increased time othersPassenger numbers peak July AugustLocal peak March yearsOverall increase across months yearsGrowth trend increasing (rate increase increasing)","code":"\nggseasonplot(AirPassengers)\nggmonthplot(AirPassengers)"},{"path":"visualizing-time-series-data.html","id":"d-visualizations-plotly-package","chapter":"28 Visualizing Time Series Data","heading":"28.0.1.2.3 3-D Visualizations: plotly package","text":"Convert format plotly understand\nAvoid using ts() object\nDataframe datetime, numeric columns\nlubridate package date manipulation\nAvoid using ts() objectDataframe datetime, numeric columnslubridate package date manipulationAllows better view relationships month year","code":"\nnew = data.frame(AirPassengers)\nnew$year=year(seq(as.Date(\"1949-01-01\"),as.Date(\"1960-12-01\"),by=\"month\"))\nnew$month=lubridate::month(seq(as.Date(\"1949-01-01\"),as.Date(\"1960-12-01\"),by=\"month\"),label=TRUE)\nplot_ly(new, x = ~month, y = ~year, z = ~AirPassengers, \n             color = ~as.factor(month)) %>%\n    add_markers() %>%\n    layout(scene = list(xaxis = list(title = 'Month'),\n                        yaxis = list(title = 'Year'),\n                        zaxis = list(title = 'Passenger Count')))"},{"path":"visualizing-time-series-data.html","id":"data-smoothing","chapter":"28 Visualizing Time Series Data","heading":"28.0.1.3 Data Smoothing","text":"Usually need smooth data starting analysis visualization\nAllows better storytelling\nIrrelevant spikes dominate narrative\nAllows better storytellingIrrelevant spikes dominate narrativeMethods:/\nMoving average/median\nGood noisy data\nRolling mean reduces variance\nKeep mind: affects accuracy, R² statistics, etc.\nZoo package rollmean()\nPrevent lookahead, use past values window (align=“right”)\nk = 6 6 month rolling window\ngsub() substitute series names clearer legend\ntidyr package gather()\nConvert wide long, use color/group ggplot\n\n\n\nMoving average/median\nGood noisy data\nRolling mean reduces variance\nKeep mind: affects accuracy, R² statistics, etc.\nZoo package rollmean()\nPrevent lookahead, use past values window (align=“right”)\nk = 6 6 month rolling window\ngsub() substitute series names clearer legend\ntidyr package gather()\nConvert wide long, use color/group ggplot\n\n\nGood noisy dataRolling mean reduces variance\nKeep mind: affects accuracy, R² statistics, etc.\nZoo package rollmean()\nPrevent lookahead, use past values window (align=“right”)\nk = 6 6 month rolling window\ngsub() substitute series names clearer legend\ntidyr package gather()\nConvert wide long, use color/group ggplot\n\nKeep mind: affects accuracy, R² statistics, etc.Zoo package rollmean()Prevent lookahead, use past values window (align=“right”)k = 6 6 month rolling windowgsub() substitute series names clearer legendtidyr package gather()\nConvert wide long, use color/group ggplot\nConvert wide long, use color/group ggplot","code":"\nnew = data.frame(AirPassengers)\nnew$AirPassengers=as.numeric(new$AirPassengers)\nnew$year=seq(as.Date(\"1949-01-01\"),as.Date(\"1960-12-01\"),by=\"month\")\n\nnew = new %>%\n  mutate(roll_mean = rollmean(new$AirPassengers,k=6,align=\"right\",fill = NA))\n\ndf <- gather(new, key = year, value = Rate, \n                            c(\"roll_mean\", \"AirPassengers\"))\ndf$year=gsub(\"AirPassengers\",\"series\",df$year)\ndf$year=gsub(\"roll_mean\",\"rolling mean\",df$year)\ndf$date = rep(new$year,2)\n\nggplot(df, aes(x=date, y = Rate, group = year, colour = year)) + \n  geom_line()"},{"path":"igraph-in-r.html","id":"igraph-in-r","chapter":"29 igraph in r","heading":"29 igraph in r","text":"Mouwei Lin (lm3756) Linhao Yu (ly2590)brief introduction R package called igraph, complex network analysis tool R. contain commonly used important functions. believe look cheat sheet, can handle network visualization problems.","code":""},{"path":"igraph-in-r.html","id":"igraph-basics","chapter":"29 igraph in r","heading":"29.1 1. igraph Basics","text":"First download install package.use open source data set named phone.call navdata.","code":"\n#install.packages('igraph')\nlibrary(igraph) #this is a must install package\n\n#install.packages('devtools')\n#library(devtools) #this is a must install package\n#install_github('kassambara/navdata') \nlibrary(navdata) #this is a must install package\n\n#install.packages('tidyverse')\nlibrary(tidyverse) #this is a must install package\n\n#install.packages('igraphdata')\nlibrary(igraphdata) #this is a must install package\ndata(\"phone.call\")\nhead(phone.call)## # A tibble: 6 × 3\n##   source  destination n.call\n##   <chr>   <chr>        <dbl>\n## 1 France  Germany          9\n## 2 Belgium France           4\n## 3 France  Spain            3\n## 4 France  Italy            4\n## 5 France  Netherlands      2\n## 6 France  UK               3"},{"path":"igraph-in-r.html","id":"create-igraph-network-object","chapter":"29 igraph in r","heading":"29.1.1 1.1 Create igraph Network Object","text":"igraph specific network (graph) object called ‘igraph’. simplest way create ‘igraph’ object use graph.formula() specify every node edge individually.real world, usually lot data. Therefore, good idea define every node edge manually. generally, use two methods:","code":"\ngraph.formula(A-B-C-D,A-E-F,Z-D-X)## IGRAPH 4392828 UN-- 8 7 -- \n## + attr: name (v/c)\n## + edges from 4392828 (vertex names):\n## [1] A--B A--E B--C C--D D--Z D--X E--F\n# prepare the data\nname<-data.frame(c(phone.call$source,phone.call$destination))\nnodes<-name%>%\n    distinct()%>%\nmutate(location=c(\"western\",\"western\",\"central\",\"nordic\",\"southeastern\",\n     \"southeastern\",\"southeastern\",\"southern\",\"sourthern\",\n     \"western\",\"western\",\"central\",\"central\",\"central\",\"central\",\"central\"))\ncolnames(nodes)<-c(\"label\",\"location\")\n\nedges<-phone.call%>%\n    rename(from=source,to=destination,weight=n.call)"},{"path":"igraph-in-r.html","id":"build-igraph-object-from-dataframe","chapter":"29 igraph in r","heading":"29.1.1.1 (1) build igraph object from dataframe:","text":"graph_from_data_frame()\nuse method, need two dataframes, one edge frame, vertices frame.can see graph created. can use V() E() visit vertices edges, as_edgelist(net, names=T), as_adjacency_matrix(net, attr=“weight”) catch edges list adjacent matrix:","code":"\nnet_pc<-graph_from_data_frame(\n   d=edges,vertices=nodes,\n   directed=TRUE)\nV(net_pc)## + 16/16 vertices, named, from 6e93970:\n##  [1] France         Belgium        Germany        Danemark       Croatia       \n##  [6] Slovenia       Hungary        Spain          Italy          Netherlands   \n## [11] UK             Austria        Poland         Switzerland    Czech republic\n## [16] Slovania\nV(net_pc)$location##  [1] \"western\"      \"western\"      \"central\"      \"nordic\"       \"southeastern\"\n##  [6] \"southeastern\" \"southeastern\" \"southern\"     \"sourthern\"    \"western\"     \n## [11] \"western\"      \"central\"      \"central\"      \"central\"      \"central\"     \n## [16] \"central\"\nE(net_pc)## + 18/18 edges from 6e93970 (vertex names):\n##  [1] France  ->Germany        Belgium ->France         France  ->Spain         \n##  [4] France  ->Italy          France  ->Netherlands    France  ->UK            \n##  [7] Germany ->Austria        Germany ->Poland         Belgium ->Germany       \n## [10] Germany ->Switzerland    Germany ->Czech republic Germany ->Netherlands   \n## [13] Danemark->Germany        Croatia ->Germany        Croatia ->Slovania      \n## [16] Croatia ->Hungary        Slovenia->Germany        Hungary ->Slovania\nas_edgelist(net_pc, names=T) ##       [,1]       [,2]            \n##  [1,] \"France\"   \"Germany\"       \n##  [2,] \"Belgium\"  \"France\"        \n##  [3,] \"France\"   \"Spain\"         \n##  [4,] \"France\"   \"Italy\"         \n##  [5,] \"France\"   \"Netherlands\"   \n##  [6,] \"France\"   \"UK\"            \n##  [7,] \"Germany\"  \"Austria\"       \n##  [8,] \"Germany\"  \"Poland\"        \n##  [9,] \"Belgium\"  \"Germany\"       \n## [10,] \"Germany\"  \"Switzerland\"   \n## [11,] \"Germany\"  \"Czech republic\"\n## [12,] \"Germany\"  \"Netherlands\"   \n## [13,] \"Danemark\" \"Germany\"       \n## [14,] \"Croatia\"  \"Germany\"       \n## [15,] \"Croatia\"  \"Slovania\"      \n## [16,] \"Croatia\"  \"Hungary\"       \n## [17,] \"Slovenia\" \"Germany\"       \n## [18,] \"Hungary\"  \"Slovania\"\nas_adjacency_matrix(net_pc, attr=\"weight\")## 16 x 16 sparse Matrix of class \"dgCMatrix\"\n##                                                 \n## France         . . 9 . . . . 3 4 2 3 . . . . .  \n## Belgium        4 . 3 . . . . . . . . . . . . .  \n## Germany        . . . . . . . . . 2 . 2 2 2 2 .  \n## Danemark       . . 2 . . . . . . . . . . . . .  \n## Croatia        . . 2 . . . 2 . . . . . . . . 2.0\n## Slovenia       . . 2 . . . . . . . . . . . . .  \n## Hungary        . . . . . . . . . . . . . . . 2.5\n## Spain          . . . . . . . . . . . . . . . .  \n## Italy          . . . . . . . . . . . . . . . .  \n## Netherlands    . . . . . . . . . . . . . . . .  \n## UK             . . . . . . . . . . . . . . . .  \n## Austria        . . . . . . . . . . . . . . . .  \n## Poland         . . . . . . . . . . . . . . . .  \n## Switzerland    . . . . . . . . . . . . . . . .  \n## Czech republic . . . . . . . . . . . . . . . .  \n## Slovania       . . . . . . . . . . . . . . . ."},{"path":"igraph-in-r.html","id":"build-igraph-object-from-adjacent-matrix","chapter":"29 igraph in r","heading":"29.1.1.2 (2) build igraph object from adjacent matrix:","text":"graph_from_adjacency_matrix()\ngraph usingnow simply plot take look","code":"\nadjacent_matrix<-as_adjacency_matrix(net_pc, attr=\"weight\")\nnet_am<-graph_from_adjacency_matrix(adjacent_matrix)\nplot(net_pc)\nplot(net_am)"},{"path":"igraph-in-r.html","id":"basic-igraph-visualization-instructions","chapter":"29 igraph in r","heading":"29.1.2 1.2 Basic igraph Visualization Instructions","text":"plot function igraph strong lot parameters make network beautiful clear. section give general introduction important visualization methods, detailed introduction next section.large number parameters used display various properties nodes, edges graphs. parameters related nodes start vertex.XXX, parameters related edges start edge.XXXIn addition specifying parameters nodes edges plot(), can also use previously mentioned V() E() add corresponding properties directly igraph object. difference two methods parameters specified plot() change properties plot. example, first specify color node according position, width edge according weight (two attributes saved net_pc object), specify size node parameters plot() (proportional degree node, node degree number edges connected node), size position node marker, color edge, size arrow, degree curvature edge.","code":"\n# Calculate node's degree\ndeg<-degree(net_pc,mode=\"all\")\n# Set up the color\nvcolor<-c(\"orange\",\"red\",\"lightblue\",\"tomato\",\"yellow\")\n# Set specific node's Color\nV(net_pc)$color<-vcolor[factor(V(net_pc)$location)]\n# Set specific edge's weight\nE(net_pc)$width<-E(net_pc)$weight/2\n\n# Set up vertex.size, vertex.label.cex & dist, edge color & arrow size & curve in graph\nplot(net_pc,vertex.size=3*deg,\n     vertex.label.cex=.7,vertex.label.dist=1,\n     edge.color=\"gray50\",edge.arrow.size=.4, edge.curved=.1)\n# Add legend\nlegend(x=-1.5,y=1.5,levels(factor(V(net_pc)$location)),pch=21,col=\"#777777\",pt.bg=vcolor)"},{"path":"igraph-in-r.html","id":"network-layout","chapter":"29 igraph in r","heading":"29.1.3 1.3 Network Layout","text":"Network layout refers method determining coordinates node network.variety layout algorithms provided igraph. Among , Force-directed layout algorithms useful. Force-directed layouts try get aesthetically pleasing graph edges similar length cross little possible. model graphics physical system. Nodes “charged particles” repel get close. edges act springs, attracting connected nodes together. result, nodes evenly distributed illustrated area, layout intuitive nodes share connections closer . disadvantage algorithms slow therefore less frequently used graphs larger 1000 vertices.using force-directed layout, can use niter parameter control number iterations perform. default setting 500 iterations. large graphs, can lower number get results faster check reasonable.Fruchterman-Reingold widely used Force-directed layout method:Fruchterman Reingold layout random different every run result slightly different layout configurations. Saving layout object l allows us obtain exact result multiple times (also possible specify random state setting seed seed())18 methods layout igraph, won’t go detail layout method widely used except Fruchterman Reingold layout. However, give example show layouts look like:","code":"\n# Fruchterman-Reingold layout method\nl <- layout_with_fr(net_pc) \nplot(net_pc, layout=l)\n# All the layout methods in igraph\nlayouts <- grep(\"^layout_\", ls(\"package:igraph\"), value=TRUE)[-1]\nlayouts##  [1] \"layout_as_bipartite\"  \"layout_as_star\"       \"layout_as_tree\"      \n##  [4] \"layout_components\"    \"layout_in_circle\"     \"layout_nicely\"       \n##  [7] \"layout_on_grid\"       \"layout_on_sphere\"     \"layout_randomly\"     \n## [10] \"layout_with_dh\"       \"layout_with_drl\"      \"layout_with_fr\"      \n## [13] \"layout_with_gem\"      \"layout_with_graphopt\" \"layout_with_kk\"      \n## [16] \"layout_with_lgl\"      \"layout_with_mds\"      \"layout_with_sugiyama\"\nlayouts <- grep(\"^layout_\", ls(\"package:igraph\"), value=TRUE)[-1]\n# Remove layouts that do not apply to our graph.\nlayouts <- layouts[!grepl(\"bipartite|merge|norm|sugiyama|tree\", layouts)]\npar(mfrow=c(5,3), mar=c(1,1,1,1)) \nfor (layout in layouts) {\n  print(layout)\n  l <- do.call(layout, list(net_pc))\n  plot(net_pc, vertex.label=\"\",edge.arrow.mode=0,\n       layout=l,main=layout) }## [1] \"layout_as_star\"## [1] \"layout_components\"## [1] \"layout_in_circle\"## [1] \"layout_nicely\"## [1] \"layout_on_grid\"## [1] \"layout_on_sphere\"## [1] \"layout_randomly\"## [1] \"layout_with_dh\"## [1] \"layout_with_drl\"## [1] \"layout_with_fr\"## [1] \"layout_with_gem\"## [1] \"layout_with_graphopt\"## [1] \"layout_with_kk\"## [1] \"layout_with_lgl\"## [1] \"layout_with_mds\""},{"path":"igraph-in-r.html","id":"decorating-igraph-visualizations","chapter":"29 igraph in r","heading":"29.1.4 2. Decorating igraph visualizations","text":"","code":""},{"path":"igraph-in-r.html","id":"details-for-decorating-igraph-visualization","chapter":"29 igraph in r","heading":"29.1.5 2.1 Details for decorating igraph visualization","text":"section, still use example , customize several parameters see influence visualization.First, customize nodes parameters.","code":"\nset.seed(111)\nl <- layout_with_fr(net_pc) \nplot(net_pc, \n     vertex.color = 'pink',                   # The color of nodes\n     vertex.frame.color = 'lightblue',        # The color of node frames\n     vertex.shape = c('circle','rectangle'),  # The color of node shapes\n     vertex.size = 25,                        # The size of a node\n     vertex.size2 = 15,                       # For rectangle, we need two parameters to specify its shape\n     \n     layout = l)\nplot(net_pc, \n     vertex.color = 'pink',                    \n     vertex.frame.color = 'lightblue',        \n     vertex.shape = c('circle','rectangle'),  \n     vertex.size = 25,                        \n     vertex.size2 = 15,                       \n     \n     vertex.label = 1:length(V(net_pc)), # Change labels to numbers\n     vertex.label.family = 'Helvetica',  # Change the font family of labels\n     vertex.label.font = 3,              # Change the font to italic\n     vertex.label.cex = 0.8,             # Change the size of labels\n     vertex.label.dist = 0.5,            # Change the distance between labels and node frames\n  \n     layout = l)\nplot(net_pc, \n     vertex.color = 'pink',                    \n     vertex.frame.color = 'lightblue',        \n     vertex.shape = c('circle','rectangle'),  \n     vertex.size = 25,                        \n     vertex.size2 = 15,                       \n     \n     vertex.label = 1:length(V(net_pc)), \n     vertex.label.family = 'Helvetica',  \n     vertex.label.font = 3,              \n     vertex.label.cex = 0.8,             \n     vertex.label.dist = 0.5,   \n     \n     edge.color = 'lightblue',  # Color of edges\n     edge.width = 3,            # Width of edges\n     edge.arrow.size = 0.8,     # Size of arrows\n     edge.arrow.width = 0.8,    # Width of arrows\n     edge.lty = 4,              # Line types of edges (4: dot dash)\n     edge.curved = 0.5,         # Curvature of edges\n     \n     layout = l)"},{"path":"igraph-in-r.html","id":"example-for-advanced-igraph-visualization","chapter":"29 igraph in r","heading":"29.1.6 2.2 Example for advanced igraph visualization","text":"section, introduce advanced network visualization using Zachary’s karate club network dataset. widely-used dataset network analysis. dataset, every node represents member karate club edges represent members’ social connection. Zachary’s study, administrator “John. .” coach “Mr. Hi” conflict led split club. Now, want use igraph visualize social network karate club.First, plot network without decoration.Next, highlight two leaders (“John. .” “Mr. Hi”) network using rectangles.\nFinally, divide edges three categories use different colors . Remember karate club split two factions, edges can divided : edges inside faction 1, edges inside faction 2, edges connecting factions.\nBesides, also set edge width according weight.","code":"\nset.seed(111)\ndata(karate)\nl <- layout_with_fr(karate) \n\nigraph.options(vertex.size = 10)\npar(mfrow = c(1,1))\nplot(karate,\n     layout = l)\n# Decoration\nV(karate)$label <- sub(\"Actor \",\"\", V(karate)$name)\n\n# Two leaders get shapes different from club members\nV(karate)$shape <- \"circle\"\nV(karate)[c(\"Mr Hi\", \"John A\")]$shape <- \"rectangle\"\n\nV(karate)$size  <- 20\nV(karate)$size2 <- 15\n\nplot(karate, \n     vertex.label = V(karate)$label,\n     layout  = l)\n# Define factions\nF1<-V(karate)[Faction==1]\nF2<-V(karate)[Faction==2]\n\n# Set up edge colors according to factions\nE(karate)[F1 %--% F1]$color<-\"darkgoldenrod2\"\nE(karate)[F2 %--% F2]$color<-\"lightblue\"\nE(karate)[F1 %--% F2]$color<-\"brown\"\n\n# Set up edge width according to weights\nE(karate)$width=E(karate)$weight\n\n# Plot the decorated graph, using same layout.\nplot(karate,layout=l)"},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"geo-mapping-coordinate-extraction-using-api-calls","chapter":"30 Geo-Mapping Coordinate Extraction Using API Calls","heading":"30 Geo-Mapping Coordinate Extraction Using API Calls","text":"Wei Xiong Toh","code":""},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"introduction-1","chapter":"30 Geo-Mapping Coordinate Extraction Using API Calls","heading":"30.1 Introduction","text":"Leaflet popular library R allows us plot interactive maps. extremely easy use many applications visualizing geometric information. However, relatively prohibitive use without prior access required Shapefiles spatial data. project aims automate process generating spatial data incorporating information existing data frames ease plotting Leaflet. detailed guide using Leaflet can found .","code":""},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"motivation-3","chapter":"30 Geo-Mapping Coordinate Extraction Using API Calls","heading":"30.2 Motivation","text":"Consider following toy problem: following population information want display outlining country data frame, individual country’s population shown color map.","code":"\ninput_list <- c('US', 'UK', 'China', 'Japan', 'Brazil')\ninput_vals <- c(329064917,  67508936, 1425887337, 128547854, 215313498)\ninput_df <- data.frame(name=input_list, val=input_vals)\nprint(input_df)##     name        val\n## 1     US  329064917\n## 2     UK   67508936\n## 3  China 1425887337\n## 4  Japan  128547854\n## 5 Brazil  215313498"},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"public-datasets","chapter":"30 Geo-Mapping Coordinate Extraction Using API Calls","heading":"30.2.1 Public Datasets","text":"several open source datasets contain Shapefiles/coordinates geographical boundaries countries. example can found . challenge following downloading data integrate existing information (population example) spatial coordinates Shapefiles, one needs manually filter dataset obtain. Another significant challenge arises areas interest located within dataset. Extracting coordinates multiple sources becomes tedious. example, following comparison might need several disparate sources.","code":"\ninput_list_2 <- c('New York', 'London', 'Shanghai', 'Tokyo', 'Rio')\ninput_vals_2 <- c(8467513, 9541000, 20217748, 37274000, 13634000)\ninput_df_2 <- data.frame(name=input_list_2, val=input_vals_2)\nprint(input_df_2)##       name      val\n## 1 New York  8467513\n## 2   London  9541000\n## 3 Shanghai 20217748\n## 4    Tokyo 37274000\n## 5      Rio 13634000"},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"api-calls","chapter":"30 Geo-Mapping Coordinate Extraction Using API Calls","heading":"30.2.2 API Calls","text":"alternative relying publicly available datasets spatial coordinates obtain information API calls. several steps process:Obtain relation number point interest OpenStreetMap\nObtain relation number point interest OpenStreetMap\nInput relation number Polygon Search API\nInput relation number Polygon Search API\nExtract coordinates response\nbenefit using method accurate levels administration, whether one looking country, state even city.\nnext section details process automate search given input data frames . Several improvements arise contribution:Eliminate need download several datasets multiple sources.Reduce time taken filter dataset obtain relevant coordinates.Obtain either point polygon coordinates.","code":""},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"code","chapter":"30 Geo-Mapping Coordinate Extraction Using API Calls","heading":"30.3 Code","text":"user specify following input parameters:input_type parameter allows user select level administration input list.granularity parameter used speed plotting reducing number points final multipolygon shape.","code":"\ninput_list <- c('US', 'UK', 'China', 'Japan', 'Brazil')\ninput_vals <- c(329064917,  67508936,   1425887337, 128547854, 215313498)\ninput_df <- data.frame(name=input_list, val=input_vals)\ninput_type <- 'Country'\ninput_ll_type <- 'poly'\ngranularity <- 50"},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"multipolygon","chapter":"30 Geo-Mapping Coordinate Extraction Using API Calls","heading":"30.3.1 Multipolygon","text":"Main code obtain longitude latitudes areas interest:","code":"\ninput_list <- c('US', 'UK', 'China', 'Japan', 'Brazil')\ninput_vals <- c(329064917,  67508936,   1425887337, 128547854, 215313498)\ninput_df <- data.frame(name=input_list, val=input_vals)\ninput_type <- 'Country'\ninput_ll_type <- 'poly'\ngranularity <- 50\n\n# check inputs\nstopifnot(input_type %in% c('City', 'State', 'Country'))\nstopifnot(input_ll_type %in% c('poly', 'point'))\nstopifnot(granularity > 0)\n\nif (str_detect(input_ll_type, 'poly')) {\n  mpoly_result = c()\n  \n  for (input in input_list) {\n    # replace spaces with '+'\n    input <- input %>% str_replace_all(string=., ' ', '+')\n    \n    # concat to get final query url\n    base_url <- 'https://www.openstreetmap.org/geocoder/search_osm_nominatim?query='\n    query_url <- paste(c(base_url, input), collapse='')\n    \n    # get relation number from OSM API\n    res <- GET(query_url)\n    results <- content(res, as='text')\n    results <- results %>% str_extract_all(., 'li class=.+')\n    results <- results[[1]]\n    \n    # Part 1: extract relation number matching input type\n    for (result in results) {\n      data_pref <- str_extract(result, 'data-prefix=.+?(\\\\s)')\n      # check for match with query\n      if (!is.na(str_extract(data_pref, input_type))) {\n        rn <- str_extract(result, 'data-id=.+?(\\\\s)')\n        rn <- rn %>% str_replace(., 'data-id=\\\"', '') %>% \n                str_replace(., '\\\" ', '')\n        break\n      }\n    }\n    \n    # Part 2: get latitude and longitude from OSM API\n    poly_base_url <- 'https://polygons.openstreetmap.fr/get_poly.py?id='\n    params <- '&params=0'\n    poly_url <- paste(c(poly_base_url, rn, params), collapse='')\n    \n    poly_res <- GET(poly_url)\n    poly_ll <- read.table(text=content(poly_res,as='text'), sep='\\n')\n    \n    # Part 3: create polygons for plotting\n    mpoly_coords <- list()\n    curr_coords <- list()\n    count <- 1\n    \n    for (coords in poly_ll[2:NROW(poly_ll)-1,]) {\n      # add to current mpoly list\n      if (str_detect(coords, 'END')) {\n        if (length(curr_coords) > 1) {\n          # verify closed polygon\n          if (curr_coords[[length(curr_coords)-1]] != curr_coords[[1]] || \n              curr_coords[[length(curr_coords)]] != curr_coords[[2]]) {\n            curr_coords[[length(curr_coords)+1]] <- curr_coords[[1]]\n            curr_coords[[length(curr_coords)+1]] <- curr_coords[[2]]\n          }\n          \n          mpoly_coords[[length(mpoly_coords)+1]] <- matrix(as.numeric(curr_coords), ncol=2, byrow=TRUE)\n          curr_coords <- list()\n          count <- 1\n        }\n      }\n      # check if coords are valid\n      else if ((count-1) %% granularity == 0 && nchar(coords) > 1) {\n        long = as.numeric(str_split(coords, '\\t')[[1]][2])\n        lat = as.numeric(str_split(coords, '\\t')[[1]][3])\n        \n        if (!is.na(long) && !is.na(lat)) {\n          curr_coords <- append(curr_coords, long)\n          curr_coords <- append(curr_coords, lat)\n        }\n      }\n      \n      count <- count + 1\n    }\n    \n    mpoly_shape <- st_multipolygon(list(mpoly_coords))\n    \n    mpoly_result[[length(mpoly_result)+1]] <- mpoly_shape\n  \n  }\n  \n  # create sf object to plot\n  mpoly_sf <- st_sf(input_df, geometry=mpoly_result)\n  \n} else {\n  # single point\n  mpoint_list <- c()\n  \n  for (input in input_list) {\n    # replace spaces with '+'\n    input <- input %>% str_replace_all(string=., ' ', '+')\n    \n    # concat to get final query url\n    base_url <- 'https://www.openstreetmap.org/geocoder/search_osm_nominatim?query='\n    query_url <- paste(c(base_url, input), collapse='')\n    \n    # get relation number from OSM API\n    res <- GET(query_url)\n    results <- content(res, as='text')\n    results <- results %>% str_extract_all(., 'li class=.+')\n    results <- results[[1]]\n    \n    # Part 1: extract relation number matching input type\n    for (result in results) {\n      data_pref <- str_extract(result, 'data-prefix=.+?(\\\\s)')\n      # check for match with query\n      if (!is.na(str_extract(data_pref, input_type))) {\n        rn <- str_extract(result, 'data-id=.+?(\\\\s)')\n        rn <- rn %>% str_replace(., 'data-id=\\\"', '') %>% \n          str_replace(., '\\\" ', '')\n        break\n      }\n    }\n    \n    # Part 2: get latitude and longitude from OSM API\n    base_url <- 'https://nominatim.openstreetmap.org/details?osmtype=R&osmid='\n    query_url <- paste(c(base_url, rn, '&format=json'), collapse='')\n    \n    # extract info from OSM API\n    res <- fromJSON(query_url)\n    \n    if (\"centroid\" %in% names(res)) {\n      long <- res$centroid$coordinates[1]\n      lat <- res$centroid$coordinates[2]\n    } else {\n      long <- NULL\n      lat <- NULL\n    }\n    \n    if (!is.na(long) && !is.na(lat)) {\n      mpoint_list[[length(mpoint_list)+1]] <- st_point(c(long, lat))\n    }\n  }\n  \n  mpoint_sf <- st_sf(input_df, geometry=mpoint_list)\n  \n}"},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"multipolygon-plots","chapter":"30 Geo-Mapping Coordinate Extraction Using API Calls","heading":"30.3.1.1 Multipolygon Plots","text":"","code":"\ninput_colorP <- \"YlOrRd\"\n\nleaflet(mpoly_sf) %>% \n    addTiles() %>%\n    addPolygons(\n      fillOpacity = 1, smoothFactor = 0.75,\n      color=~colorNumeric(input_colorP, val)(val)\n    )"},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"points","chapter":"30 Geo-Mapping Coordinate Extraction Using API Calls","heading":"30.3.2 Points","text":"","code":"\ninput_list <- c('New York', 'London', 'Shanghai', 'Tokyo', 'Rio')\ninput_vals <- c(8467513,    9541000,    20217748, 37274000, 13634000)\ninput_df <- data.frame(name=input_list, val=input_vals)\ninput_type <- 'City'\ninput_ll_type <- 'point'\ngranularity <- 50\n\n# check inputs\nstopifnot(input_type %in% c('City', 'State', 'Country'))\nstopifnot(input_ll_type %in% c('poly', 'point'))\nstopifnot(granularity > 0)\n\nif (str_detect(input_ll_type, 'poly')) {\n  mpoly_result <- c()\n  \n  for (input in input_list) {\n    # replace spaces with '+'\n    input <- input %>% str_replace_all(string=., ' ', '+')\n    \n    # concat to get final query url\n    base_url <- 'https://www.openstreetmap.org/geocoder/search_osm_nominatim?query='\n    query_url <- paste(c(base_url, input), collapse='')\n    \n    # get relation number from OSM API\n    res <- GET(query_url)\n    results <- content(res, as='text')\n    results <- results %>% str_extract_all(., 'li class=.+')\n    results <- results[[1]]\n    \n    # Part 1: extract relation number matching input type\n    for (result in results) {\n      data_pref <- str_extract(result, 'data-prefix=.+?(\\\\s)')\n      # check for match with query\n      if (!is.na(str_extract(data_pref, input_type))) {\n        rn <- str_extract(result, 'data-id=.+?(\\\\s)')\n        rn <- rn %>% str_replace(., 'data-id=\\\"', '') %>% \n                str_replace(., '\\\" ', '')\n        break\n      }\n    }\n    \n    # Part 2: get latitude and longitude from OSM API\n    poly_base_url <- 'https://polygons.openstreetmap.fr/get_poly.py?id='\n    params <- '&params=0'\n    poly_url <- paste(c(poly_base_url, rn, params), collapse='')\n    \n    poly_res <- GET(poly_url)\n    poly_ll <- read.table(text=content(poly_res,as='text'), sep='\\n')\n    \n    # Part 3: create polygons for plotting\n    mpoly_coords <- list()\n    curr_coords <- list()\n    count <- 1\n    \n    for (coords in poly_ll[2:NROW(poly_ll)-1,]) {\n      # add to current mpoly list\n      if (str_detect(coords, 'END')) {\n        if (length(curr_coords) > 1) {\n          # verify closed polygon\n          if (curr_coords[[length(curr_coords)-1]] != curr_coords[[1]] || \n              curr_coords[[length(curr_coords)]] != curr_coords[[2]]) {\n            curr_coords[[length(curr_coords)+1]] <- curr_coords[[1]]\n            curr_coords[[length(curr_coords)+1]] <- curr_coords[[2]]\n          }\n          \n          mpoly_coords[[length(mpoly_coords)+1]] <- matrix(as.numeric(curr_coords), ncol=2, byrow=TRUE)\n          curr_coords <- list()\n          count <- 1\n        }\n      }\n      # check if coords are valid\n      else if ((count-1) %% granularity == 0 && nchar(coords) > 1) {\n        long = as.numeric(str_split(coords, '\\t')[[1]][2])\n        lat = as.numeric(str_split(coords, '\\t')[[1]][3])\n        \n        if (!is.na(long) && !is.na(lat)) {\n          curr_coords <- append(curr_coords, long)\n          curr_coords <- append(curr_coords, lat)\n        }\n      }\n      \n      count <- count + 1\n    }\n    \n    mpoly_shape <- st_multipolygon(list(mpoly_coords))\n    \n    mpoly_result[[length(mpoly_result)+1]] <- mpoly_shape\n  \n  }\n  \n  # create sf object to plot\n  mpoly_sf <- st_sf(input_df, geometry=mpoly_result)\n  \n} else {\n  # single point\n  mpoint_list <- c()\n  \n  for (input in input_list) {\n    # replace spaces with '+'\n    input <- input %>% str_replace_all(string=., ' ', '+')\n    \n    # concat to get final query url\n    base_url <- 'https://www.openstreetmap.org/geocoder/search_osm_nominatim?query='\n    query_url <- paste(c(base_url, input), collapse='')\n    \n    # get relation number from OSM API\n    res <- GET(query_url)\n    results <- content(res, as='text')\n    results <- results %>% str_extract_all(., 'li class=.+')\n    results <- results[[1]]\n    \n    # Part 1: extract relation number matching input type\n    for (result in results) {\n      data_pref <- str_extract(result, 'data-prefix=.+?(\\\\s)')\n      # check for match with query\n      if (!is.na(str_extract(data_pref, input_type))) {\n        rn <- str_extract(result, 'data-id=.+?(\\\\s)')\n        rn <- rn %>% str_replace(., 'data-id=\\\"', '') %>% \n          str_replace(., '\\\" ', '')\n        break\n      }\n    }\n    \n    # Part 2: get latitude and longitude from OSM API\n    base_url <- 'https://nominatim.openstreetmap.org/details?osmtype=R&osmid='\n    query_url <- paste(c(base_url, rn, '&format=json'), collapse='')\n    \n    # extract info from OSM API\n    res <- fromJSON(query_url)\n    \n    if (\"centroid\" %in% names(res)) {\n      long <- res$centroid$coordinates[1]\n      lat <- res$centroid$coordinates[2]\n    } else {\n      long <- NULL\n      lat <- NULL\n    }\n    \n    if (!is.na(long) && !is.na(lat)) {\n      mpoint_list[[length(mpoint_list)+1]] <- st_point(c(long, lat))\n    }\n  }\n  \n  mpoint_sf <- st_sf(input_df, geometry=mpoint_list)\n  \n}"},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"single-marker-plots","chapter":"30 Geo-Mapping Coordinate Extraction Using API Calls","heading":"30.3.3 Single Marker Plots","text":"","code":"\nleaflet(mpoint_sf) %>% \n    addTiles() %>% \n    addMarkers()"},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"circle-plots","chapter":"30 Geo-Mapping Coordinate Extraction Using API Calls","heading":"30.3.4 Circle Plots","text":"","code":"\nleaflet(mpoint_sf) %>% \n    addTiles() %>% \n    addCircles(radius=~sqrt(val)*50)"},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"evaluation-1","chapter":"30 Geo-Mapping Coordinate Extraction Using API Calls","heading":"30.4 Evaluation","text":"set code generates different simple feature collection objects depending input variables set user. allows lot flexibility obtaining spatial coordinates based user’s needs. compared traditional method downloading open source spatial data, approach convenient user obtain data, user need know manipulate data prior plotting.Since method relies two key API calls, accuracy limited accuracy data maintained API libraries used. feasible rely API calls administrators OpenStreetMap decide remove public access libraries charge fee usage.","code":""},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"future-work","chapter":"30 Geo-Mapping Coordinate Extraction Using API Calls","heading":"30.5 Future Work","text":"Given time work project, publish code library R overall code neater even error handling incorporated.","code":""},{"path":"geo-mapping-coordinate-extraction-using-api-calls.html","id":"conclusion-5","chapter":"30 Geo-Mapping Coordinate Extraction Using API Calls","heading":"30.6 Conclusion","text":"project automates method API calls efficiently extract geospatial coordinates points interests (POIs), can merged existing data frames visualizing spatial information. Users need provide names POIs, administrative level POIs (Country/State/City), type plot required granularity selecting coordinates plotting polygons. Users can use combined data frame input Leaflet visualize data.","code":""},{"path":"edav-garden.html","id":"edav-garden","chapter":"31 EDAV Garden","heading":"31 EDAV Garden","text":"Chenyu Zhang🌐Address: edav-garden.netlify.appEDAV Garden digital garden EDAV.\nsimple words, collection published notes, like edav.info.\nHowever, EDAV Garden special features:captures journey learning EDAV. practice learning public, learning public.notes collection external information; instead, notes containing thoughts interpretation.Therefore, may mistakes question marks scattered around. may fixed come back later.frozen garden; evolving garden continue gardening learn.site reference, also roam.","code":""},{"path":"edav-garden.html","id":"how-to-explore-the-garden","chapter":"31 EDAV Garden","heading":"31.1 How to Explore the Garden","text":"significant difference garden traditional book/wiki/website notes linear: chronological logical order. Instead, connected contextual links form knowledge graph, making garden explorable. also matches fundamental problem EDAV: exploratory vs. explanatory. navigate different notes, can follow different types links, illustrated belowan outgoing link points another note;backlink points note mentions current note;interactive graph presents connections current note .","code":""},{"path":"edav-garden.html","id":"main-sections","chapter":"31 EDAV Garden","heading":"31.2 Main Sections","text":"Right now, three main sections EDAV garden:Plots Gallery: gallery graphs categorized data type.R Garden: notes R.Git Garden: notes Git.","code":""},{"path":"using-raster-data-in-r.html","id":"using-raster-data-in-r","chapter":"32 Using raster data in R","heading":"32 Using raster data in R","text":"Andre Evard","code":""},{"path":"using-raster-data-in-r.html","id":"introduction-to-rasters","chapter":"32 Using raster data in R","heading":"32.1 Introduction to rasters","text":"Community Contribution project gives overview online book\nGeocomputation R,\nspecifically regards raster type geographic data representation.\nPlease note, second version book active development, \npackages references may --date near future.Frequent references package called terra made, may read \ndetail . extra\ndata sources used detailed book, except snow cover data found website,\nZenodo, Geocomputation R already links .","code":""},{"path":"using-raster-data-in-r.html","id":"what-is-a-raster","chapter":"32 Using raster data in R","heading":"32.1.1 What is a Raster?","text":"raster \nspatial model crosswise divides area land regularized boxes,\ncells. raster strictly rectangular X Y dimensions, \ncell dimensions defined point one corner, \nstores associated values, say temperature, elevation, population density, \nlist goes . raster may multiple layers, cell exactly one\nvalue per layer.rasters extremely useful environmental scientific modeling outside\nconfines political boundaries, example capturing number frog\nspecies discovered square kilometer sections Amazon Rainforest, \ncan widespread RGB pixels screen viewing \n.reference, type geocomputational model book concerned\nvector.\ndefined cells specified shapes geometries, better\nable represent human political boundaries instance. Although project\ncentered raster, vectors referenced occasion data\ntranslation transformation.","code":""},{"path":"using-raster-data-in-r.html","id":"what-is-terra","chapter":"32 Using raster data in R","heading":"32.1.2 What is Terra?","text":"(covers sections 2.3.1 2.3.3)Terra primary package\nGeocomputation R cites raster processing. Reportedly, terra \ncommon usecase easier learn package, whilst stars’s niche\npremire specialized datasets. sf third final package book describes heavy detail, vector data.Terra built upon older raster package, internal foundation \nC++ efficiency. Raster representation, one book’s first\ncode snippets, can see default attributes.can see , raster defined :dimensions:\nnrow: number rows\nncol: number columns\nnlyr: number layers\nnrow: number rowsncol: number columnsnlyr: number layersresolution: space covered single cell, two dimensionsextent: boundaries coordinatecoordinate reference: distance’s data type (“EPSG:4326” )access additional fields source, name, class, \nsmall metadata.terra also provides dedicated functions return components:Dimensions: ndim()Cells: ncell()Spatial Extent: ext()Coordinate Reference System: crs()Geocomputation R makes frequent usage example, srtm, elevation map\nsurrounding Utah national park, though provide examples,\nsometimes quick reference provides best example.\nQuickly plotting shows following graphic:intuitively, cell value, represented visually pixels\ncolor scale.","code":"\nraster_filepath = system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nsrtm = rast(raster_filepath)\nplot(srtm)"},{"path":"using-raster-data-in-r.html","id":"raster-creation","chapter":"32 Using raster data in R","heading":"32.2 Raster Creation","text":"","code":""},{"path":"using-raster-data-in-r.html","id":"the-direct-approach","chapter":"32 Using raster data in R","heading":"32.2.1 The direct approach","text":"(covers sections 2.3.4, 3.3.1)\nknow rasters , basic data structure. Okay, well \ncreate, load, populate general rasters? ’s options, always.Naturally, can directly create raster rast() function follows:isn’t exactly terribly useful raster, intuitive.\ncan also extract replace values, though \naren’t recommended anything past small structures experiments. \nknowledge row/col values ’re targeting, can directly set values\nsimilar fashion, using c() functions one cell desired.\nMutliple layers may also modified time way.\nSee last section better approach raster manipulation.","code":"\nmanual_raster = rast(\n  nrows = 100,\n  ncols = 100,\n  nlyrs = 2,\n  resolution = .5,\n  xmin = -25,\n  xmax = 25,\n  ymin = -25,\n  ymax = 25,\n  vals = 1:20000\n)\nplot(manual_raster)\nterra::extract(manual_raster, 10, 10)##   lyr.1 lyr.2\n## 1    10 10010\nmanual_raster[15, -10] <- 15000\nmanual_raster[15, -10]##   lyr.1 lyr.2\n## 1    NA    NA\nmanual_raster[c(19, 20, 21), c(19, 20, 21)] <- cbind(c(5000:5008), c(15000:15008)) \nplot(manual_raster)\n# You can also use <raster>[] as a shortcut of .values()."},{"path":"using-raster-data-in-r.html","id":"raster-files-and-their-formats","chapter":"32 Using raster data in R","heading":"32.2.2 Raster files and their formats","text":"(covers sections 8.5 8.6.2)Loading files preferred approach far robust, \nimage file already handy (one producedyourself!).Firstly, ’s lot know raster files. Pulling section 8.5, can\nfind quick overview popular raster (vector) formats. short though,\nprimary file format GeoTIFF, embeds additional geospatial data\n(coordinate systems) .tif/.tiff image files. Among others, NASA \nGoogle data publicaly available COGs.Outside GeoTIFF, also Arc ASCII (.asc) text-based storage,\nSpatiaLite extension SQLite, ESRI FileGDB proprietary.\n, outside specific use cases, recommendation use GeoTIFF.Notably, GeoTIFF also supports Cloud Optimized GeoTIFFs (COG), allows\nrasters hosted HTTP servers enables users download \nsegment can rather large files.Given path, rast() function\ngood job loading rasters.previously mentioned, whole GeoTIFF downloaded \nway, merely attached . /vsicurl/ followed https url ’s\nnecessary download information, provided image provider\nlocated already course. relevant portions loaded utilize\nmanipulate manner, :discuss cropping later. Naturally, can also load local files using\nrast(). can produce save , use packages spData.","code":"\nsnowurl = \"/vsicurl/https://zenodo.org/record/5774954/files/clm_snow.cover_esa.modis_p05_1km_s0..0cm_2001.01_epsg4326_v1.tif\"\nsnow = rast(snowurl)\nsnow## class       : SpatRaster \n## dimensions  : 17924, 43200, 1  (nrow, ncol, nlyr)\n## resolution  : 0.008333333, 0.008333333  (x, y)\n## extent      : -180, 180, -61.99666, 87.37  (xmin, xmax, ymin, ymax)\n## coord. ref. : lon/lat WGS 84 (EPSG:4326) \n## source      : clm_snow.cover_esa.modis_p05_1km_s0..0cm_2001.01_epsg4326_v1.tif \n## name        : clm_snow.cover_esa.modis_p05_1km_s0..0cm_2001.01_epsg4326_v1\ndimension_raster = rast(\n  nrows = 1000,\n  ncols = 1000,\n  resolution = 0.008333333,\n  xmin = 2.5,\n  xmax = 17.5,\n  ymin = 42.5,\n  ymax = 57.5\n)\nsnow_clipped = crop(snow, dimension_raster)\nsnow_clipped## class       : SpatRaster \n## dimensions  : 1800, 1800, 1  (nrow, ncol, nlyr)\n## resolution  : 0.008333333, 0.008333333  (x, y)\n## extent      : 2.499993, 17.49999, 42.50334, 57.50333  (xmin, xmax, ymin, ymax)\n## coord. ref. : lon/lat WGS 84 (EPSG:4326) \n## source      : memory \n## name        : clm_snow.cover_esa.modis_p05_1km_s0..0cm_2001.01_epsg4326_v1 \n## min value   :                                                            0 \n## max value   :                                                          100\nplot(snow_clipped, main=\"Snow cover around the Alps in Jan 2001\")\n# what it would look like with a local file \n#snow_rast = rast(\"2019_jan_snow_cover.tif\") \nsnow_rast = rast(\"/vsicurl/https://zenodo.org/record/5774954/files/clm_snow.cover_esa.modis_p95_1km_s0..0cm_2019.01_epsg4326_v1.tif?download=1\")\nUSA_NE_raster = rast(\n  nrows = 1000,\n  ncols = 1000,\n  resolution = 0.008333333,\n  xmin = -83.5,\n  xmax = -68.5,\n  ymin = 35.5,\n  ymax = 50.5\n)\ncropped_NE_snow = crop(snow_rast, USA_NE_raster)\nplot(cropped_NE_snow, main=\"USA Northeast snow cover in jan 2019\")"},{"path":"using-raster-data-in-r.html","id":"rasterization","chapter":"32 Using raster data in R","heading":"32.2.3 Rasterization","text":"(covers section 6.4)’s one type raster creation: transformations vectors,\nrasterize. Usage \nrequires knowledge spatial vector data time go depth\n, short, set extremely flexible operations convert\nvarious statistics vector set transform sort grid.\nExamples provided state boundaries, aggregating count average values\npoint clusters, ’re confident right vector datasets \npossibilities quite endless.Geocomputation R’s section 6.4 ’re curious.","code":""},{"path":"using-raster-data-in-r.html","id":"raster-manipulations-and-their-applications","chapter":"32 Using raster data in R","heading":"32.3 Raster Manipulations, and their applications","text":"knowing create well good, can \nrasters? use ?","code":""},{"path":"using-raster-data-in-r.html","id":"spatial-operations","chapter":"32 Using raster data in R","heading":"32.3.0.1 Spatial operations","text":"good number geometrically-based operations terra supports\nbox. sections Geocomputation R lays Subsetting 4.3.1,\nLocal, Focal,\nZonal,\nGlobal\nmap algebra operations sections 4.3.3 4.3.6, raster Merging\nsection 4.3.8","code":""},{"path":"using-raster-data-in-r.html","id":"subsetting","chapter":"32 Using raster data in R","heading":"32.3.1 Subsetting","text":"(covers section 4.3.1)ways (cleanly) draw values individual cells, cells,\nrows, columns ids coordinates, slice apart data.\ncellFromXY() assorted functions, \nterra::extract two good ones,\nthough beware overlap tidyverse’s extract. can also define smaller\nrasters clip crop , done examples prior.well direct access, know either desired row & column numbers\n1D cell ids:","code":"\nxy <- matrix(c(-73.94, 40.73), ncol = 2)\nmatrix_cells = cellFromXY(cropped_NE_snow, xy)\ncropped_NE_snow[matrix_cells]##   clm_snow.cover_esa.modis_p95_1km_s0..0cm_2019.01_epsg4326_v1\n## 1                                                           31\nterra::extract(cropped_NE_snow, xy)##   clm_snow.cover_esa.modis_p95_1km_s0..0cm_2019.01_epsg4326_v1\n## 1                                                           31\nfilter = rast(\n  xmin = -74.5,\n  xmax = -73.5,\n  ymin = 40.2,\n  ymax = 41.2,\n  resolution = 0.0083\n)\n\nNYC_sieve = terra::extract(cropped_NE_snow, ext(filter))\nhead(NYC_sieve)##   clm_snow.cover_esa.modis_p95_1km_s0..0cm_2019.01_epsg4326_v1\n## 1                                                           62\n## 2                                                           61\n## 3                                                           68\n## 4                                                           74\n## 5                                                           75\n## 6                                                           81\n# extract does a lot more things more flexibly, cellfromxy and related\n# better for method header precision and location manipulation\n# NYC was not very snowy that winter, it seems...\nsrtm[50, 50]##   srtm\n## 1 1885\nsrtm[c(49, 50, 51), c(49, 50, 51)]##   srtm\n## 1 1854\n## 2 1919\n## 3 1986\n## 4 1835\n## 5 1885\n## 6 1962\n## 7 1819\n## 8 1869\n## 9 1929\nsrtm[100]##   srtm\n## 1 2034"},{"path":"using-raster-data-in-r.html","id":"cropping-and-masking","chapter":"32 Using raster data in R","heading":"32.3.2 Cropping and Masking","text":"(covers section 6.2)Two similar concepts, crop() mask()\nperform task downsizing raster, typically tandem, require\nusage additional spatial object, practice typically vector.\ndiffer cropping reduces dimensions original raster\nbased second object’s boundaries, whereas masking sets everything\noutside second object’s boundaries zero, leaves structure intact.course can combine :","code":"\nzion = read_sf(system.file(\"vector/zion.gpkg\", package = \"spDataLarge\"))\nzion = st_transform(zion, crs(srtm))\nsrtm_cropped = crop(srtm, zion)\nsrtm_masked = mask(srtm, zion)\nplot(zion[0], main=\"Outline of Zion national park\")\nplot(srtm_cropped, main=\"SRTM cropped by Zion national park\")\nplot(srtm_masked, main=\"SRTM masked by Zion national park\")\nplot(crop(srtm_masked, zion))"},{"path":"using-raster-data-in-r.html","id":"map-algebra","chapter":"32 Using raster data in R","heading":"32.3.3 Map Algebra","text":"(covers sections 4.3.2, 4.3.3, 4.3.4,\n4.3.5, \n4.3.6)can see , many algebraic calculations can performed rasters local level.well boolean operationsAnd use app() efficiency large data.Given right attributes data (book lists coil class, pH, etc),\ncan used predictive modeling.Focal transformations:Similar sliding windows machine learning algorithms, focal operations\ntransform cells neighborhoods, working process extremes whilst\npreserving form original data. Another potential focal operator \nterrain(), calculates\nterrain characteristics elevation data.Zonal similar focal one\nlevel aggregation . uses second raster define zones group \nfirst , applying statistical measurements divided population.Global cross-raster statistical\ncalculations, can used distance\npossibilities–namely, summary functions ’d see type \ndataset.summary() describes archetypal\nstatistical bands, freq() counts \noccurrences class, boxplot(),\ndensity(),\nhist(),\npairs() can run ---box\nimmediate exploration. global() remains robust approach, can\nself-define statistical function apply particular dataset,\nmuch can aforementioned app() entourage.","code":"\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nplot(srtm)\nplot(log(srtm))\nplot(srtm ^ 2)\nplot(snow_clipped > 20, main=\"Areas with more than 20% snow cover\")\nplot(snow_clipped == 0, main=\"Areas with no snow cover\")\napp(snow_clipped, fun=function(i) { 2 * log(i) })## class       : SpatRaster \n## dimensions  : 1800, 1800, 1  (nrow, ncol, nlyr)\n## resolution  : 0.008333333, 0.008333333  (x, y)\n## extent      : 2.499993, 17.49999, 42.50334, 57.50333  (xmin, xmax, ymin, ymax)\n## coord. ref. : lon/lat WGS 84 (EPSG:4326) \n## source      : memory \n## name        :   lyr.1 \n## min value   :    - ?  \n## max value   : 9.21034\nmean_srtm = focal(srtm, w = matrix(1, nrow = 5, ncol = 5), fun = mean)\nplot(mean_srtm)\nsummary(snow_clipped) # 3rd quartile of 1, but mean of 10?##  clm_snow.cover_esa.modis_p05_1km_s0..0cm_2001.01_epsg4326_v1\n##  Min.   :  0.00                                              \n##  1st Qu.:  0.00                                              \n##  Median :  0.00                                              \n##  Mean   : 10.27                                              \n##  3rd Qu.:  1.00                                              \n##  Max.   :100.00                                              \n##  NA's   :28886\nsnow_tenths = app(snow_clipped, fun=function(i) {ceiling(i / 10) * 10})\nfreq(snow_tenths) # Indeed, 0 is by far the most common##    layer value   count\n## 1      1     0 1696965\n## 2      1    10  190843\n## 3      1    20   61456\n## 4      1    30   44132\n## 5      1    40   42039\n## 6      1    50   43384\n## 7      1    60   46092\n## 8      1    70   47859\n## 9      1    80   46767\n## 10     1    90   43627\n## 11     1   100   46772"},{"path":"using-raster-data-in-r.html","id":"extraction","chapter":"32 Using raster data in R","heading":"32.3.4 Extraction","text":"(covers section 6.3)Extraction process gathering targetted subset values raster,\nuse terra::extract().\ntake use vector spatial objects, result rather\npowerful specialized inferences. ’ll keep brief, please visit 6.3\n’re interested .Example use cases raster extraction can vary wildly creativity.\nquick examples, import boundaries European country\naround alps aggregate snow cover , find highest/lowest points\nalong Zion National Park’s boundaries. demonstrations book provides \npicking elevation selection points, elevation graph along \nline segment planning hike. Categorical continuous data can \nextracted, sometimes across different layers.One quick note–book points can performance issues scale,\nuse exact_extract() exactextract package possible.","code":""},{"path":"using-raster-data-in-r.html","id":"vectorization","chapter":"32 Using raster data in R","heading":"32.3.5 Vectorization","text":"(covers section 6.5)opposite rasterization, spatial vectorization transforms rasters vectors.\n’re familiar vectors, abundance flexibility , lying .spatvector class \nfunctions. One particularly eye-catching use quick visualization \ncontour lines using .contour() follows:","code":"\ncl = as.contour(srtm)\nplot(cl, axes=FALSE)"},{"path":"using-raster-data-in-r.html","id":"file-output","chapter":"32 Using raster data in R","heading":"32.3.6 File Output","text":"(covers section 8.7.2)\nOkay, ’ve done experiments, like preserve rasters\nshare results save later computational power. ?Simple. writeRaster().little involved, selecting bit datatype storage,\nfiletypes writeRaster can’t infer filename, names layer names,\nassortment memory-related variables debugging throughput \nparticularly large rasters. Compared loading rasters, though,\nfunction conveniently straightforward, yet still plenty capable terms\nflexibility; like rasters hole, rigid, yet surprisingly flexible just one layer beyond surface.","code":"\n# Is how I would do it, if I wanted to dirty the CC project\n# writeRaster(snow_clipped, filename=\"2007_jan_eu_snow.tif\", datatype=\"INT1U\",\n#             overwrite=TRUE)"},{"path":"make-geographical-maps-with-different-packages.html","id":"make-geographical-maps-with-different-packages","chapter":"33 Make Geographical Maps with Different Packages","heading":"33 Make Geographical Maps with Different Packages","text":"Xindi Deng","code":"\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(osmextract)"},{"path":"make-geographical-maps-with-different-packages.html","id":"introduction-2","chapter":"33 Make Geographical Maps with Different Packages","heading":"33.1 Introduction","text":"spacial analysis, always focus map USA extract dataset maps package R. However, number datasets maps package limited contain information sub-regions many countries, China, Japan .tutorial, first introduce common way plot map extracting coordinates “maps” package, talk “osmextract” package contains information geographical maps.","code":""},{"path":"make-geographical-maps-with-different-packages.html","id":"make-geographical-maps-with-map-package","chapter":"33 Make Geographical Maps with Different Packages","heading":"33.2 Make geographical maps with “map” package","text":"make geographical map, first need know coordinates region boundaries target country. introduce two ways extract data. first one user-friendly second one contains data sets.","code":""},{"path":"make-geographical-maps-with-different-packages.html","id":"import-dataset-with-function-map_data","chapter":"33 Make Geographical Maps with Different Packages","heading":"33.2.1 Import dataset with function “map_data”","text":"One common user-friendly functions extract map data maps package “map_data”. can turn data maps package data frame suitable plotting ggplot2.","code":""},{"path":"make-geographical-maps-with-different-packages.html","id":"usage-of-map_data","chapter":"33 Make Geographical Maps with Different Packages","heading":"33.2.1.1 Usage of “map_data”","text":"map_data(map, region = “.”, exact = FALSE, …)map: name map provided maps package.\n choices map include world map, three USA databases (usa, state, county), (including Italy database, France database, New Zealand database ). \n can use help(package='maps') check map data sets can used \"map_data\".map: name map provided maps package.region: select sub-regions map. Default “.”.region: select sub-regions map. Default “.”.","code":" The choices of map include a world map, three USA databases (usa, state, county), and more (including Italy database, France database, New Zealand database and so on). \n We can use help(package='maps') to check what map data sets can be used by \"map_data\"."},{"path":"make-geographical-maps-with-different-packages.html","id":"dataset-exploration","chapter":"33 Make Geographical Maps with Different Packages","heading":"33.2.1.2 Dataset exploration","text":"use map=“state” example see format function returnThe data set “states” 15537 coordinates 6 features.\n- long: longitude coordinate boundaries\n- lat: latitude coordinate boundaries\n- group: minimum closed region unique group number\n- region: name region","code":"\nstatesMap <- map_data(\"state\")\nhead(statesMap)##        long      lat group order  region subregion\n## 1 -87.46201 30.38968     1     1 alabama      <NA>\n## 2 -87.48493 30.37249     1     2 alabama      <NA>\n## 3 -87.52503 30.37249     1     3 alabama      <NA>\n## 4 -87.53076 30.33239     1     4 alabama      <NA>\n## 5 -87.57087 30.32665     1     5 alabama      <NA>\n## 6 -87.58806 30.32665     1     6 alabama      <NA>\nunique(statesMap$region)##  [1] \"alabama\"              \"arizona\"              \"arkansas\"            \n##  [4] \"california\"           \"colorado\"             \"connecticut\"         \n##  [7] \"delaware\"             \"district of columbia\" \"florida\"             \n## [10] \"georgia\"              \"idaho\"                \"illinois\"            \n## [13] \"indiana\"              \"iowa\"                 \"kansas\"              \n## [16] \"kentucky\"             \"louisiana\"            \"maine\"               \n## [19] \"maryland\"             \"massachusetts\"        \"michigan\"            \n## [22] \"minnesota\"            \"mississippi\"          \"missouri\"            \n## [25] \"montana\"              \"nebraska\"             \"nevada\"              \n## [28] \"new hampshire\"        \"new jersey\"           \"new mexico\"          \n## [31] \"new york\"             \"north carolina\"       \"north dakota\"        \n## [34] \"ohio\"                 \"oklahoma\"             \"oregon\"              \n## [37] \"pennsylvania\"         \"rhode island\"         \"south carolina\"      \n## [40] \"south dakota\"         \"tennessee\"            \"texas\"               \n## [43] \"utah\"                 \"vermont\"              \"virginia\"            \n## [46] \"washington\"           \"west virginia\"        \"wisconsin\"           \n## [49] \"wyoming\""},{"path":"make-geographical-maps-with-different-packages.html","id":"plot-maps","chapter":"33 Make Geographical Maps with Different Packages","heading":"33.2.2 Plot maps","text":"","code":""},{"path":"make-geographical-maps-with-different-packages.html","id":"the-logic-of-ploting-maps","chapter":"33 Make Geographical Maps with Different Packages","heading":"33.2.2.1 The logic of ploting maps","text":"plot maps, take minimum closed region polygon. Since know coordinates boundaries minimum closed region, can plot polygon taking longitude x-axis taking latitude y-axis. Since minimum closed region unique group number, can group coordinates group number plot polygons one graph.","code":""},{"path":"make-geographical-maps-with-different-packages.html","id":"plot-the-map","chapter":"33 Make Geographical Maps with Different Packages","heading":"33.2.2.2 Plot the map","text":"","code":"\nggplot(statesMap,aes(x=long,y=lat,group=group))+\n  geom_polygon(fill=\"white\",color=\"black\")\nNewYorkMap <- subset(statesMap, region %in% c(\"new jersey\", \"new york\",\"connecticut\"))\nggplot(NewYorkMap,aes(x=long,y=lat, group=group, fill = region))+\n  geom_polygon(color=\"white\")"},{"path":"make-geographical-maps-with-different-packages.html","id":"data-visualization-on-the-map","chapter":"33 Make Geographical Maps with Different Packages","heading":"33.2.2.3 Data visualization on the map","text":"Step 1: prepare data frame.Import dataset need visualized data frame. Combine new data frame data frame map region names. can get data frame suitable plotting.Step 2: plot visualization result","code":"\nmurders <- read_csv(\"./resources/make_geographical_maps_with_different_packages/murders.csv\", show_col_types = FALSE)\nmurders$region=tolower(murders$State)\nmurderMap <- merge(statesMap,murders,by=\"region\")\nhead(murderMap)##    region      long      lat group order subregion   State Population\n## 1 alabama -87.46201 30.38968     1     1      <NA> Alabama    4779736\n## 2 alabama -87.48493 30.37249     1     2      <NA> Alabama    4779736\n## 3 alabama -87.52503 30.37249     1     3      <NA> Alabama    4779736\n## 4 alabama -87.53076 30.33239     1     4      <NA> Alabama    4779736\n## 5 alabama -87.57087 30.32665     1     5      <NA> Alabama    4779736\n## 6 alabama -87.58806 30.32665     1     6      <NA> Alabama    4779736\n##   PopulationDensity Murders GunMurders GunOwnership\n## 1             94.65     199        135        0.517\n## 2             94.65     199        135        0.517\n## 3             94.65     199        135        0.517\n## 4             94.65     199        135        0.517\n## 5             94.65     199        135        0.517\n## 6             94.65     199        135        0.517\nggplot(murderMap,aes(x=long, y=lat, group=group,fill=GunMurders))+\n  geom_polygon(color=\"black\")+\n  scale_fill_gradient(low=\"white\",high=\"red\",guide=\"legend\")+\n  coord_fixed(1.5)"},{"path":"make-geographical-maps-with-different-packages.html","id":"make-geographical-maps-with-osmextract-package","chapter":"33 Make Geographical Maps with Different Packages","heading":"33.3 Make geographical maps with “osmextract” package","text":"Even though defined function “map_data” easy use, maps package contains data coordinates countries. extract coordinates boundaries countries, China, can use “osmextract” package.","code":""},{"path":"make-geographical-maps-with-different-packages.html","id":"import-dataset-with-object-openstreetmap_fr_zones","chapter":"33 Make Geographical Maps with Different Packages","heading":"33.3.1 Import dataset with object “openstreetmap_fr_zones”","text":"“openstreetmap_fr_zones” sf object geographical zones taken download.openstreetmap.fr.","code":""},{"path":"make-geographical-maps-with-different-packages.html","id":"define-a-function-to-extract-the-coordinations-data","chapter":"33 Make Geographical Maps with Different Packages","heading":"33.3.1.1 Define a function to extract the coordinations data","text":"define function called “extract_map_data”, can turn data openstreetmap_fr_zones data frame suitable plotting ggplot2.","code":"\nlibrary(sf)\nlibrary(osmextract) \n#> OpenStreetMap® is open data, licensed under the Open Data Commons Open Database License (ODbL) by the OpenStreetMap Foundation (OSMF).\n#> Data (c) OpenStreetMap contributors, ODbL 1.0. https://www.openstreetmap.org/copyright.\n#> Check the package website, https://docs.ropensci.org/osmextract/, for more details.\n\nextract_map_data <- function(region_name){\n  # get polygons in the target region\n  poly_region <- openstreetmap_fr_zones[which(tolower(openstreetmap_fr_zones$parent) == region_name), ]\n\n  # extract the coordinations and save the coordinations in a data.frame\n  poly_region_coords <- as.data.frame(st_coordinates(poly_region))\n\n  # extract the region name\n  my_times <- vapply(st_geometry(poly_region), function(x) nrow(st_coordinates(x)), numeric(1))\n  poly_region_coords$region_name <- rep(tolower(poly_region$name), times = my_times)\n  \n  # make the format of data frame similar to that from map_data and make it suitable for plotting with ggplot2\n  poly_region_coords$group <- paste(poly_region_coords$L1,\",\",poly_region_coords$L2,\",\",poly_region_coords$L3)\n  colnames(poly_region_coords) <- c(\"long\",\"lat\",\"group1\",\"group2\",\"group3\", \"region\",\"group\")\n  \n  # explore the data frame\n  print(head(poly_region_coords))\n  \n  return <- poly_region_coords \n}"},{"path":"make-geographical-maps-with-different-packages.html","id":"usage-of-extract_map_data","chapter":"33 Make Geographical Maps with Different Packages","heading":"33.3.1.2 Usage of “extract_map_data”","text":"extract_map_data(map)map: name region. available regions shown .","code":"\nunique(tolower(openstreetmap_fr_zones$parent))##  [1] NA                                 \"africa\"                          \n##  [3] \"spain\"                            \"asia\"                            \n##  [5] \"china\"                            \"india\"                           \n##  [7] \"indonesia\"                        \"japan\"                           \n##  [9] \"central-america\"                  \"europe\"                          \n## [11] \"austria\"                          \"belgium\"                         \n## [13] \"czech_republic\"                   \"finland\"                         \n## [15] \"france\"                           \"alsace\"                          \n## [17] \"aquitaine\"                        \"auvergne\"                        \n## [19] \"basse_normandie\"                  \"bourgogne\"                       \n## [21] \"bretagne\"                         \"centre\"                          \n## [23] \"champagne_ardenne\"                \"corse\"                           \n## [25] \"franche_comte\"                    \"haute_normandie\"                 \n## [27] \"ile_de_france\"                    \"languedoc_roussillon\"            \n## [29] \"limousin\"                         \"lorraine\"                        \n## [31] \"midi_pyrenees\"                    \"nord_pas_de_calais\"              \n## [33] \"pays_de_la_loire\"                 \"picardie\"                        \n## [35] \"poitou_charentes\"                 \"provence_alpes_cote_d_azur\"      \n## [37] \"rhone_alpes\"                      \"germany\"                         \n## [39] \"nordrhein_westfalen\"              \"italy\"                           \n## [41] \"netherlands\"                      \"norway\"                          \n## [43] \"poland\"                           \"portugal\"                        \n## [45] \"seas\"                             \"slovakia\"                        \n## [47] \"sweden\"                           \"switzerland\"                     \n## [49] \"ukraine\"                          \"united_kingdom\"                  \n## [51] \"england\"                          \"north-america\"                   \n## [53] \"canada\"                           \"ontario\"                         \n## [55] \"quebec\"                           \"us-west\"                         \n## [57] \"california\"                       \"oceania\"                         \n## [59] \"australia\"                        \"russia\"                          \n## [61] \"central_federal_district\"         \"far_eastern_federal_district\"    \n## [63] \"north_caucasian_federal_district\" \"northwestern_federal_district\"   \n## [65] \"siberian_federal_district\"        \"southern_federal_district\"       \n## [67] \"ural_federal_district\"            \"volga_federal_district\"          \n## [69] \"south-america\"                    \"argentina\"                       \n## [71] \"brazil\"                           \"central-west\"                    \n## [73] \"north\"                            \"northeast\"                       \n## [75] \"south\"                            \"southeast\""},{"path":"make-geographical-maps-with-different-packages.html","id":"plot-the-map-of-china","chapter":"33 Make Geographical Maps with Different Packages","heading":"33.3.2 Plot the map of China","text":"","code":"\nmap2 = extract_map_data(\"china\")##      long    lat group1 group2 group3 region     group\n## 1 114.900 32.950      1      1      1  anhui 1 , 1 , 1\n## 2 114.930 32.915      1      1      1  anhui 1 , 1 , 1\n## 3 115.000 32.920      1      1      1  anhui 1 , 1 , 1\n## 4 115.000 32.905      1      1      1  anhui 1 , 1 , 1\n## 5 115.015 32.890      1      1      1  anhui 1 , 1 , 1\n## 6 115.120 32.885      1      1      1  anhui 1 , 1 , 1\nggplot(map2,aes(x=long,y=lat,group=group))+\n  geom_polygon(fill=\"white\",color=\"black\")"},{"path":"make-geographical-maps-with-different-packages.html","id":"plot-the-map-of-japan","chapter":"33 Make Geographical Maps with Different Packages","heading":"33.3.3 Plot the map of Japan","text":"References:https://ggplot2.tidyverse.org/reference/map_data.htmlhttps://cran.r-project.org/web/packages/osmextract/vignettes/osmextract.html","code":"\nmap2 = extract_map_data(\"japan\")##      long    lat group1 group2 group3 region     group\n## 1 135.430 35.510      1      1      1  chubu 1 , 1 , 1\n## 2 135.450 35.495      1      1      1  chubu 1 , 1 , 1\n## 3 135.440 35.485      1      1      1  chubu 1 , 1 , 1\n## 4 135.445 35.450      1      1      1  chubu 1 , 1 , 1\n## 5 135.460 35.435      1      1      1  chubu 1 , 1 , 1\n## 6 135.470 35.435      1      1      1  chubu 1 , 1 , 1\nggplot(map2,aes(x=long,y=lat,group=group))+\n  geom_polygon(fill=\"white\",color=\"black\")"},{"path":"ten-interview-questions-and-answers.html","id":"ten-interview-questions-and-answers","chapter":"35 Ten interview questions and answers","heading":"35 Ten interview questions and answers","text":"Mengsu Alan Yang Zhe Wang","code":""},{"path":"ten-interview-questions-and-answers.html","id":"forward","chapter":"35 Ten interview questions and answers","heading":"35.1 Forward","text":"following cheatsheet created help fellow classmates internship search current strong-headwind job market environment.\nDon’t give ! can ! Happy hunting.","code":""},{"path":"ten-interview-questions-and-answers.html","id":"questions-and-answers","chapter":"35 Ten interview questions and answers","heading":"35.2 Questions and Answers","text":"","code":""},{"path":"ten-interview-questions-and-answers.html","id":"q1.-what-is-the-difference-between-r-and-python","chapter":"35 Ten interview questions and answers","heading":"35.2.1 Q1. What is the difference between R and Python?","text":"Answer: model building Data Science libraries similar comparable.\nModel Interpretability: R better model interpretability compared python (easier humans understand, important reporting management).\nProduction: Python programming good production, R falls short.\nCommunity Support: R better community support Python.\nData Visualization: R better data visualization libraries python.\nLearning curve: learning curve python steep R, R higher technical barrier entry.","code":""},{"path":"ten-interview-questions-and-answers.html","id":"q2.-what-are-r-packages","chapter":"35 Ten interview questions and answers","heading":"35.2.2 Q2. What are R packages?","text":"Answer: Packages collections data, functions, documentation extends capabilities base R [1]. 10,000 packages stored CRAN repository number still increasing [3]! Packages ggplot2, tibble, tidyr, dplyr part opinionated collection known “tidyverse.” One can install package calling install.packages() name package quotes argument function.","code":""},{"path":"ten-interview-questions-and-answers.html","id":"q3.-what-are-the-advantages-of-using-r","chapter":"35 Ten interview questions and answers","heading":"35.2.3 Q3. What are the advantages of using R?","text":"Answer: One strength R lies fact open source, therefore freely available distribute. R support data wrangling needs able create high quality highly manipulatible graphs plots ggplot2. R also platform independent highly compatible [2] runs operating systems. language great statistics lot Statistician buy-therefore great community support. needed, R also capable supporting Machine Learning operations.","code":""},{"path":"ten-interview-questions-and-answers.html","id":"q4.-what-are-the-disadvantages-of-using-r","chapter":"35 Ten interview questions and answers","heading":"35.2.4 Q4. What are the disadvantages of using R?","text":"Answer: beginners, R complication language steep learning curve. lacks standard GUI RStudio must used, inferior Python Jupyter Notebooks organization work flow. R good big data; consumes high memory slower run time Python MATLAB [3]. R falls short security language basic security measures. Many functionalities spread across many different packages, inconsistent quality, functionality, documentation.","code":""},{"path":"ten-interview-questions-and-answers.html","id":"q5.-what-is-the-difference-between-matrix-and-data-frames","chapter":"35 Ten interview questions and answers","heading":"35.2.5 Q5. What is the difference between matrix and data frames?","text":"Answer: data structure special way organizing data computer can used effectively. idea reduce spatial temporal complexity different tasks. two important data structures R Matrix Dataframe, look differ nature.matrix R –\nhomogeneous collection data sets arranged two-dimensional rectangular organization. *n array similar data types. created using vector input. fixed number rows columns. can perform many arithmetic operations R matrices, addition, subtraction, multiplication, division.Dataframes R –\nused store data tables. can contain multiple data types multiple columns called fields. ’s list equal length vectors. ’s generalized form matrix. ’s like table Excel worksheet. column row names. Row names unique empty columns. data stored must number, character, factor type. DataFrame heterogeneous.","code":""},{"path":"ten-interview-questions-and-answers.html","id":"q6.-what-is-the-difference-between-library-and-require","chapter":"35 Ten interview questions and answers","heading":"35.2.6 Q6. What is the difference between library() and require()?","text":"Answer: library() require() can used attach load installed additional packages. Installed packages identified help “DESCRPTION” file contains Build:field. name package needs loaded using library() require() must match name package’s “DESCRPTION” file.require() designed used inside functions gives warning message returns logical value, FALSE requested package found TRUE \npackage loaded.library() default returns error requested package exist.","code":""},{"path":"ten-interview-questions-and-answers.html","id":"q7.-what-types-of-data-files-can-be-read-and-exported-in-r","chapter":"35 Ten interview questions and answers","heading":"35.2.7 Q7. What types of data files can be read and exported in R?","text":"Answer: Data import output R programming means can read data external files, write data external files, can access files outside R environment. File formats like CSV, XML, xlsx, JSON, web data like HTML tables can imported R environment read manipulated data analysis [4]; data present R environment can stored external files file formats.","code":""},{"path":"ten-interview-questions-and-answers.html","id":"q8.-how-many-data-structures-does-r-have","chapter":"35 Ten interview questions and answers","heading":"35.2.8 Q8. How many data structures does R have?","text":"Answer: R six types basic data structures. can organize data structures according dimensions(1d, 2d, nd). can also classify homogeneous heterogeneous (can contents different types ).VectorListMatrixData frameArrayFactor","code":""},{"path":"ten-interview-questions-and-answers.html","id":"q9.-what-is-the-difference-between-pivot_wider-and-pivot_longer","chapter":"35 Ten interview questions and answers","heading":"35.2.9 Q9. What is the difference between pivot_wider() and pivot_longer()?","text":"Answer: pivot_longer() makes datasets longer vertically increasing number rows decreasing number columns; every row becomes observation. Length relative term, can say (e.g.) dataset longer dataset B. pivot_longer() commonly needed tidy wild-caught datasets often optimize ease data entry ease comparison rather ease analysis.pivot_wider() opposite pivot_longer(): makes dataset wider increasing number columns decreasing number rows. ’s relatively rare need pivot_wider() make tidy data, ’s often useful creating summary tables presentation, data format needed tools.","code":""},{"path":"ten-interview-questions-and-answers.html","id":"q10.-why-do-people-use-ggplot2","chapter":"35 Ten interview questions and answers","heading":"35.2.10 Q10. Why do people use ggplot2?","text":"Answer: ggplot2 plotting package provides helpful commands create complex plots data data frame. provides programmatic interface specifying variables plot, displayed, general visual properties following Grammar Graphics. Therefore, need minimal changes underlying data change decide change bar plot scatterplot. helps creating publication quality plots minimal amounts adjustments tweaking.","code":""},{"path":"ten-interview-questions-and-answers.html","id":"references-1","chapter":"35 Ten interview questions and answers","heading":"35.3 References","text":"[1] Wickham, Hadley, Garrett Grolemund. R Data Science: Import, Tidy, Transform, Visualize Model Data, O’Reilly, Pozostate, 2017.[2] Intellipaaat. “R Programming Interview Questions.” YouTube, 3 Jan. 2021, https://www.youtube.com/watch?v=lyFDNkbsQuE&ab_channel=Intellipaat.[3] Simplilearn. “R: Overview, Applications R Used .” Simplilearn.com, Simplilearn, 3 Oct. 2022, https://www.simplilearn.com/--r-article.[4] “R Programming Language - Introduction.” GeeksforGeeks, 15 Aug. 2021, https://www.geeksforgeeks.org/r-programming-language-introduction/.","code":""},{"path":"data-science-internship-search-survival-guide.html","id":"data-science-internship-search-survival-guide","chapter":"36 Data science internship search survival guide","heading":"36 Data science internship search survival guide","text":"Austin Chen Kelly DuAs current graduate student (even undergraduate), always knew importance gaining real world experience, translate classroom teachings something tangible understand field likely investing better part career towards. However, process obtain valuable experience procure summer internship always easy, fact, actually quite contrary. personal experience, despite knowing plethora positions available, still struggle find actual application forms links. even applying numerous positions (thankfully getting past resume screenings), found idea expect different kinds interviews facing. Although certainly “interview preparation” resources available online, often find generalized, frankly unhelpful. Therefore, Community Contribution project pool together actual experiences two Data Science students, hopefully save fellow peers pain anguish felt first started “internship grind”.Throughout composition “survival guide”, genuinely surprised amount useful information able put words . Despite using majority included daily basis (Summer 2023 cycle still ongoing), actually translating knowledge pen paper made realize complicated interview process can . Thus, evaluate project, can confidently say work accomplished help others least getting familiarized data science internship search. certainly guarantee interview, even explicitly increase rate success, definitely equip new candidates toolbelt utilization. point something might differently next time, include section actual interview processes (company name, many rounds, interview type …etc). Although list going extraordinarily long (respectable), act reference need . conclude, internship search, especially current economic climate, undoubtedly frustrating time consuming, hope project ease stress future data science interns.link survival guide!\nhttps://github.com/austinchen11/Fall2022_EDAV_Community_Contribution","code":""},{"path":"benford-case-study.html","id":"benford-case-study","chapter":"38 Benford Case Study","heading":"38 Benford Case Study","text":"Abhiram Gaddam, Devan Samant","code":"\nlibrary(benford.analysis)  #install.packages(\"benford.analysis\")\nlibrary(dplyr)\nlibrary(plotly)\nlibrary(tidyverse)"},{"path":"benford-case-study.html","id":"contents","chapter":"38 Benford Case Study","heading":"38.1 Contents","text":"tutorial, introduce numeric property called Benford’s Law illustrate applications fraud detection. using benford_analysis package R along example case study.Sections:IntroductionBenford.Analysis PackageRandomized Data Case StudyConclusionSources","code":""},{"path":"benford-case-study.html","id":"introduction-3","chapter":"38 Benford Case Study","heading":"38.2 Introduction","text":"niche area consulting industry called forensic analytics analysts try identify risks quantify wrongdoing using array statistical data techniques. example, imagine whistleblower notifies company’s general counsel collusion sales finance representatives artificially create invoices. company may hire forensic analysts extract determine happening. many quantitative qualitative methods perform concluding anything need specific context project. One heuristic Benford’s Law.","code":""},{"path":"benford-case-study.html","id":"what-is-benfords-law","chapter":"38 Benford Case Study","heading":"38.2.1 What is Benford’s Law?","text":"Benford’s law (also known first digit law) states leading digits many data sets probably going small. example, numbers set (30%) leading digit 1, one might expect probability 11.1% (one nine digits). one, second common leading digit 2 17.5%. forth 3 onward. put simply, Benford’s law probability distribution likelihood first digit set numbers (Frunza, 2015). pattern intuitive phenomenon holds true many naturally occurring datasets (ex. height mountains around world) well man-made ones company’s general ledger.formula Benford’s Law :\n\\(P(d) = \\frac{ln(1 + \\frac{1}{d})}{ln10}\\)\n\\(d\\) leading digit (number 1 9)distribution shows expected occurrence leading digits according Benford’s Law.Benfords Law Distribution.However, caveats regarding application Benford’s Law:Benford’s Law works better larger sets data. law shown hold true data sets containing 50 100 numbers, experts believe data sets 500 numbers better suited type analysis.\n(https://tinyurl.com/2p97fsvn)Benford’s Law works better larger sets data. law shown hold true data sets containing 50 100 numbers, experts believe data sets 500 numbers better suited type analysis.\n(https://tinyurl.com/2p97fsvn)conform law, data set use must contain data number 1 9 equal chance occurring leading digit. Otherwise, Benford’s Law doesn’t apply. example, consider listing heights current NBA players. Since NBA players range height 5 feet 10 inches 7 feet 3 inches, player heights begin 1, 2, 3, 4, 8, 9; Clearly digits chance first digit listing, making Benford’s Law inapplicable.conform law, data set use must contain data number 1 9 equal chance occurring leading digit. Otherwise, Benford’s Law doesn’t apply. example, consider listing heights current NBA players. Since NBA players range height 5 feet 10 inches 7 feet 3 inches, player heights begin 1, 2, 3, 4, 8, 9; Clearly digits chance first digit listing, making Benford’s Law inapplicable.Benford’s Law applicable datasets, generally applicable large sets naturally occurring numbers connection like:Companies’ stock market values.Data found texts.Demographic data, including state city populations.Income tax data.Mathematical tables, like logarithms.River drainage rates.Scientific data.\nBenford, F (1938)","code":""},{"path":"benford-case-study.html","id":"applications-in-fraud-detection","chapter":"38 Benford Case Study","heading":"38.2.2 Applications in Fraud Detection","text":"One primary practical use Benford’s Law fraud error detection. expected large set numbers follow law, accountants, auditors, economists tax professionals benchmark normal levels particular number set . famously documented examples Benford’s Law applied towards fraud detection (Frunza, 2015):1990s, accountant named Mark Nigrini found Benford’s law can effective red-flag test fabricated tax returns. Authentic tax data usually follows Benford’s law, whereas made-returns .law used 2001 study economic data Greece, implication country may manipulated numbers join European Union.Ponzi schemes can detected using law. Unrealistic returns, purported Maddoff scam, fall far expected Benford probability distribution .","code":""},{"path":"benford-case-study.html","id":"benford-package-in-r","chapter":"38 Benford Case Study","heading":"38.3 Benford Package in R","text":"Benford Analysis (benford.analysis) package provides tools make easier validate data using Benford’s Law. main purpose package identify suspicious data may need verification.Documentation package can found :\nhttps://cran.r-project.org/web/packages/benford.analysis/benford.analysis.pdfYou can install package CRAN running following (uncommented):package comes 6 real datasets Mark Nigrini’s book Benford’s Law: Applications Forensic Accounting, Auditing, Fraud Detection.","code":"\n#install.packages(\"benford.analysis\")"},{"path":"benford-case-study.html","id":"example-usage-of-benford.analysis","chapter":"38 Benford Case Study","heading":"38.3.1 Example Usage of benford.analysis","text":"section give example using 189,470 records corporate payments dataset provided package.Load package dataTo validate data Benford’s law simply use function “benford” appropriate column:creates object class “Benford” results analysis using first two significant digits default.Lets plot bfd observe trends. Note running analysis using default parameters, .e., ..digits = 2. parameter can modified want analyze first digit .original data blue expected frequency according Benford’s law red.\nexample, first plot shows data tendency follow Benford’s law.\nalso clear outlier 50.package also provides helper functions investigate data. example, can easily extract observations largest discrepancies using “getSuspects” function.","code":"\ndata(corporate.payment) \n\ndf <- corporate.payment\nhead(df)##   VendorNum       Date  InvNum Amount\n## 1      2001 2010-01-02 0496J10  36.08\n## 2      2001 2010-01-02 1726J10  77.80\n## 3      2001 2010-01-02 2104J10  34.97\n## 4      2001 2010-01-02 2445J10  59.00\n## 5      2001 2010-01-02 3281J10  59.56\n## 6      2001 2010-01-02 3822J10  50.38\nbfd <- benford(df$Amount)\nbfd## \n## Benford object:\n##  \n## Data: df$Amount \n## Number of observations used = 185083 \n## Number of obs. for second order = 65504 \n## First digits analysed = 2\n## \n## Mantissa: \n## \n##    Statistic  Value\n##         Mean  0.496\n##          Var  0.092\n##  Ex.Kurtosis -1.257\n##     Skewness -0.002\n## \n## \n## The 5 largest deviations: \n## \n##   digits absolute.diff\n## 1     50       5938.25\n## 2     11       3331.98\n## 3     10       2811.92\n## 4     14       1043.68\n## 5     98        889.95\n## \n## Stats:\n## \n##  Pearson's Chi-squared test\n## \n## data:  df$Amount\n## X-squared = 32094, df = 89, p-value < 2.2e-16\n## \n## \n##  Mantissa Arc Test\n## \n## data:  df$Amount\n## L2 = 0.0039958, df = 2, p-value < 2.2e-16\n## \n## Mean Absolute Deviation (MAD): 0.002336614\n## MAD Conformity - Nigrini (2012): Nonconformity\n## Distortion Factor: -1.065467\n## \n## Remember: Real data will never conform perfectly to Benford's Law. You should not focus on p-values!\nplot(bfd)\nsuspects <- getSuspects(bfd, df)\nsuspects##        VendorNum       Date       InvNum  Amount\n##     1:      2001 2010-01-02      3822J10   50.38\n##     2:      2001 2010-01-07     100107-2 1166.29\n##     3:      2001 2010-01-08  11210084007 1171.45\n##     4:      2001 2010-01-08      1585J10   50.42\n##     5:      2001 2010-01-08      4733J10  113.34\n##    ---                                          \n## 17852:     52867 2010-07-01 270358343233   11.58\n## 17853:     52870 2010-02-01 270682253025   11.20\n## 17854:     52904 2010-06-01 271866383919   50.15\n## 17855:     52911 2010-02-01 270957401515   11.20\n## 17856:     52934 2010-02-01 271745237617   11.88"},{"path":"benford-case-study.html","id":"randomized-case-study","chapter":"38 Benford Case Study","heading":"38.4 Randomized Case Study","text":"prior section saw data manipulated static values, clearly first digit rule broken. data randomly manipulated?Let us try experiment see can use Benford’s Law detect potential data manipulation bad actor randomly generated fake sales. experiment, baseline also using random data highlight point conclusion . Let us pretend company sales data comprised prices quantities items sold. code chunck sets data frame one row recorded sale number items sold set price.Now let us set see dataset exhibits expected pattern first digit.[value] sales exhibits commonality Benford’s Law exact see first digit quite 30%. However still decreasing probability leading digit. address limitation next section. now, let us try manipulating data see patterns change.","code":"\nprice <- sample(1:1e3, size = 1e5, replace=TRUE)\nquantity <- sample(1:1e4, size = 1e5, replace=TRUE)\ndf <- data.frame(price,quantity) %>% \n  mutate(value = price*quantity) %>% \n  mutate(digit = substr(as.character(value), 1, 1))\n\nhead(df)##   price quantity   value digit\n## 1   382     2174  830468     8\n## 2   754     7813 5891002     5\n## 3    26     5486  142636     1\n## 4   209     9865 2061785     2\n## 5   499     8341 4162159     4\n## 6   526     4060 2135560     2\ndf_group <- df %>% group_by(digit) %>% summarise(count = n()) %>%\n    mutate(count_percent = count/sum(count))\n\nbase_benford = data.frame(c(1,2,3,4,5,6,7,8,9), c(.31,.176,.125,.097,.079,.067,.058,.051,.046))\ncolnames(base_benford) <- c('digit','percent')\n\nggplot(data=df_group, aes(x=digit, y=count_percent, fill=\"blue\")) +\n  geom_bar(stat=\"identity\", fill='lightblue') + \n  geom_point(aes(x=base_benford$digit, y=base_benford$percent)) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"First Digit of Randomized Sales\",\n              subtitle = \"Original Data (bar) vs Expected Benford (point)\",\n              #caption = \"TBD\",\n              x = \"First Digit\", y = \"% Occurance\",\n              #tag = \"A\"\n              ) +  theme(legend.position=\"none\")"},{"path":"benford-case-study.html","id":"manipulated-data","chapter":"38 Benford Case Study","heading":"38.4.1 Manipulated Data","text":"next piece code, can pretend someone entered additional sales “randomly” adding lots relatively smaller sales 100k.can see adding sales randomly change distribution (enough added). can quantify change? Let us use benford package cacluate statistics. followingPrinting benford object creates verbose output copied main statistics two dataframes:bfd_1 (df - original)\nStatistic Value\nMean 0.526\nVar 0.073\nEx.Kurtosis -1.048\nSkewness -0.145bfd_1 (df2 - manipulated)\nStatistic Value\nMean 0.65\nVar 0.07\nEx.Kurtosis -0.59\nSkewness -0.66If data follows Benford’s Law, expected statistics close :\nStatistic Value\nMean 0.5\nVar 0.083\nEx.Kurtosis -1.2\nSkewness 0From glance, evident statistics corresponding original df much closer expected statistics df2. particular, Ex.Kurtosis Skewness differ significantly df2 expected values. indicators set values deviate expected distribution corresponding Benford’s Law.","code":"\nvalue2 <- sample(1:1e6, size = 4e5, replace=TRUE)\ndf_extra_sales <- data.frame(0,0,value2)\ncolnames(df_extra_sales) <- c('price','quantity','value')\ndf2 <- data.frame(price,quantity) %>% \n  mutate(value = price*(quantity)) %>% \n  rbind(df_extra_sales)  %>% \n  mutate(digit = substr(as.character(value), 1, 1))\n\ndf_group2 <- df2 %>% group_by(digit) %>% summarise(count = n()) %>%\n    mutate(count_percent = count/sum(count))\n\ndf_group_combined <- data.frame(df_group$digit,df_group$count_percent,df_group2$count_percent) \ncolnames(df_group_combined) <- c('digit','Original','Manipulated')\ndf_group_combined <- df_group_combined %>% pivot_longer(cols=c('Original','Manipulated'))\ncolnames(df_group_combined) <- c('digit','Dataset','count_percent')\n\nggplot(data=df_group_combined, aes(x=digit, y=count_percent, fill=fct_rev(as.factor(Dataset)) )) +\n  geom_bar(stat=\"identity\", position='dodge') +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"First Digit of Manipulated Sales\",\n              subtitle = \"Original Data vs Manipulated Data\",\n              x = \"First Digit\", y = \"% Occurance\",\n              fill=\"Dataset\"\n              ) +  theme(legend.position=\"bottom\")\nbfd_1 <- benford(df$value)\n#bfd_1\n\nbfd_2 <- benford(df2$value)\n#bfd_2"},{"path":"benford-case-study.html","id":"findings","chapter":"38 Benford Case Study","heading":"38.4.2 Findings","text":"show us? sets randomly generated, one conform Benford’s Law? One reason data product many random variables tends exhibit log-normal distribution fits well first digit rule. However, adding values static way, change data uniform distort decaying curve.Now important understand 1) case extreme example highlight point, 2) Benford’s Law statistics prove anything - provide guidance areas may look strange. reality, forensic testing done specific segments accounts, people interviewed, systems logs analyzed, etc. Similarly, even Benford’s Law holds true, mean fraud; bad actor understands fraud detection methods likely hide better.","code":""},{"path":"benford-case-study.html","id":"conclusion-6","chapter":"38 Benford Case Study","heading":"38.5 Conclusion","text":"important thing note Benford’s law strict mathematical proof, simply heuristic. broad guideline helps investigation large data sets see conform observed trends. feature dataset conforms Benford’s Law proof validity vice versa. reason, Benford analysis usually conducted primary investigatory exercise, following thorough investigation described previous section conducted.","code":""},{"path":"benford-case-study.html","id":"sources","chapter":"38 Benford Case Study","heading":"38.6 Sources","text":"Benford, F. “Law Anomalous Numbers,” Proceedings American Philosophical Society, 78, 551–572. 1938.Frunza, M. (2015). Solving Modern Crime Financial Markets: Analytics Case Studies. Academic Press.https://cran.r-project.org/web/packages/benford.analysis/benford.analysis.pdfhttps://www.statisticshowto.com/benfords-law/https://tinyurl.com/2p97fsvn","code":""},{"path":"github-initial-setup.html","id":"github-initial-setup","chapter":"39 Github initial setup","heading":"39 Github initial setup","text":"Joyce Robbins","code":""},{"path":"github-initial-setup.html","id":"create-new-repo","chapter":"39 Github initial setup","heading":"39.1 Create new repo","text":"Create new repository copying template: http://www.github.com/jtr13/cctemplate following instructions README.","code":""},{"path":"github-initial-setup.html","id":"pages-in-repo-settings","chapter":"39 Github initial setup","heading":"39.2 Pages in repo settings","text":"Change source gh-pagesMay trigger GHA get work","code":""},{"path":"github-initial-setup.html","id":"add-packages-to-description-file","chapter":"39 Github initial setup","heading":"39.3 Add packages to DESCRIPTION file","text":"Need better process…Downloaded submissions CourseWorksCreate DESCRIPTION file. Add add dependencies projthis::proj_update_deps()https://twitter.com/ijlyttle/status/1370776366585614342Add Imports real DESCRIPTION file.Found problematic packages looking reverse dependencies packages failed install:devtools::revdep()Also used pak::pkg_deps_tree()Problems:magickrJava dependency qdap","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"tutorial-for-pull-request-mergers","chapter":"40 Tutorial for pull request mergers","heading":"40 Tutorial for pull request mergers","text":"","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"general","chapter":"40 Tutorial for pull request mergers","heading":"40.1 General","text":"following checklist steps perform merging pull request. point, ’re sure , request review one PR leaders.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"check-branch","chapter":"40 Tutorial for pull request mergers","heading":"40.2 Check branch","text":"PR submitted non-main branch.PR submitted main branch, provide instructions fix problem:Close PR.Close PR.Follow instructions forgetting branch committed pushed GitHub: https://edav.info/github#fixing-mistakesFollow instructions forgetting branch committed pushed GitHub: https://edav.info/github#fixing-mistakesIf trouble 2., delete local folder project, delete fork GitHub, start .trouble 2., delete local folder project, delete fork GitHub, start .Open new PR.Open new PR.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"examine-files-that-were-added-or-modified","chapter":"40 Tutorial for pull request mergers","heading":"40.3 Examine files that were added or modified","text":"ONE .Rmd file.ONE .Rmd file.additional resources resources/<project_name>/ folder.additional resources resources/<project_name>/ folder.files root directory besides .Rmd file.files root directory besides .Rmd file.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"check-.rmd-filename","chapter":"40 Tutorial for pull request mergers","heading":"40.4 Check .Rmd filename","text":".Rmd filename words joined underscores, white space. (Update: need branch name.).Rmd filename can contain lowercase letters. (Otherwise filenames sort nicely repo home page.)","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"check-.rmd-file-contents","chapter":"40 Tutorial for pull request mergers","heading":"40.5 Check .Rmd file contents","text":"file contain YAML header --- line.second line blank, followed author name(s).first line start single hashtag #, followed single whitespace, title.additional single hashtag headers chapter. (, new chapters created.)hashtag headers followed numbers since hashtags create numbered subheadings. Correct: ## Subheading. Incorrect: ## 3. Subheading.file contains setup chunk .Rmd file, contain setup label. (bookdown render fail duplicate chunk labels.)\n.e. use {r, include=FALSE} instead {r setup, include=FALSE}.\nSee sample .RmdLinks internal files must contain resources/<project_name>/ path, : ![Test Photo](resources/sample_project/election.jpg)file contain install.packages(), write functions, setwd(), getwd().’s anything else looks odd ’re sure, assign jtr13 review explain issue.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"request-changes","chapter":"40 Tutorial for pull request mergers","heading":"40.6 Request changes","text":"problems checks listed , explain pull request merged request changes following steps:, add changes requested label pull request.job pull request done now. contributors fix requests, review either move forward merge explain changes still need made.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"steps-to-merge-the-pr","chapter":"40 Tutorial for pull request mergers","heading":"40.7 Steps to Merge the PR","text":"click “Merge” things .","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"update-the-branch","chapter":"40 Tutorial for pull request mergers","heading":"40.7.1 Update the branch","text":"“Update Branch” visible toward end Conversation tab pull request, click . ensure working --date versions _bookdown.yml DESCRIPTION.Next make changes files contributor’s branch.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"add-the-filename-of-the-chapter-to-_bookdown.yml","chapter":"40 Tutorial for pull request mergers","heading":"40.7.2 Add the filename of the chapter to _bookdown.yml","text":"Go “Files Changed” copy filename .Rmd file.Open branch submitted PR following steps:\naccess PR branch:\n\nMake sure PR branch checking PR branch name shown (main):\nOpen branch submitted PR following steps:access PR branch:Make sure PR branch checking PR branch name shown (main):Add name new file single quotes followed comma labelled section (eg. Cheatsheets, Tutorials etc).Add name new file single quotes followed comma labelled section (eg. Cheatsheets, Tutorials etc).Save edited version.Save edited version.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"add-part-names-to-.rmd-for-every-first-article-in-part","chapter":"40 Tutorial for pull request mergers","heading":"40.7.3 (Add part names to .Rmd for every first article in part)","text":"adding first chapter PART.One person manage , otherwise hard keep project organized.every first article part, add chapter name top .Rmd file, propose changes. example like .\n","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"add-new-libraries-to-description.","chapter":"40 Tutorial for pull request mergers","heading":"40.7.4 Add new libraries to DESCRIPTION.","text":"Check .Rmd libraries needed. missing, add DESCRIPTION file contributor’s branch, manner edited _bookdown.yml file.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"merge-the-pull-request","chapter":"40 Tutorial for pull request mergers","heading":"40.7.5 Merge the pull request","text":"’re sure things correctly, assign one maintainers @jtr13 review merge PR.Return PR main page repo www.github.com/jtr13/...Return PR main page repo www.github.com/jtr13/...necessary resolve merge conflicts clicking resolve merge conflicts button:necessary resolve merge conflicts clicking resolve merge conflicts button:delete lines <<<<<<< xxxx, ======= >>>>>>>> main edit file desired. Click “Marked resolved” button green “Commit merge” button. –>Click “Merge pull request” “Confirm merge”. Add thank note perhaps emoji :tada:.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"check-actions","chapter":"40 Tutorial for pull request mergers","heading":"40.7.6 Check Actions","text":"minutes, click Actions tabs check whether build successful: green dot indicates successful run, red X indicates failed run.minutes, click Actions tabs check whether build successful: green dot indicates successful run, red X indicates failed run.Check log figure went wrong, can, fix . ’re sure , problem, just open issue linking failed run others can help (important can fix problems quickly). (click revert merge).Check log figure went wrong, can, fix . ’re sure , problem, just open issue linking failed run others can help (important can fix problems quickly). (click revert merge).","code":""}]
